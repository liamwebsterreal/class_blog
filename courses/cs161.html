<!DOCTYPE html>
<html>
    <head>
        <title>Hi</title>
        <link href="../css/styles.css" rel="stylesheet" type="text/css">
      </head>
<body>
    <h2>Computer Science 161: Computer Security</h2>
    <a href="../index.html">Home</a>
    <div id="toc_container">
        <p class="toc_title">Content:</p>
        <ul class="toc_list">
        <li><a href="#FP1">Overview</a></li>
        <li><a href="#FP2">Security Principles</a></li>
        <li><a href="#FP3">x86 Assembly and Call Stack</a></li>
        <li><a href="#FP4">Memory Safety Vulnerabilities</a></li>


        </ul>
    </div>
        <h3 id="FP1">Overview</h3> 
        <p id="subtext">
            1. Security Principles<br>
            2. Memory Safety: x86 Assembly and Call Stack, Memory Safety Vulnerabilities, Mitigating Memory-Safety Vulnerabilities<br>
            3. Cryptography: <br>
            4. Web Security: <br>
            5. Network Security: <br>

        </p>

        <h3 id="FP2">Security Principles</h3>
        <p id="subtext">
            Summary: Always know your threat model-- the who and what. Consider human factors in security implementations-- keep tools fool proof and user friendly. No system is ever 100% secure, its a matter of resources. Don't put a 100 dollar lock on a dollar item, and vice versa. If an attack is inevitable always at least have detection of said attack when prevention is not possible. Layer defenses in depth. Always limit access to least privilege, give enough access to get the job done. Split up privilege so no one party has complete access. Takes multiple to launch the nuke. Ensure mediation, check all access points in and out. Never rely on obscurity for security-- the deets always get out. Use fail safe defaults-- meaning when a fail happens it better default to a safe space. Design security from the start don't try to back track. The TCB, the portion of the system that must operate correctly in order for the security goals of the system to be assured. Keep the principle of Time-of-Check To Time-Of-Use in mind.<br>
        </p>
        <h4 id="subhead">Know your threat model:</h4>
        <p id="subtext">
            Threat Model: a model of who your attacker is and what resources they have.<br>
            Commone Assumptions that are taken into account for attackers:<br>
            <p id="subtext_bullet">
                The attacker can inter with your systems without anyone noticing.<br>
                The attacker has some general information about your system.<br>
                The attacker is persistent and lucky.<br>
                The attacker has the resources required to undertake the attack.<br>
                The attacker can coordinate several complex attacks across various systems.<br>
                Every system is a potential target.<br>
            </p>"
        </p>
        <h4 id="subhead">Consider Human Factors:</h4>
        <p id="subtext">
            Security systems must be usable by ordinary people and therefore must be designed to take into account the role that humans will play. <br>
            Takeaway: consider the tools that are presented to users, and try to make them fool-proof and as user-friendly as possible.<br>
        </p>
        <h4 id="subhead">Security is Economics:</h4>
        <p id="subtext">
            Security is often a cost-benefit analysis where someone needs to make a decision regarding how much security is worth.<br>
            A corollary of this principle is you should focus your energy on securing the weakest links. Security is like a chain: a system is only as secure as the weakest link. Attackers follow the path of least resistance, and they will attack the system at its weakest point.<br>
            A closely related principle is conservative design, which states that systems should be evaluated according to the worst security failure that is at all plausible, under assumptions favorable to the attacker.<br>
        </p>
        <h4 id="subhead">Detect if you can't Prevent:</h4>
        <p id="subtext">
            If prevention is stopping an attack from taking place, detection is simply learning that the attack has taken place, and response would be doing something about the attack. The idea is that if you cannot prevent the attack from happening, you should at least be able to know that the attack has happened. Once you know that the attack has happened, you should find a way to respond, since detection without response is pointless.<br>
            When dealing with response, you should always assume that bad things will happen, and therefore prepare your systems for the worst case outcome.<br>
        </p>
        <h4 id="subhead">Defense in depth:</h4>
        <p id="subtext">
            Defense in Depth: defenses should be layered together so an attacker would have to breach all the defenses to successfully attack a system.<br>
            Beware of diminishing returns–if you’ve already built 100 walls, the 101st wall may not add enough additional protection to justify the cost of building it (security is economics).<br>
        </p>
        <h4 id="subhead">Least Privilege:</h4>
        <p id="subtext">
            Give a program the set of access privileges that it legitimately needs to do its job—but nothing more. Try to minimize how much privilege you give each program and system component.<br>
            Least privilege is an enormously powerful approach. It doesn’t reduce the probability of failure, but it can reduce the expected cost of failures. The less privilege that a program has, the less harm it can do if it goes awry or becomes subverted.<br>
        </p>
        <h4 id="subhead">Separation of Responsibility:</h4>
        <p id="subtext">
            Split up privilege, so no one person or program has complete power. Require more than one party to approve before access is granted.<br>
            In summary, if you need to perform a privileged action, require multiple parties to work together to exercise that privilege, since it is more likely for a single party to be malicious than for all of the parties to be malicious and collude with one another.<br>
        </p>
        <h4 id="subhead">Ensure Complete Mediation:</h4>
        <p id="subtext">
            When enforcing access control policies, make sure that you check every access to every object. This kind of thinking is helpful to detect where vulnerabilities could be. As such, you have to ensure that all access is monitored and protected. One way to accomplish this is through a reference monitor, which is a single point through which all access must occur.<br>
        </p>
        <h4 id="subhead">Shannon's Maxim:</h4>
        <p id="subtext">
            Shannon’s Maxim states that the attacker knows the system that they are attacking.<br>
            “Security through obscurity” refers to systems that rely on the secrecy of their design, algorithms, or source code to be secure. The issue with this, however, is that it is extremely brittle and it is often difficult to keep the design of a system secret from a sufficiently motivated attacker. Historically, security through obscurity has a lousy track record: many systems that have relied upon the secrecy of their code or design for security have failed miserably.<br>
            As such, you should never rely on obscurity as part of your security. Always assume that the attacker knows every detail about the system that you are working with (including its algorithms, hardware, defenses, etc.)<br>
            erckhoff’s Principle, which states that cryptographic systems should remain secure even when the attacker knows all internal details of the system.<br>
        </p>
        <h4 id="subhead">Use Fail-Safe Defaults:</h4>
        <p id="subtext">
            Choose default settings that “fail safe”, balancing security with usability when a system goes down. Ensure that if the security mechanisms fail or crash, they will default to secure behavior, not to insecure behavior.<br>
        </p>
        <h4 id="subhead">Design security in from the start:</h4>
        <p id="subtext">
            Trying to retrofit security to an existing application after it has already been spec’ed, designed, and implemented is usually a very difficult proposition. At that point, you’re stuck with whatever architecture has been chosen, and you don’t have the option of decomposing the system in a way that ensures least privilege, separation of privilege, complete mediation, defense in depth, and other good properties. Backwards compatibility is often particularly painful, because you can be stuck with supporting the worst insecurities of all previous versions of the software.<br>
        </p>
        <h4 id="subhead">The Trusted Computing Base (TCB):</h4>
        <p id="subtext">
            In any system, the trusted computing base (TCB) is that portion of the system that must operate correctly in order for the security goals of the system to be assured. We have to rely on every component in the TCB to work correctly. However, anything that is outside the TCB isn’t relied upon in any way; even if it misbehaves or operates maliciously, it cannot defeat the system’s security goals. Generally, the TCB is made to be as small as possible since a smaller, simpler TCB is easier to write and audit.<br>
            TCB Design Principles:<br>
            <p id="subtext">
                Unbypassable: there must be no way to breach system security by bypassing the TCB.<br>
                Tamper-resistant: the TCB should be protected from tampering by anyone else. <br>
                Verifiable: It should be possible to verify the correctness of the TCB.<br>
            </p>
        </p>
        <p id="subtext">
            Design your system so that as much code as possible can be moved outside the TCB.<br>
            Benefits of TCBs: The notion of a TCB is a very powerful and pragmatic one as it allows a primitive yet effective form of modularity. It lets us separate the system into two parts: the part that is security-critical (the TCB), and everything else.<br>
        </p>s
        <h4 id="subhead">TOCTTOU Vulnerabilities:</h4>
        <p id="subtext">
            This is known as a Time-Of-Check To Time-Of-Use (TOCTTOU) vulnerability, because between the check and the use of whatever state was checked, the state somehow changed.<br>
        </p>

        <h3 id="FP3">x86 Assembly and Call Stack</h3>
        <h4 id="subhead">Number Representation:</h4>
        <p id="subtext">
            At the lowest level, computers store memory as individual bits, where each bit is either 0 or 1. <br>
            1 nibble = 4 bits<br>
            1 byte = 8 bits<br>
            1 word = 32 bits(on 32-bit architecture)<br>
            A "word" is the size of a pointer, which depends on your CPU architecture. 
        </p>
        <h4 id="subhead">Call Stack:</h4>
        <p id="subtext">
            The compiler translates your C code into assembly instructions. 61c uses the RISC-V instruction set but in 161 we use x86, which is more commonly seen in the real world.<br>
            The assembler translates the assembly instructions from the compiler into machine code.<br>
            The linker resolves dependencies on external libraries. After the linker finishes linking external libraries, it outputs a binary executable of the program that you can run. <br>
            The user runs the executable, the loader sets up an address space in memory and runs the machine code instructions in the executable.<br>
        </p>
        <h4 id="subhead">C memory layout:</h4> 
        <p id="subtext">
            At runtime, the OS gives the program an address space to store any state necessary for program execution. Each byte has a unique address. The size of the address space depends on the OS and CPU architecture. In a 32 bit system address are 32 bits long, which means the address space has 2^32 bytes of memory.<br>
            <img id="small_image" src="../assets/cs161/memorylayout.jpeg" alt=""><br><br>
            The code section contains executable instructions of the program. The assembler and linker output raw bytes that can be interpreted as machine code. These bytes are stored in the code section.<br>
            The static section contains constants and static variables that never change during program execution, and are usually allocated when the program starts.<br>
            The heap stores dynamically allocated data. When malloc is called in C, memory is allocated on the heap and persists until free is called. The heap starts at lower addresses and "grows up" to higher addresses as more memory is allocated.<br>
            The stack stores local variables and other information associated with function calls. The stack starts at higher addresses and "grows down" as more functions are called.<br>
            x86 is a Little Endian system this means when storing a word in memory the least significant byte is stored as the lowest address, and the most significant byte is stored at the highest address.<br>
        </p>
        <h4 id="subhead">Registers:</h4> 
        <p id="subtext">
            In addition to teh 2^32 bytes of memory in the address space, there are also registers, which store memory directly on the CPU. Each register can store one word. Unlike memory registers do not have addresses. Instead, registers are referred to by names. There are three special x86 registers that are relevant:
            <p id="subtext_bullet">
                eip: the instruction pointer, stores that address fo teh machine instruction currently being executed. IN RISC-V this register is called the PC.<br>
                ebp: the base pointer, stores the address of the top of the current stack frame. IN RISC-V systems this register is called the FP. <br>
                esp: the stack pointer, stores the address of the bottom of the current stack frame. In RISC-V this register is called the SP.<br>
                eax and ebx are general purpose registers in x86<br>
                Note: The eip register points to the code section of memory while the ebp and esp registers typically point to stack memory.<br>
            </p>
        </p>
        <h4 id="subhead">Stack--Pushing and Popping:</h4> 
        <p id="subtext">
            When it is desirable to save a variable on the stack there are two steps to take. First allocated additional space on the stack by decrementing the esp. Next store the value in the newly allocated space. The x86 push instruction does both of these steps to add a value to the stack. To remove a value from the stack increments the esp register, in x86 the pop instruction does this. It also takes the value that was just popped and copies the value to a register. Note when we pop a value off the stack the bits of memory aren't removed but the memory space becomes undefined.<br>
        </p>
        <h4 id="subhead">x86 calling convention:</h4> 
        <p id="subtext">
            This class uses AT&T x86 syntax(since that is what GDB uses). This means that the destination register comes last; note that this is in contrast with RISc_v assembly where the destination register comes first. Suppose our assembly instruction was addl $0x*, %ebx; here, the opcode is addl, the source is $0x8, and the destination register is %ebx, so in pseudocode this can be read as EBX = EBX + 0x8.<br>
            References to registers are preceded with a percent sign, so if we wanted to reference eax, we would do so as %eax. Immediates are preceded with a dollar sign(i.e. $1, $0x4, etc.). Furthermore, memory references use parenthesis and can have immediate offsets; for example. 12(%esp) dereferences memory 12 bytes above the address contained in ESP. If parenthesis are used without an immediate offset, the offset can be thought of as an implicit 0.<br>
        </p>
        <h4 id="subhead">x86 function calls:</h4> 
        <p id="subtext">
            When a function is called, the stack allocates extra space to store local variables and other information relevant to that function. The stack grows down, so this extra space will be at lower addresses in memory. Once the function returns, the space on the stack is freed up for future function calls. In a function call, the caller calls the callee. Program execution starts in the caller, moves to the callee as a result of the function call and then returns to the caller after the function call completes.<br>
            When a function call is made in x86 the three special registers-- eip, evp, esp --need to be updated. The eip needs to be changed to point to the instructions of the callee. The ebp and esp currently point to the top and bottom of the caller stack frame, respectively. Both registers need to be updated to point to the top and bottom of a new stack frame for the callee. When the function returns the old register values need to be restored, so that the rest of the caller function can execute. There are 11 steps to calling an x86 function and returning:<br><br>
            <img id="small_image" src="../assets/cs161/callercallee1.jpeg" alt=""><br><br>
            1. Push arguments onto the stack. RISC-V passes arguments by storing them in registers, but x86 passes arguments by pushing them onto the stack. Note that esp is decrements as we push arguments onto the stack. Arguments are pushed onto the stack in reverse order. <br>
            <img id="small_image" src="../assets/cs161/callercallee2.jpeg" alt=""><br><br>
            2. Push the old eip(rip) on the stack. Before changing the value of the eip register its needed to save the current value on the stack. When the eip is pushed to the stack its is called the old eip or the rip(return instruction pointer).<br>
            <img id="small_image" src="../assets/cs161/callercallee3.jpeg" alt=""><br><br>
            3. Move eip. Now that we've saved the old value of eip, we can safely change eip to point to the instructions for the callee function.<br>
            <img id="small_image" src="../assets/cs161/callercallee4.jpeg" alt=""><br><br>
            4. Push the old ebp(sfp) on the stack. Before changing the value in the ebp register, its needed to save the current value on the stack. Push the current ebp is pushed onto the stack its referred to as the old ebp or the sfp(saved frame pointer). Not that esp has been decremented because of pushing the ebp onto the stack.<br>
            <img id="small_image" src="../assets/cs161/callercallee5.jpeg" alt=""><br><br>
            5. Move the ebp down. Now that we've saved the old value of ebp, we can safely change ebp to point to the top of the new stack frame. The top of the new stack frame is where esp is currently points since we are about to allocate new space below esp for the new stack frame.<br>
            <img id="small_image" src="../assets/cs161/callercallee6.jpeg" alt=""><br><br>
            6. Move esp down. Now we can allocate new space for the new stack frame by decrementing esp. The compiler looks at the complexity of the function to determine how far esp should be decremented. For example, a function with only a few local variables doesn't require too much space on the stack, so esp will only be decremented by a few bytes.<br>
            <img id="small_image" src="../assets/cs161/callercallee7.jpeg" alt=""><br><br>
            7. Execute the function. Local variables and any other necessary data can now be saved in the new stack frame. Additionally, since ebp is always pointing to the top of the stack frame, we can use it as a point of reference to find other variables on the stack. For example, the arguments will be located starting at the address stored in ebp, plus 8.<br>
            <img id="small_image" src="../assets/cs161/callercallee8.jpeg" alt=""><br><br>
            8. Move esp up. Once the function is ready to return, we increment esp to point to the top of the stack frame(ebp). This effectively erases the stack frame, since the stack frame is now located below esp. (Anything on the stack below the esp is undefined)<br>
            <img id="small_image" src="../assets/cs161/callercallee9.jpeg" alt=""><br><br>
            9. Restore the old ebp(sfp). The next value on the stack is the sfp, the old value of the ebp before we started executing the function. We pop the sfp off the stack and store it back into the ebp register. This returns ebp to its old value before the callee function was called.<br>
            <img id="small_image" src="../assets/cs161/callercallee10.jpeg" alt=""><br><br>
            10. Restore the old eip(rip). The next value on the stack is the rip, the old value of eip before we started executing the function. we pop the rip off the stack and store it back into the register.<br>
            <img id="small_image" src="../assets/cs161/callercallee11.jpeg" alt=""><br><br>
            11. Remove arguments from the stack. Since the functin call is over, we don't need to store the arguments anymore. We can remove them by incrementing esp. <br>
            <img id="small_image" src="../assets/cs161/callercallee12.jpeg" alt=""><br><br>
        </p>

        <h3 id="FP4">Memory Safety Vulnerabilities</h3>
        <h4 id="subhead">Buffer overflow vulnerabilities:</h4> 
        <p id="subtext">
            Buffer overflow vulnerabilities are a particular risk in C, and since C is an especially widely used systems programming language, you might not be surprised to hear that buffer overflows are one of the post pervasive kind of implementation flaws around. C is a low-level language, meaning that the programmer is always exposed to the bare machine, one of the reasons why C is such a popular systems language. A particular weakness in C is the absence of automatic bounds-checking for array or pointer accesses. For example, if the programmer declares an array char buffer[4], C will not automatically throw an error if the programmer tries to access buffer[5]. It is through this absence of automatic bounds-checking that buffer overflows take advantage of. A buffer overflow bug is one where the programmer fails to perform adequate bounds chekcs, triggering an out-of-bounds memory access that writes beyond the bounds of some meory region. Attackers can use these out-of-bounds memory accesses to corrupt the program's intended behavior.<br> 
            Malicious code injection attack uses a buffer overflow to override a function pointer to point to malicious code. Malicious code injection attacks allows an attacker to seize control of the program. At the conclusion of the attack, the program is still running, but now it is executing code chosen by the attackers, rather than the original code. This type of attack are only possible when the code satisfies special conditions: the buffer that can be overflowed must be followed in memory by some security-critical data. Typically attacking the heap memory.<br>
            Stack smashing takes advantage of the way local variables are laid out on the stack. Stack smashing attacks exploit the x86 call convention. An attacker would write past bounds overwriting the sfp and rip. The rip is the pointer to the next instruction to execute, overwriting this allows for again malicious code injection. Suppose the malicious code didn't already exist in memory, and we have to inject it ourselves during the stack smashing attack. Sometimes this malicious code is called shellcode, because the malicious code is often written to spawn an interactive shell that lets the attacker perform arbitrary actions.<br>
            Bottom Line: If your program has a buffer overflow bug, you should assume that the bug is exploitable and an attacker can take control of your program.<br>
        </p>
        <h4 id="subhead">Format String Vulnerabilities:</h4> 
        <p id="subtext">
            When the printf() function executes, it looks for a format string modifier denoted by a "%" in its first arguments located 4 bytes above the rip of printf(). If it finds the modifier, it then looks 8 bytes above the rip for the "actual" argument. What happens when there is a mismatch in the number of format string modifiers in the first arguments and number of additional arguments? Such as "printf("x has the value %d, y has the value %d, z has the value %d \n", x, y);" The format string asks for 3 arguments but having three "%d" modifiers but the printf() function is only passed 2 additional arguments. The C compiler does not catch this error. As long as at least one additional argument is passed everything looks fine to the C compiler. Thus prinf() simply fetches arguments from the stack according to the number of format modifiers present. In cases of mismatch it will fetch some data from the stack that does not belong to the function call. Similar to how the %d format modifier makes the printf() function print the value located at the expected address, various string modifiers have different uses: %s->treat the argument as an address and print the string at that address up until the first null byte, %n->treat the argument as an address and write the number of characters that have been printed so far to that address, %c->treat the argument as a value and print it out as a character, %x look at the stack and read the first variable after the format string, %[b]u->print out [b] bytes starting from the argument.<br>
            Bottom Line: if your program has a format string vulnerability assume that the attacker cal learn any value stored in memory adn can take control of your program.<br>
        </p>
        <h4 id="subhead">Integer Conversion Vulnerabilities:</h4> 
        <p id="subtext">
            <img id="small_image" src="../assets/cs161/numberconversionvulnerabilities.jpg" alt=""><br><br>
            If the attacker provides a negative value for len, the if statement won't notice anything wrong and memcpy() will be executed with a negative third argument. C will cast this negative value to an unsigned int and it will become a very large positive integer. Thus memcpy() will copy a huge amount of memory into buf, overflowing the buffer.Note the C compiler won't warn about the type mismatch between the signed int and unsigned int; it silently inserts an implicit cast. This kind of bug can be hard to spot, because on the surface it appears that the programmer has applied the correct bounds checks, but they are flawed.<br>
            <img id="small_image" src="../assets/cs161/numberconversionvulnerabilites2.jpg" alt=""><br><br>
            This code seems to avoid buffer overflow problems(indeed it allocates 5 more bytes than necessary). But, there is a subtle problem: len+5 can wrap around if len is too large. For instance, if len = 0xFFFFFFFF, then the value of len+5 is 4(on 42-bit platforms). In this case, the code allocates a 4-byte buffer and then writes a lot more than 4 bytes into it: a classic buffer overflow.<br>
        </p>
        <h4 id="subhead">Off-by_one Vulnerabilities:</h4> 
        <p id="subtext">
            Off-by-one errors are very common in programming: for example, you might accidentally use <= instead of <, or you might accidentally start a loop at i=0 instead of i=1. As it turns out, even an off-by-one error can lead to dangerous memory safety vulnerabilities. Consider a buffer whose bounds checks are off by one.<br>
            <img id="small_image" src="../assets/cs161/offbyonevulnerability.jpg" alt=""><br><br>
            Step 1: This is what normal execution during a function looks like. The stack has the rip(saved eip), sfp(saved ebp), and the local variable buff. The esp register points to the bottom of the stack. The ebp register points to the sfp at the top of the stack. The sfp(saved ebp) points ot the ebp of the previous function, which is higher up in memory. The rip(saved eip) points to somewhere in the code section.<br>
            Step 2: We overwrite all of buff, plus the byte immediately after buff, which is the least significant byte of the sfp directly above buff. We can change the last byte of sfp so that the sfp points to somewhere inside buff. The sfp label becomes fsp here to indicate that it is now a forged sfp with the last byte changed. Eventually after your function finishes executing it returns and executes: mov %ebp %esp = change the esp register to point to wherever ebp is currently point to, pop %ebp = take the next value on the stack and place it in the ebp register and move esp up by 4 to delete this value off the stack, pop %eip = take the next value on the stack and place it in the eip register and move the exp up by 4 to delete this value off the stack.<br> 
            Step 3: mov %ebp, %esp- esp now points where ebp is point which is the forged sfp.<br>
            Step 4: pop %ebp take the next value on the stack, the forged sfp, and place it in the ebp register. Now ebp is point inside the buffer.<br>
            Step 5: pop %eip take the next value on the stack, the rip, and place it in the eip register. Since we didn't maliciously change the rip, the old eip is correctly restored.<br>
            After step 5, nothing has changed exept that the ebp now points inside the buffer, This make sense: we only changed the sfp so when ebp is restored it will point to where the forged sfp was pointing(inside buffer). The key insight for this exploit is that one function return is not enough. However, eventually if a second function return happens, it will allow us to start executing instructions at an arbitrary location. <br>
            Step 6: move %ebp, %esp esp now points where ebp is pointing which is inside the buffer. At this point in normal execution both ebp and esp think that they are pointing at the sfp.<br>
            Step 7: pop %ebp take the next value on the stack(which the program thinks is the sfp but is actually some attacker-controlled value inside the buffer) and place it in the ebp register. The question mark here says that even though the attacker controls what gets placed in the ebp register, we don't care what the value actually is.<br>
            Step 8: pop %eip take the next value on the stack(which the program thinks is the rip but is actually some attacker-controlled value inside the buffer) and place it in the eip register. This is where you place the address of shellcode, since you control the values in buff, and the program is taking an address from buff and jumping there is execute instructions. Also, note that it is not enough to place the shellcode 4 bytes above where the forged sfp is points, You need to put the address of shellcode there, since the program will interpret that part of memory as the rip.<br>
        </p>
        <h4 id="subhead">Other Memory Safety Vulnerabilities:</h4> 
        <p id="subtext">
            Buffer overflows, format string vulnerabilities, and other examples above are examples of memory safety bugs: cases where an attacker can read or write beyond the valid range of memory regions. Other examples of memory safety violations include using a dangling pointer(a pointer into a memory region that has been freed and is no longer valid) and double-free bugs(where a dynamically allocated object is explicitly freed multiple times). "Use after free" bugs, where an object or structure in memory is deallocated but still used. are particularly attractive targets for exploitation. Exploiting these vulnerabilities generally involve the attacker triggering the creation of two separate objects that, because of the use-after-free on the first object, actually share the same memory. The attacker can now use the second object to manipulate the interpretation of the first object. <br>
        </p>



</body>
</html>