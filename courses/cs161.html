<!DOCTYPE html>
<html>
    <head>
        <title>Hi</title>
        <link href="../css/styles.css" rel="stylesheet" type="text/css">
      </head>
<body>
    <h2>Computer Science 161: Computer Security</h2>
    <a href="../index.html">Home</a>
    <div id="toc_container">
        <p class="toc_title">Content:</p>
        <ul class="toc_list">
        <li><a href="#FP1">Overview</a></li>
        <li><a href="#FP2">Security Principles</a></li>
        <li><a href="#FP3">x86 Assembly and Call Stack</a></li>
        <li><a href="#FP4">Memory Safety Vulnerabilities</a></li>
        <li><a href="#FP5">Mitigating Memory-Safety Vulnerabilities</a></li>


        </ul>
    </div>
        <h3 id="FP1">Overview</h3> 
        <p id="subtext">
            1. Security Principles<br>
            2. Memory Safety: x86 Assembly and Call Stack, Memory Safety Vulnerabilities, Mitigating Memory-Safety Vulnerabilities<br>
            3. Cryptography: <br>
            4. Web Security: <br>
            5. Network Security: <br>

        </p>

        <h3 id="FP2">Security Principles</h3>
        <p id="subtext">
            Summary: Always know your threat model-- the who and what. Consider human factors in security implementations-- keep tools fool proof and user friendly. No system is ever 100% secure, its a matter of resources. Don't put a 100 dollar lock on a dollar item, and vice versa. If an attack is inevitable always at least have detection of said attack when prevention is not possible. Layer defenses in depth. Always limit access to least privilege, give enough access to get the job done. Split up privilege so no one party has complete access. Takes multiple to launch the nuke. Ensure mediation, check all access points in and out. Never rely on obscurity for security-- the deets always get out. Use fail safe defaults-- meaning when a fail happens it better default to a safe space. Design security from the start don't try to back track. The TCB, the portion of the system that must operate correctly in order for the security goals of the system to be assured. Keep the principle of Time-of-Check To Time-Of-Use in mind.<br>
        </p>
        <h4 id="subhead">Know your threat model:</h4>
        <p id="subtext">
            Threat Model: a model of who your attacker is and what resources they have.<br>
            Commone Assumptions that are taken into account for attackers:<br>
            <p id="subtext_bullet">
                The attacker can inter with your systems without anyone noticing.<br>
                The attacker has some general information about your system.<br>
                The attacker is persistent and lucky.<br>
                The attacker has the resources required to undertake the attack.<br>
                The attacker can coordinate several complex attacks across various systems.<br>
                Every system is a potential target.<br>
            </p>"
        </p>
        <h4 id="subhead">Consider Human Factors:</h4>
        <p id="subtext">
            Security systems must be usable by ordinary people and therefore must be designed to take into account the role that humans will play. <br>
            Takeaway: consider the tools that are presented to users, and try to make them fool-proof and as user-friendly as possible.<br>
        </p>
        <h4 id="subhead">Security is Economics:</h4>
        <p id="subtext">
            Security is often a cost-benefit analysis where someone needs to make a decision regarding how much security is worth.<br>
            A corollary of this principle is you should focus your energy on securing the weakest links. Security is like a chain: a system is only as secure as the weakest link. Attackers follow the path of least resistance, and they will attack the system at its weakest point.<br>
            A closely related principle is conservative design, which states that systems should be evaluated according to the worst security failure that is at all plausible, under assumptions favorable to the attacker.<br>
        </p>
        <h4 id="subhead">Detect if you can't Prevent:</h4>
        <p id="subtext">
            If prevention is stopping an attack from taking place, detection is simply learning that the attack has taken place, and response would be doing something about the attack. The idea is that if you cannot prevent the attack from happening, you should at least be able to know that the attack has happened. Once you know that the attack has happened, you should find a way to respond, since detection without response is pointless.<br>
            When dealing with response, you should always assume that bad things will happen, and therefore prepare your systems for the worst case outcome.<br>
        </p>
        <h4 id="subhead">Defense in depth:</h4>
        <p id="subtext">
            Defense in Depth: defenses should be layered together so an attacker would have to breach all the defenses to successfully attack a system.<br>
            Beware of diminishing returns–if you’ve already built 100 walls, the 101st wall may not add enough additional protection to justify the cost of building it (security is economics).<br>
        </p>
        <h4 id="subhead">Least Privilege:</h4>
        <p id="subtext">
            Give a program the set of access privileges that it legitimately needs to do its job—but nothing more. Try to minimize how much privilege you give each program and system component.<br>
            Least privilege is an enormously powerful approach. It doesn’t reduce the probability of failure, but it can reduce the expected cost of failures. The less privilege that a program has, the less harm it can do if it goes awry or becomes subverted.<br>
        </p>
        <h4 id="subhead">Separation of Responsibility:</h4>
        <p id="subtext">
            Split up privilege, so no one person or program has complete power. Require more than one party to approve before access is granted.<br>
            In summary, if you need to perform a privileged action, require multiple parties to work together to exercise that privilege, since it is more likely for a single party to be malicious than for all of the parties to be malicious and collude with one another.<br>
        </p>
        <h4 id="subhead">Ensure Complete Mediation:</h4>
        <p id="subtext">
            When enforcing access control policies, make sure that you check every access to every object. This kind of thinking is helpful to detect where vulnerabilities could be. As such, you have to ensure that all access is monitored and protected. One way to accomplish this is through a reference monitor, which is a single point through which all access must occur.<br>
        </p>
        <h4 id="subhead">Shannon's Maxim:</h4>
        <p id="subtext">
            Shannon’s Maxim states that the attacker knows the system that they are attacking.<br>
            “Security through obscurity” refers to systems that rely on the secrecy of their design, algorithms, or source code to be secure. The issue with this, however, is that it is extremely brittle and it is often difficult to keep the design of a system secret from a sufficiently motivated attacker. Historically, security through obscurity has a lousy track record: many systems that have relied upon the secrecy of their code or design for security have failed miserably.<br>
            As such, you should never rely on obscurity as part of your security. Always assume that the attacker knows every detail about the system that you are working with (including its algorithms, hardware, defenses, etc.)<br>
            erckhoff’s Principle, which states that cryptographic systems should remain secure even when the attacker knows all internal details of the system.<br>
        </p>
        <h4 id="subhead">Use Fail-Safe Defaults:</h4>
        <p id="subtext">
            Choose default settings that “fail safe”, balancing security with usability when a system goes down. Ensure that if the security mechanisms fail or crash, they will default to secure behavior, not to insecure behavior.<br>
        </p>
        <h4 id="subhead">Design security in from the start:</h4>
        <p id="subtext">
            Trying to retrofit security to an existing application after it has already been spec’ed, designed, and implemented is usually a very difficult proposition. At that point, you’re stuck with whatever architecture has been chosen, and you don’t have the option of decomposing the system in a way that ensures least privilege, separation of privilege, complete mediation, defense in depth, and other good properties. Backwards compatibility is often particularly painful, because you can be stuck with supporting the worst insecurities of all previous versions of the software.<br>
        </p>
        <h4 id="subhead">The Trusted Computing Base (TCB):</h4>
        <p id="subtext">
            In any system, the trusted computing base (TCB) is that portion of the system that must operate correctly in order for the security goals of the system to be assured. We have to rely on every component in the TCB to work correctly. However, anything that is outside the TCB isn’t relied upon in any way; even if it misbehaves or operates maliciously, it cannot defeat the system’s security goals. Generally, the TCB is made to be as small as possible since a smaller, simpler TCB is easier to write and audit.<br>
            TCB Design Principles:<br>
            <p id="subtext">
                Unbypassable: there must be no way to breach system security by bypassing the TCB.<br>
                Tamper-resistant: the TCB should be protected from tampering by anyone else. <br>
                Verifiable: It should be possible to verify the correctness of the TCB.<br>
            </p>
        </p>
        <p id="subtext">
            Design your system so that as much code as possible can be moved outside the TCB.<br>
            Benefits of TCBs: The notion of a TCB is a very powerful and pragmatic one as it allows a primitive yet effective form of modularity. It lets us separate the system into two parts: the part that is security-critical (the TCB), and everything else.<br>
        </p>s
        <h4 id="subhead">TOCTTOU Vulnerabilities:</h4>
        <p id="subtext">
            This is known as a Time-Of-Check To Time-Of-Use (TOCTTOU) vulnerability, because between the check and the use of whatever state was checked, the state somehow changed.<br>
        </p>

        <h3 id="FP3">x86 Assembly and Call Stack</h3>
        <h4 id="subhead">Number Representation:</h4>
        <p id="subtext">
            At the lowest level, computers store memory as individual bits, where each bit is either 0 or 1. <br>
            1 nibble = 4 bits<br>
            1 byte = 8 bits<br>
            1 word = 32 bits(on 32-bit architecture)<br>
            A "word" is the size of a pointer, which depends on your CPU architecture. 
        </p>
        <h4 id="subhead">Call Stack:</h4>
        <p id="subtext">
            The compiler translates your C code into assembly instructions. 61c uses the RISC-V instruction set but in 161 we use x86, which is more commonly seen in the real world.<br>
            The assembler translates the assembly instructions from the compiler into machine code.<br>
            The linker resolves dependencies on external libraries. After the linker finishes linking external libraries, it outputs a binary executable of the program that you can run. <br>
            The user runs the executable, the loader sets up an address space in memory and runs the machine code instructions in the executable.<br>
        </p>
        <h4 id="subhead">C memory layout:</h4> 
        <p id="subtext">
            At runtime, the OS gives the program an address space to store any state necessary for program execution. Each byte has a unique address. The size of the address space depends on the OS and CPU architecture. In a 32 bit system address are 32 bits long, which means the address space has 2^32 bytes of memory.<br>
            <img id="small_image" src="../assets/cs161/memorylayout.jpeg" alt=""><br><br>
            The code section contains executable instructions of the program. The assembler and linker output raw bytes that can be interpreted as machine code. These bytes are stored in the code section.<br>
            The static section contains constants and static variables that never change during program execution, and are usually allocated when the program starts.<br>
            The heap stores dynamically allocated data. When malloc is called in C, memory is allocated on the heap and persists until free is called. The heap starts at lower addresses and "grows up" to higher addresses as more memory is allocated.<br>
            The stack stores local variables and other information associated with function calls. The stack starts at higher addresses and "grows down" as more functions are called.<br>
            x86 is a Little Endian system this means when storing a word in memory the least significant byte is stored as the lowest address, and the most significant byte is stored at the highest address.<br>
        </p>
        <h4 id="subhead">Registers:</h4> 
        <p id="subtext">
            In addition to teh 2^32 bytes of memory in the address space, there are also registers, which store memory directly on the CPU. Each register can store one word. Unlike memory registers do not have addresses. Instead, registers are referred to by names. There are three special x86 registers that are relevant:
            <p id="subtext_bullet">
                eip: the instruction pointer, stores that address fo teh machine instruction currently being executed. IN RISC-V this register is called the PC.<br>
                ebp: the base pointer, stores the address of the top of the current stack frame. IN RISC-V systems this register is called the FP. <br>
                esp: the stack pointer, stores the address of the bottom of the current stack frame. In RISC-V this register is called the SP.<br>
                eax and ebx are general purpose registers in x86<br>
                Note: The eip register points to the code section of memory while the ebp and esp registers typically point to stack memory.<br>
            </p>
        </p>
        <h4 id="subhead">Stack--Pushing and Popping:</h4> 
        <p id="subtext">
            When it is desirable to save a variable on the stack there are two steps to take. First allocated additional space on the stack by decrementing the esp. Next store the value in the newly allocated space. The x86 push instruction does both of these steps to add a value to the stack. To remove a value from the stack increments the esp register, in x86 the pop instruction does this. It also takes the value that was just popped and copies the value to a register. Note when we pop a value off the stack the bits of memory aren't removed but the memory space becomes undefined.<br>
        </p>
        <h4 id="subhead">x86 calling convention:</h4> 
        <p id="subtext">
            This class uses AT&T x86 syntax(since that is what GDB uses). This means that the destination register comes last; note that this is in contrast with RISc_v assembly where the destination register comes first. Suppose our assembly instruction was addl $0x*, %ebx; here, the opcode is addl, the source is $0x8, and the destination register is %ebx, so in pseudocode this can be read as EBX = EBX + 0x8.<br>
            References to registers are preceded with a percent sign, so if we wanted to reference eax, we would do so as %eax. Immediates are preceded with a dollar sign(i.e. $1, $0x4, etc.). Furthermore, memory references use parenthesis and can have immediate offsets; for example. 12(%esp) dereferences memory 12 bytes above the address contained in ESP. If parenthesis are used without an immediate offset, the offset can be thought of as an implicit 0.<br>
        </p>
        <h4 id="subhead">x86 function calls:</h4> 
        <p id="subtext">
            When a function is called, the stack allocates extra space to store local variables and other information relevant to that function. The stack grows down, so this extra space will be at lower addresses in memory. Once the function returns, the space on the stack is freed up for future function calls. In a function call, the caller calls the callee. Program execution starts in the caller, moves to the callee as a result of the function call and then returns to the caller after the function call completes.<br>
            When a function call is made in x86 the three special registers-- eip, evp, esp --need to be updated. The eip needs to be changed to point to the instructions of the callee. The ebp and esp currently point to the top and bottom of the caller stack frame, respectively. Both registers need to be updated to point to the top and bottom of a new stack frame for the callee. When the function returns the old register values need to be restored, so that the rest of the caller function can execute. There are 11 steps to calling an x86 function and returning:<br><br>
            <img id="small_image" src="../assets/cs161/callercallee1.jpeg" alt=""><br><br>
            1. Push arguments onto the stack. RISC-V passes arguments by storing them in registers, but x86 passes arguments by pushing them onto the stack. Note that esp is decrements as we push arguments onto the stack. Arguments are pushed onto the stack in reverse order. <br>
            <img id="small_image" src="../assets/cs161/callercallee2.jpeg" alt=""><br><br>
            2. Push the old eip(rip) on the stack. Before changing the value of the eip register its needed to save the current value on the stack. When the eip is pushed to the stack its is called the old eip or the rip(return instruction pointer).<br>
            <img id="small_image" src="../assets/cs161/callercallee3.jpeg" alt=""><br><br>
            3. Move eip. Now that we've saved the old value of eip, we can safely change eip to point to the instructions for the callee function.<br>
            <img id="small_image" src="../assets/cs161/callercallee4.jpeg" alt=""><br><br>
            4. Push the old ebp(sfp) on the stack. Before changing the value in the ebp register, its needed to save the current value on the stack. Push the current ebp is pushed onto the stack its referred to as the old ebp or the sfp(saved frame pointer). Not that esp has been decremented because of pushing the ebp onto the stack.<br>
            <img id="small_image" src="../assets/cs161/callercallee5.jpeg" alt=""><br><br>
            5. Move the ebp down. Now that we've saved the old value of ebp, we can safely change ebp to point to the top of the new stack frame. The top of the new stack frame is where esp is currently points since we are about to allocate new space below esp for the new stack frame.<br>
            <img id="small_image" src="../assets/cs161/callercallee6.jpeg" alt=""><br><br>
            6. Move esp down. Now we can allocate new space for the new stack frame by decrementing esp. The compiler looks at the complexity of the function to determine how far esp should be decremented. For example, a function with only a few local variables doesn't require too much space on the stack, so esp will only be decremented by a few bytes.<br>
            <img id="small_image" src="../assets/cs161/callercallee7.jpeg" alt=""><br><br>
            7. Execute the function. Local variables and any other necessary data can now be saved in the new stack frame. Additionally, since ebp is always pointing to the top of the stack frame, we can use it as a point of reference to find other variables on the stack. For example, the arguments will be located starting at the address stored in ebp, plus 8.<br>
            <img id="small_image" src="../assets/cs161/callercallee8.jpeg" alt=""><br><br>
            8. Move esp up. Once the function is ready to return, we increment esp to point to the top of the stack frame(ebp). This effectively erases the stack frame, since the stack frame is now located below esp. (Anything on the stack below the esp is undefined)<br>
            <img id="small_image" src="../assets/cs161/callercallee9.jpeg" alt=""><br><br>
            9. Restore the old ebp(sfp). The next value on the stack is the sfp, the old value of the ebp before we started executing the function. We pop the sfp off the stack and store it back into the ebp register. This returns ebp to its old value before the callee function was called.<br>
            <img id="small_image" src="../assets/cs161/callercallee10.jpeg" alt=""><br><br>
            10. Restore the old eip(rip). The next value on the stack is the rip, the old value of eip before we started executing the function. we pop the rip off the stack and store it back into the register.<br>
            <img id="small_image" src="../assets/cs161/callercallee11.jpeg" alt=""><br><br>
            11. Remove arguments from the stack. Since the functin call is over, we don't need to store the arguments anymore. We can remove them by incrementing esp. <br>
            <img id="small_image" src="../assets/cs161/callercallee12.jpeg" alt=""><br><br>
        </p>

        <h3 id="FP4">Memory Safety Vulnerabilities</h3>
        <h4 id="subhead">Buffer overflow vulnerabilities:</h4> 
        <p id="subtext">
            Buffer overflow vulnerabilities are a particular risk in C, and since C is an especially widely used systems programming language, you might not be surprised to hear that buffer overflows are one of the post pervasive kind of implementation flaws around. C is a low-level language, meaning that the programmer is always exposed to the bare machine, one of the reasons why C is such a popular systems language. A particular weakness in C is the absence of automatic bounds-checking for array or pointer accesses. For example, if the programmer declares an array char buffer[4], C will not automatically throw an error if the programmer tries to access buffer[5]. It is through this absence of automatic bounds-checking that buffer overflows take advantage of. A buffer overflow bug is one where the programmer fails to perform adequate bounds chekcs, triggering an out-of-bounds memory access that writes beyond the bounds of some meory region. Attackers can use these out-of-bounds memory accesses to corrupt the program's intended behavior.<br> 
            Malicious code injection attack uses a buffer overflow to override a function pointer to point to malicious code. Malicious code injection attacks allows an attacker to seize control of the program. At the conclusion of the attack, the program is still running, but now it is executing code chosen by the attackers, rather than the original code. This type of attack are only possible when the code satisfies special conditions: the buffer that can be overflowed must be followed in memory by some security-critical data. Typically attacking the heap memory.<br>
            Stack smashing takes advantage of the way local variables are laid out on the stack. Stack smashing attacks exploit the x86 call convention. An attacker would write past bounds overwriting the sfp and rip. The rip is the pointer to the next instruction to execute, overwriting this allows for again malicious code injection. Suppose the malicious code didn't already exist in memory, and we have to inject it ourselves during the stack smashing attack. Sometimes this malicious code is called shellcode, because the malicious code is often written to spawn an interactive shell that lets the attacker perform arbitrary actions.<br>
            Bottom Line: If your program has a buffer overflow bug, you should assume that the bug is exploitable and an attacker can take control of your program.<br>
        </p>
        <h4 id="subhead">Format String Vulnerabilities:</h4> 
        <p id="subtext">
            When the printf() function executes, it looks for a format string modifier denoted by a "%" in its first arguments located 4 bytes above the rip of printf(). If it finds the modifier, it then looks 8 bytes above the rip for the "actual" argument. What happens when there is a mismatch in the number of format string modifiers in the first arguments and number of additional arguments? Such as "printf("x has the value %d, y has the value %d, z has the value %d \n", x, y);" The format string asks for 3 arguments but having three "%d" modifiers but the printf() function is only passed 2 additional arguments. The C compiler does not catch this error. As long as at least one additional argument is passed everything looks fine to the C compiler. Thus prinf() simply fetches arguments from the stack according to the number of format modifiers present. In cases of mismatch it will fetch some data from the stack that does not belong to the function call. Similar to how the %d format modifier makes the printf() function print the value located at the expected address, various string modifiers have different uses: %s->treat the argument as an address and print the string at that address up until the first null byte, %n->treat the argument as an address and write the number of characters that have been printed so far to that address, %c->treat the argument as a value and print it out as a character, %x look at the stack and read the first variable after the format string, %[b]u->print out [b] bytes starting from the argument.<br>
            Bottom Line: if your program has a format string vulnerability assume that the attacker cal learn any value stored in memory adn can take control of your program.<br>
        </p>
        <h4 id="subhead">Integer Conversion Vulnerabilities:</h4> 
        <p id="subtext">
            <img id="small_image" src="../assets/cs161/numberconversionvulnerabilities.jpg" alt=""><br><br>
            If the attacker provides a negative value for len, the if statement won't notice anything wrong and memcpy() will be executed with a negative third argument. C will cast this negative value to an unsigned int and it will become a very large positive integer. Thus memcpy() will copy a huge amount of memory into buf, overflowing the buffer.Note the C compiler won't warn about the type mismatch between the signed int and unsigned int; it silently inserts an implicit cast. This kind of bug can be hard to spot, because on the surface it appears that the programmer has applied the correct bounds checks, but they are flawed.<br>
            <img id="small_image" src="../assets/cs161/numberconversionvulnerabilites2.jpg" alt=""><br><br>
            This code seems to avoid buffer overflow problems(indeed it allocates 5 more bytes than necessary). But, there is a subtle problem: len+5 can wrap around if len is too large. For instance, if len = 0xFFFFFFFF, then the value of len+5 is 4(on 42-bit platforms). In this case, the code allocates a 4-byte buffer and then writes a lot more than 4 bytes into it: a classic buffer overflow.<br>
        </p>
        <h4 id="subhead">Off-by_one Vulnerabilities:</h4> 
        <p id="subtext">
            Off-by-one errors are very common in programming: for example, you might accidentally use <= instead of <, or you might accidentally start a loop at i=0 instead of i=1. As it turns out, even an off-by-one error can lead to dangerous memory safety vulnerabilities. Consider a buffer whose bounds checks are off by one.<br>
            <img id="small_image" src="../assets/cs161/offbyonevulnerability.jpg" alt=""><br><br>
            Step 1: This is what normal execution during a function looks like. The stack has the rip(saved eip), sfp(saved ebp), and the local variable buff. The esp register points to the bottom of the stack. The ebp register points to the sfp at the top of the stack. The sfp(saved ebp) points ot the ebp of the previous function, which is higher up in memory. The rip(saved eip) points to somewhere in the code section.<br>
            Step 2: We overwrite all of buff, plus the byte immediately after buff, which is the least significant byte of the sfp directly above buff. We can change the last byte of sfp so that the sfp points to somewhere inside buff. The sfp label becomes fsp here to indicate that it is now a forged sfp with the last byte changed. Eventually after your function finishes executing it returns and executes: mov %ebp %esp = change the esp register to point to wherever ebp is currently point to, pop %ebp = take the next value on the stack and place it in the ebp register and move esp up by 4 to delete this value off the stack, pop %eip = take the next value on the stack and place it in the eip register and move the exp up by 4 to delete this value off the stack.<br> 
            Step 3: mov %ebp, %esp- esp now points where ebp is point which is the forged sfp.<br>
            Step 4: pop %ebp take the next value on the stack, the forged sfp, and place it in the ebp register. Now ebp is point inside the buffer.<br>
            Step 5: pop %eip take the next value on the stack, the rip, and place it in the eip register. Since we didn't maliciously change the rip, the old eip is correctly restored.<br>
            After step 5, nothing has changed exept that the ebp now points inside the buffer, This make sense: we only changed the sfp so when ebp is restored it will point to where the forged sfp was pointing(inside buffer). The key insight for this exploit is that one function return is not enough. However, eventually if a second function return happens, it will allow us to start executing instructions at an arbitrary location. <br>
            Step 6: move %ebp, %esp esp now points where ebp is pointing which is inside the buffer. At this point in normal execution both ebp and esp think that they are pointing at the sfp.<br>
            Step 7: pop %ebp take the next value on the stack(which the program thinks is the sfp but is actually some attacker-controlled value inside the buffer) and place it in the ebp register. The question mark here says that even though the attacker controls what gets placed in the ebp register, we don't care what the value actually is.<br>
            Step 8: pop %eip take the next value on the stack(which the program thinks is the rip but is actually some attacker-controlled value inside the buffer) and place it in the eip register. This is where you place the address of shellcode, since you control the values in buff, and the program is taking an address from buff and jumping there is execute instructions. Also, note that it is not enough to place the shellcode 4 bytes above where the forged sfp is points, You need to put the address of shellcode there, since the program will interpret that part of memory as the rip.<br>
        </p>
        <h4 id="subhead">Other Memory Safety Vulnerabilities:</h4> 
        <p id="subtext">
            Buffer overflows, format string vulnerabilities, and other examples above are examples of memory safety bugs: cases where an attacker can read or write beyond the valid range of memory regions. Other examples of memory safety violations include using a dangling pointer(a pointer into a memory region that has been freed and is no longer valid) and double-free bugs(where a dynamically allocated object is explicitly freed multiple times). "Use after free" bugs, where an object or structure in memory is deallocated but still used. are particularly attractive targets for exploitation. Exploiting these vulnerabilities generally involve the attacker triggering the creation of two separate objects that, because of the use-after-free on the first object, actually share the same memory. The attacker can now use the second object to manipulate the interpretation of the first object. <br>
        </p>
        <p id="subtext">
            Vulnerabilities Overview:<br><br>
            <img id="small_image" src="../assets/cs161/stacksmashing.jpg" alt="">
            <img id="small_image" src="../assets/cs161/integerconversionattack.jpg" alt=""><br>
            <img id="small_image" src="../assets/cs161/integeroverflowvulnerability.jpg" alt="">
            <img id="small_image" src="../assets/cs161/formatstringattack.jpg" alt=""><br>
            <img id="small_image" src="../assets/cs161/offbyoneattack.jpg" alt="">
            <img id="small_image" src="../assets/cs161/ret2espattack.jpg" alt="">
        </p>

        <h3 id="FP4">Memory Safety Vulnerabilities</h3>
        <h4 id="subhead">Use Memory-Safe Languages:</h4> 
        <p id="subtext">
            Some modern languages are designed to be intrinsically memory-safe, no matter what the programmer does. Java, Python, Go, Rust, Swift, and many other programming languages include a combination of compile-time and runtime checks that prevent memory errors from occurring. Using a memory safe language is the only way to stop 100% of memory safety vulnerabilities. In an ideal world, everyone would program in memory-safe languages and buffer overflow vulnerabilities would no longer exist. However, because of legacy code and perceived1 performance concerns, memory-unsafe languages such as C are still prevalent today.<br>
        </p>
        <h4 id="subhead">Writing Memory-Safe code:</h4> 
        <p id="subtext">
            One way to ensure memory safety is to carefully reason about memory accesses in your code, by defining pre-conditions and post-conditions for every function you write and using invariants to prove that these conditions are satisfied. Although it is a good skill to have, this process is painstakingly tedious and rarely used in practice, so it is no longer in scope for this class.<br>
            Another example of defending against memory safety vulnerabilities is writing memory-safe code through defensive programming and using safe libraries. Defensive programming is very similar to defining pre and post conditions for every written function, wherein you always add checks in your code just in case something could go wrong. For example, you would always check that a pointer is not null before dereferencing it, even if you are sure that the pointer is always going to be valid. However, as mentioned earlier, this relies a lot on programmer discipline and is very tedious to properly implement. As such, a more common method is to use safe libraries, which, in turn, use functions that check bounds so you don’t have to. For example, using fgets instead of gets, strncpy or strlcpy instead of strcpy, and snprintf instead of sprintf, are all steps towards making your code slightly more safe.<br>
        </p>
        <h4 id="subhead">Building Secure Software:</h4> 
        <p id="subtext">
            Yet another way to defend your code is to use tools to analyze and patch insecure code. Utilizing run-time checks that do automatic bound-checking, for example is an excellent way to help your code stay safe. If your check fails, you can direct it towards a controlled crash, ensuring that the attacker does not succeed. Hiring someone to look over your code for memory safety errors, though expensive, can prove to be extremely beneficial as well. You can also probe your own system for vulnerabilities, by subjecting your code to thorough tests. Fuzz testing, or testing with random inputs, testing corner cases, and using tools like Valgrind (to detect memory leaks), are all excellent ways to help test your code. Though it is pretty difficult to know whether you have tested your code “enough” to deem it safe, there are several code-coverage tools that can help you out.<br>
        </p>
        <h4 id="subhead">Exploit Mitigations:</h4> 
        <p id="subtext">
            Sometimes you might be forced to program in a memory-unsafe language, and you cannot reason about every memory access in your code. For example, you might be asked to update an existing C codebase that is so large that you cannot go through and reason about every memory access. In these situations, a good strategy is to compile and run code with code hardening defenses to make common exploits more difficult.<br>
            Code hardening defenses are mitigations: they try to make common exploits harder and cause exploits to crash instead of succeeding, but they are not foolproof. The only way to prevent all memory safety exploits is to use a memory-safe language. Instead, these mitigations are best thought of as defense-in-depth: they cannot prevent all attacks, but by including many different defenses in your code, you can prevent more attacks. Over the years, there has been a back-and-forth arms race between security researchers developing new defenses and attackers developing new ways to subvert those defenses.<br>
            The rest of this section goes into more detail about some commonly-used code hardening defenses, and techniques for subverting those defenses. In many cases, using multiple mitigations produces a synergistic effect: one mitigation on its own can be bypassed, but a combination of multiple mitigations forces an attacker to discover multiple vulnerabilities in the target program.<br>
        </p>
        <h4 id="subhead">Mitigation: Non-executable pages:</h4> 
        <p id="subtext">
            Many common buffer overflow exploits involve the attacker writing some machine code into memory, and then redirecting the program to execute that injected code. For example, one of the stack smashing attacks in the previous section ([shellcode] + [4 bytes of garbage] + [address of buf]) involves the attacker writing machine code into memory and overwriting the rip to cause the program to execute that code.<br>
            One way to defend against this category of attacks is to make some portions of memory non-executable. What this means is that the computer should not interpret any data in these regions as CPU instructions. You can also think of it as not allowing the eip to ever contain the address of a non-executable part of memory.<br>
            Modern systems separate memory into pages in order to support virtual memory (see 61C or 162 to learn more). To defend against memory safety exploits, each page of memory is set to either be writable or executable, but not both. If the user can write to a page in memory, then that page of memory cannot be interpreted as machine instructions. If the program can execute a page of memory as machine instructions, then the user cannot write to that page.<br>
            This defense stops the stack smashing attack in the previous section where the attacker wrote machine code into memory. Because the attacker wrote machine code to a page in memory, that page cannot be executed as machine instructions, so the attack no longer works.<br>
            This defense has several names in practice, including W\^X (Write XOR Execute), DEP (Data Execution Prevention), and the NX bit (no-execute bit).<br>
        </p>
        <h4 id="subhead">Subverting non-executable pages--Return into libc:</h4>
        <p id="subtext">
            Non-executable pages do not stop an attacker from executing existing code in memory. Most C programs import libraries with thousands or even million lines of instructions. All of these instructions are marked as executable (and non-writable), since the programmer may want to call these functions legitimately.<br>
            An attacker can exploit this by overwriting the rip with the address of a C library function. For example, the execv function lets the attacker start executing the instructions of some other executable.<br>
            Some of these library functions may take arguments. For example, execv takes a string with the filename of the program to execute. Recall that in x86, arguments are passed on the stack. This means that an attacker can carefully place the desired arguments to the library function in the right place on the stack, so that when the library function starts to execute, it will look on the stack for arguments and find the malicious argument placed there by the attacker. The argument is not being run as code, so non-executable pages will not stop this attack.<br>
        </p>
        <h4 id="subhead">Subverting non-executable pages: Return-oriented programming:</h4>
        <p id="subtext">
            We can take this idea of returning to already-loaded code and extend it further to now execute arbitrary code. Return-oriented programming is a technique that overwrites a chain of return addresses starting at the RIP in order to execute a series of “ROP gadgets” which are equivalent to the desired malicious code. Essentially, we are constructing a custom shellcode using pieces of code that already exist in memory. Instead of executing an existing function, like we did in “Return to libc”, with ROP you can execute your own code by simply executing different pieces of different code. For example, imagine we want to add 4 to the value currently in the EDX register as part of a larger program. In loaded memory, we have the following functions:
            foo:<br>
            ...<br>
                0x4005a1 <foo+33> mov %edx, %eax<br>
                0x4005a3 <foo+35> leave<br>
                0x4005a4 <foo+36> ret<br>
            ...<br>
            bar:<br>
            ...<br>
                0x400604 <bar+20> add $0x4, %eax<br>
                0x400608 <bar+24> pop %ebx<br>
                0x40060a <bar+26> leave<br>
                0x40060b <bar+27> ret<br>
                To emulate the add $0x4, %edx instruction, we could move the value in EDX to EAX using the gadget in foo and then add 4 to EAX using the gadget in bar! If we set the first return address to 0x004005a1 and the second return address to 0x00400604, we produce the desired result. Each time we jump to ROP gadget, we eventually execute the ret instruction and then pop the next return address off the stack, jumping to the next gadget. We just have to keep track that our desired value is now in a different register, and because we execute a pop %ebx instruction in bar before we return, we also have to remember that the value in EBX has been updated after executing these gadgets—but these are all behaviors that we can account for using standard compiler techniques. In fact, so-called “ROP compilers” exist to take an existing vulnerable program and a desired execution flow and generate a series of return addresses.<br>
                The general strategy for executing ROPs is to write a chain of return addresses at the RIP to achieve the behavior that we want. Each return address should point to a gadget, which is a small set of assembly instructions that already exist in memory and usually end in a ret instruction (note that gadgets are not functions, they don’t need to start with a prologue or end with an epilogue!). The gadget then executes its instructions and ends with a ret instruction, which tells the code to jump to the next address on the stack, thus allowing us to jump to the next gadget!<br>
                If the code base is big enough, meaning that the code imports enough libraries, there are usually enough gadgets in memory for you to be able to run any shellcode that you want. In fact, ROP compilers exist on the Internet that will automatically generate an ROP chain for you based on a target binary and desired malicious code! ROP has become so common that non-executable pages are no longer a huge issue for attackers nowadays; while having writable and executable pages makes an attacker’s life easier, not a lot of effort has to be put in to subvert this defense mechanism.<br>
        </p>
        <h4 id="subhead">Mitigation: Stack Canaries:</h4>
        <p id="subtext">
            In the old days, miners would protect themselves against toxic gas buildup in the mine by bringing a caged canary into the mine. These particularly noisy birds are also sensitive to toxic gas. If toxic gas builds up in the mine, the canary dies first, which gives the miners a warning sign that the air is toxic and they should evacuate immediately. The canary in the coal mine is a sacrificial animal: the miners don’t expect it to survive, but its death acts as a warning to save the lives of the miners.<br>
            We can use this same idea to prevent against buffer overflow attacks. When we call a function, the compiler places a known dummy value, the stack canary, on the stack. This canary value is not used by the function at all, so it should stay unchanged throughout the duration of the function. When the function returns, the compiler checks that the canary value has not been changed. If the canary value has changed, then just like the canary in the mine dying, this is evidence that something bad has happened, and the program will crash before any further damage is done.<br>
            Like the canary in the coal mine, the stack canary is a sacrifical value: it has no purpose in the function execution and nothing bad happens if it is changed, but the canary changing acts as a warning that someone may be trying to exploit our program. This warning lets us safely crash the program instead of allowing the exploit to succeed.<br>
            The stack canary uses the fact that many common stack smashing attacks involve overflowing a local variable to overwrite the saved registers (sfp and rip) directly above. These attacks often write to consecutive, increasing addresses in memory, without any gaps. In other words, if the attacker starts writing at a buffer and wants to overwrite the rip, they must overwrite everything in between the buffer and the rip. The stack canary is placed directly above the local variables and directly below the saved registers (sfp and rip):<br>
            Suppose an attacker wants to overflow a local variable to overwrite the rip on the stack, and the vulnerability only allows the attacker to write to consecutive, increasing addresses in memory. Then the attacker must overwrite the stack canary before overwriting the rip, since the rip is located above the buffer in the stack.<br>
            Before the function returns and starts executing instructions at the rip, the compiler will check whether the canary value is unchanged. If the attacker has attempted to overwrite the rip, they will have also changed the canary value. The program will conclude that something bad is happening and crash before the attacker can take control. Note that the stack canary detects an attack before the function returns.<br>
            The stack canary is a random value generated at runtime. The canary is 1 word long, so it is 32 bits long in 32-bit architectures. In Project 1, the canary is 32 completely random bits. However, in reality, stack canaries are usually guaranteed to contain a null byte (usually as the first byte). This lets the canary defend against string-based memory safety exploits, such as vulnerable calls to strcpy that read or write values from the stack until they encounter a null byte. The null byte in the canary stops the strcpy call before it can copy past the canary and affect the rip.<br>
            The canary value changes each time the program is run. If the canary was the same value each time the program was run, then the attacker could run the program once, write down the canary value, then run the program again and overwrite the canary with the correct value. Within a single run of the program, the canary value is usually the same for each function on the stack.<br>
            Modern compilers automatically add stack canary checking when compiling C code. The performance overhead from checking stack canaries is negligible, and they defend against many of the most common exploits, so there is really no reason not to include stack canaries when programming in a memory-unsafe language.<br>
        </p>
        <h4 id="subhead">Subverting: Stack Canaries:</h4>
        <p id="subtext">
            Stack canaries make buffer overflow attacks harder for an attacker, but they do not defend programs against all buffer overflow attacks. There are many exploits that the stack canary cannot detect:<br>
            Stack canaries can’t defend against attacks outside of the stack. For example, stack canaries do nothing to protect vulnerable heap memory.<br>
            Stack canaries don’t stop an attacker from overwriting other local variables. Consider the authenticated example from the previous section. An attacker overflowing a buffer to overwrite the authenticated variable never actually changes the canary value.<br>
            Some exploits can write to non-consecutive parts of memory. For example, format string vulnerabilities let an attacker write directly to the rip without having to overwrite everything between a local variable and the rip. This lets the attacker write "around" the canary and overwrite the rip without changing the value of the canary.<br>
            Additionally, there are several techniques for defeating the stack canary. These usually involve the attacker modifying their exploit to overwrite the canary with its original value. When the program returns, it will see that the canary is unchanged, and the program won’t detect the exploit.<br>
            Guess the canary: On a 32-bit architecture, the stack canary usually only has 24 bits of entropy (randomness), because one of the four bytes is always a null byte. If the attacker runs the program with an exploit, there is a roughly 1 in 2^24 chance that the the value the attacker is overwriting the canary with matches the actual canary value. Although the probability of success is low on one try, the attacker can simply run the program 2^24 times and successfully exploit the program at least once with high probability.<br>
            Depending on the setting, it may be easy or hard to run a program and inject an exploit 2^24 times. If each try takes 1 second, the attacker would need to try for over 100 days before they succeed. If the program is configured to take exponentially longer to run each time the attacker crashes it, the attacker might never be able to try enough times to succeed. However, if the attacker can try thousands of times per second, then the attacker will probably succeed in just a few hours.<br>
            On a 64-bit architecture, the stack canary has 56 bits of randomness, so it is significantly harder to guess the canary value. Even at 1,000 tries per second, an attacker would need over 2 million years on average to guess the canary!<br>
            Leak the canary: Sometimes the program has a vulnerability that allows the attacker to read parts of memory. For example, a format string vulnerability might let the attacker print out values from the stack. An attacker could use this vulnerability to leak the value of the canary, write it down, and then inject an exploit that overwrites the canary with its leaked value. All of this can happen within a single run of the program, so the canary value doesn’t change on program restart.<br>
        </p>
        <h4 id="subhead">Mitigation: Pointer authentication:</h4>
        <p id="subtext">
            As we saw earlier, stack canaries help detect if an attacker has modified the rip or sfp pointers by storing a secret value on the stack and checking if the secret value has been modified. As it turns out, we can generalize this idea of using secrets on the stack to detect when an attacker modifies any pointer on the stack.<br>
            Pointer authentication takes advantage of the fact that in a 64-bit architecture, many bits of the address are unused. A 64-bit address space can support 2^64 bytes, or 18 exabytes of memory, but we are a long way off from having a machine with this much memory. A modern CPU might support a 4 terabyte address space, which means 42 bits are needed to address all of memory. This still leaves 22 unused bits in every address and pointer (the top 22 bits in the address are always 0).<br> 
            Consider using these unused bits to store a secret like the stack canary. Any time we need to store an address on the stack, the CPU first replaces the 22 unused bits with some secret value, known as the pointer authentication code, or PAC, before pushing the value on the stack. When the CPU reads an address off the stack, it will check that the PAC is unchanged. If the PAC is unchanged, then the CPU replaces the PAC with the original unused bits and uses the address normally. However, if the PAC has been changed, this is a warning sign that the attacker has overwritten the address! The CPU notices this and safely crashes the program.<br>
            As an example, suppose the rip of a function in a 64-bit system is 0x0000001234567899. The address space for this architecture is 40 bits, which means the top 24 bits (3 bytes) are always 0 for every address. Instead of pushing this address directly on the stack, the CPU will first replace the 3 unused bytes with a PAC. For example, if the PAC is 0xABCDEF, then the address pushed on the stack is 0xABCDEF1234567899.<br>
            This address (with the secret value inserted) is invalid, and dereferencing it will cause the program to crash. When the function returns and the program needs to start executing instructions at the rip, the CPU will read this address from the stack and check that the PAC 0xABCDEF is unchanged. If the PAC is correct, then the CPU replaces the secret with the original unused bits to make the address valid again. Now the CPU can start executing instructions at the original rip 0x0000001234567899.<br>
            Now, an attacker trying to overwrite the rip would need to know the PAC in order to overwrite the rip with the address of some attacker shellcode. If the attacker overwrites the PAC with an incorrect value, the CPU will detect this and crash the program.<br>
            We can strengthen this defense even further. Since it is the CPU’s job to add and check the PAC, we can ask the CPU to use a different PAC for every pointer stored on the stack. However, we don’t want to store all these PACs on the CPU, so we’ll use some special math to help us generate secure PACs on the fly.<br>
            Consider a special function f ( K E Y , A D D R E S S ) . The function f takes a secret key K E Y and an address A D D R E S S , and outputs a PAC by performing some operation on these two inputs. This function is deterministic, which means if we supply the same key and address twice, it will output the same secret value twice. This function is also secure: an attacker who doesn’t know the value of K E Y cannot output secret values of their own.<br>
            Now, instead of using the same PAC for every address, we can generate a different PAC for each address we store in memory. Every time an address needs to be stored in memory, the CPU runs f with the secret key and the address to generate a unique secret value. Every time an address from memory needs to be dereferenced, the CPU runs f again with the secret key and the address to re-generate the PAC, and checks that the generated value matches the value in memory. The CPU only has to remember the secret key, because all the secret values can be re-generated by running f again with the key and the address.<br>
            Using a different PAC for every address makes this defense extremely strong. An attacker who can write to random parts of memory can defeat the stack canary, but cannot easily defeat pointer authentication: they could try to leave the PAC untouched, but because they’ve changed the address, the old secret value will no longer check out. The CPU will run f on the attacker-generated address, and the output will be different from the old secret value (which was generated by running f on the original address). The attacker also cannot generate the correct secret value for their malicious address, because they don’t know what the secret key is. Finally, an attacker could try to leak some addresses and secret values from memory, but knowing the PACs doesn’t help the attacker generate a valid PAC for their chosen malicious address.<br>
            With pointer authentication enabled, an attacker is never able to overwrite pointers on the stack (including the rip) without generating the corresponding secret for the attacker’s malicious address. Without knowing the key, the attacker is forced to guess the correct secret value for their address. For a 20-bit secret, the attacker has a 1 in 2^20 chance of success.<br>
            Another way to subvert pointer authentication is to find a separate vulnerability in the program that allows the attacker to trick the program into creating a validated pointer. The attacker could also try to discover the secret key stored in the CPU, or find a way to subvert the function f used to generate the secret values.<br>
        </p>
        <h4 id="subhead">Mitigation: Address Space Layout Randomization:</h4>
        <p id="subtext">
            Recall the stack smashing attacks from the previous section, where we overwrote the rip with the address of some malicious code in memory. This required knowing the exact address of the start of the malicious code. ASLR is a mitigation that tries to make predicting addresses in memory more difficult.<br>
            Although we showed that C memory is traditionally arranged with the code section starting at the lowest address and the stack section starting at the highest address, nothing is stopping us from shifting or rearranging the memory layout. With ASLR, each time the program is run, the beginning of each section of memory is randomly chosen. Also, if the program imports libraries, we can also randomize the starting addresses of each library’s source code.<br>
            ASLR causes the absolute addresses of variables, saved registers (sfp and rip), and code instructions to be different each time the program is run. This means the attacker can no longer overwrite some part of memory (such as the rip) with a constant address. Instead, the attacker has to guess the address of their malicious instructions. Since ASLR can shuffle all four segments of memory, theoretically, certain attacks can be mitigated. By randomizing the stack, the attacker cannot place shellcode on the stack without knowing the address of the stack. By randomizing the heap, the attacker, similarly, cannot place shellcode on the heap without knowing the address of the heap. Finally, by randomizing the code, the attacker cannot construct an ROP chain or a return-to-libc attack without knowing the address of the code.<br>
            There are some constraints to randomizing the sections of memory. For example, segments usually need to start at a page boundary. In other words, the starting address of each section of memory needs to be a multiple of the page size (typically 4096 bytes in a 32-bit architecture).<br>
            Modern systems can usually implement ASLR with minimal overhead because they dynamically link libraries at runtime, which requires each segment of memory to be relocatable.<br>
        </p>
        <h4 id="subhead">Subverting ASLR:</h4>
        <p id="subtext">
            Guess the address: Because of the constraints on address randomization, a 32-bit system will sometimes only have around 16 bits of entropy for address randomization. In other words, the attacker can guess the correct address with a 1 in 2^16 probability, or the attacker can try the exploit 2^16 times and expect to succeed at least once. This is less of a problem on 64-bit systems, which have more entropy available for address randomization.<br>
            Like guessing the stack canary, the feasibility of guessing addresses in ASLR depends on the attack setting. For example, if each try takes 1 second, then the attacker can make 2^16 attempts in less than a day. However, if each try after a crash takes exponentially longer, 2^16 attempts may become infeasible.<br>
            Leak the address: Sometimes the program has a vulnerability that allows the attacker to read parts of memory. For example, a format string vulnerability might let the attacker print out values from the stack. The stack often stores absolute addresses, such as pointers and saved registers (sfp and rip). If the attacker can leak an absolute address, they may be able to determine the absolute address of other parts of memory relative to the absolute address they leaked.<br>
            Note that ASLR randomizes absolute addresses by changing the start of sections of memory, but it does not randomize the relative addresses of variables. For example, even if ASLR is enabled, the rip will still be 4 bytes above the sfp in a function stack frame. This means that an attacker who leaks the absolute address of the sfp could deduce the address of the rip (and possibly other values on the stack).<br>
        </p>
        <h4 id="subhead">Combining Mitigations:</h4>

        <p id="subtext">
            We can use multiple mitigations together to force the attacker to find multiple vulnerabilities to exploit the program; this is a process known as synergistic protection, where one mitigation helps strengthen another mitigation. For example, combining ASLR and non-executable pages results in an attacker not being able to write their own shellcode, because of non-executable pages, and not being able to use existing code in memory, because they don’t know the addresses of that code (ASLR). Thus, to defeat ASLR and non-executable pages, the attacker needs to find two vulnerabilities. First, they need to find a way to leak memory and reveal the address location (to defeat ASLR). Next, they need to find a way to write to memory and write an ROP chain (to defeat non-executable pages).<br>
        </p>



</body>
</html>