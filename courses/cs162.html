<!DOCTYPE html>
<html>
    <head>
        <title>Hi</title>
        <link href="../css/styles.css" rel="stylesheet" type="text/css">
      </head>
<body>
    <h2>Computer Science 162: Operating Systems</h2>
    <a href="../index.html">Home</a>
    <div id="toc_container">
        <p class="toc_title">Content:</p>
        <ul class="toc_list">
        <li><a href="#FP1">Operating System Overview</a></li>
        <li><a href="#FP2">Protection: Processes and Kernels</a></li>
        <li><a href="#FP3">Abstractions: Files, I/O, IPC, Pipes and Sockets</a></li>
        <li><a href="#FP4">Concurrency and Threads</a></li>
        <li><a href="#FP5">Synchronizing Access to Shared Objects</a></li>
        <li><a href="#FP6">Multi-Object Synchronization</a></li>



        </ul>
    </div>


    <h3 id="FP1">Operating System Overview</h3>
    <h4 id="subhead">Why OS:</h4>
    <p id="subtext">
        Every device runs an operating system. Every program ever runs on an operating system. Performance and execution behavior will depend on the operating system.<br>
        Operating Systems are becoming largely more complex. This is due to hardware becoming smarter, need for better reliability and security, need for better performance(efficient code/parallel code), and need for better energy usage.<br>
    </p>
    <h4 id="subhead">What OS:</h4>
    <p id="subtext">
        Operating: manages multiple tasks and users. 
        System: a set of interconnected components with an expected behavior observed at the interface with its environment.<br>
        Operating System(v1): an operating system is the layer of software that interfaces between(diverse) hardware resources and the (many) applications running on the machine.<br>
        Operating System(v2): an operating system implements a virtual machine for the application whose interface is more convenient than the raw hardware interface(convenint = security, reliability, portability).<br>
        Three Main Hats:<br>
        <p id="subtext_bullet">
            Referee: manage protection, isolation, and sharing of resources<br>
            Illusionist: provide clean, easy-to-use abstractions of physical resources<br>
            Glue: provides a set of common services<br>
        </p>
    </p>
    <p id="subtext">
        OS as referee:<br>
        Allow multiple(untrusted) applications to run concurrently.<br>
        Fault Isolation: Isolate programs from each other. Isolate OS from other programs. Concepts: process and dual mode execution<br>
        Resource Sharing: How to choose which task to run next? How to split physical resources? Concepts: scheduling<br>
        Communication: How can OS support communication to share results? Concepts: Pipes/Sockets<br><br>
        OS as illusionist:<br>
        Mask the restrictions inherent in computer hardware through virtualization.<br>
        All alone: provide abstraction that application has exclusive use of resources.<br>
        All powerful: provide abstraction that hardware resources are infinite.<br>
        All expressive: provide abstraction of hardware capabilities that are not physically present.<br><br>
        OS as glue:<br>
        Provide set of common standard services to applications to simplify and regularize their design.<br>
        Make Sharing Easier: simpler if all assume same basic primitives.<br>
        Minimize reuse: avoid re-implementing functionality from scratch. Evolve components independently.<br><br>
        Putting it All Together:<br>
        <img id="medium_image" src="../assets/cs162/OSoverview.jpg" alt=""><br><br>
    </p>
    <p id="subtext">
        Definitions:<br>
        Overhead: added resource cost of implementing an abstraction<br>
        Fairness: How "well" are resources distributed across applications<br>
        Response Time: how long does it take for a task to complete<br>
        Throughput: rate at which group of tasks can be completed<br>
        Predictability: are performance metrics constant over time<br>
        Availability: mean time to failure + mean time to repair<br>
        Integrity: computer's operation cannot be compromised by a malicious attacker<br>
        Privacy: data stored on computer accessible to authorized users<br>
        Enforcement Policy: How the OS ensures only permitted actions are allowed<br>
        Security Policy: What is permitted<br>
    </p>
    <p id="subtext">
        OS Evaluations Criteria:<br>
        Performance: OS must implement the abstraction efficiently, with low overhead and equitably. Related: overhead, fairness, response time, throughput, predictability<br>
        Reliability: system does what it is supposed to do-- OS failures are catastrophic. Related: availability<br>
        Security: minimize vulnerability to attack. Related: integrity, privacy, enforcement policy, security policy<br>
        Portability: a portable abstraction does not change as the hardware changes. Can't rewrite application(or OS) every time, must plan for hardware that does not exist yet.<br>
    </p>
    <p id="subtext">
        What functions do we need an operating system to provide applications?<br>
        Process Management:  Can a program create an instance of another program? Wait for it to complete? Stop or resume another running program? Send it an asynchronous event?<br>
        Input/output. How do processes communicate with devices attached to the computer and through them to the physical world? Can processes communicate with each other?<br>
        Thread management. Can we create multiple activities or threads that share memory or other resources within a process? Can we stop and start threads? How do we synchronize their use of shared data structures?<br>
        Memory management. Can a process ask for more (or less) memory space? Can it share the same physical memory region with other processes?<br>
        File systems and storage. How does a process store the user’s data persistently so that it can survive machine crashes and disk failures? How does the user name and organize their data?<br>
        Networking and distributed systems. How do processes communicate with processes on other computers? How do processes on different computers coordinate their actions despite machine crashes and network problems?<br>
        Graphics and window management. How does a process control pixels on its portion of the screen? How does a process make use of graphics accelerators?<br>
        Authentication and security. What permissions does a user or a program have, and how are these permissions kept up to date? On what basis do we know the user (or program) is who they say they are?<br>
    </p>


    <h3 id="FP2">Protection: Processes and Kernels</h3>
    <p id="subtext">
        The OS system implements a virtual machine for the application whose interface is more convenient than the raw hardware interface. Convenient = security, reliability, portability.<br>
    </p>
    <h4 id="subhead">Mechanisms vs Policy:</h4>
    <p id="subtext">
        Mechanism: Lowe-level methods or protocols that implement a needed piece of functionality.(e.g. A brake pedal) <br>
        Policy: Algorithms for making decisions wihtin the OS. Use the mechanism. (e.g. "I break when I see a stop sign")<br>
    </p>
    <h4 id="subhead">Requirements for Virtualization:</h4>
    <p id="subtext">
        Protection is necessary to preserve the virtualization abstraction. Protect application from other application's code. Protect OS from the application. Protect applications against inequitable resource utilisation.<br>
    </p>
    <h4 id="subhead">What is a process?:</h4>
    <p id="subtext">
        A process is an instance of a running program. Which has access to:<br>
        CPU, Memory(store code, data, stack, heap), registers(PC, SP, regular registers), IO information(open files, etc).<br>
        <img id="medium_image" src="../assets/cs162/process.jpg" alt="">
        <img id="medium_image" src="../assets/cs162/processlifecycle.jpg" alt=""><br><br>
        When a process is in the running state it is in the CPU. Blocked and Ready processes are distinguished so that processes waiting on IO aren't rescheduled.<br>
        Process Management: <br>
        <p id="subtext_bullet">
            Process Control Block: in OS stores necessary metadata-- pc, stack ptr, registers, PID, UID, list of open files, process state, etc.<br>
            Process List: stores all processes. Run Queues: List all PCBs in Ready state. Wait Queues: lists all PCBs in blocked state<br>
        </p>
    </p>
    <h4 id="subhead">OS Kernel:</h4>
    <p id="subtext">
        Lowest level of OS running on system. Kernel is trusted with full access to all hardware capabilities. All other software(OS or applications) is considered untrusted.<br>
        The Kernel has full access to keep it simple and small is security. This is the principle of lowest access, keep entities with as little access as possible.<br>
        Process Refined: an executing program with restricted rights. Processes are boxed in with the OS and Hardware and the kernel is the door. Enforcing mechanism must not hinder functionality or hurt performance.<br>
        User Mode vs Kernel Mode:<br>
        <p id="subtext_bullet">
            Application/User Code(untrusted): run all the processor with all potentially dangerous operations disabled.<br>
            Kernel Code(trusted): runs directly on processor with unlimited rights. Performs any hardware operations.<br>
        </p>
    </p>
    <h4 id="subhead">How can the kernel enforce restricted rights?:</h4>
    <p id="subtext">
        Attempt 1: Simulation<br>
        <img id="medium_image" src="../assets/cs162/kernel1.jpg" alt=""><br><br>
        Attempt 2: Dual Mode Operation<br>
        <img id="medium_image" src="../assets/cs162/kernel2.jpg" alt="">
        <img id="medium_image" src="../assets/cs162/kernel3.jpg" alt=""><br><br>
        Privileged Instructions: Unsafe instructions cannot be executed in user mode.<br>
        cannot change privilege level, cannot change address space, cannot disable interrupts, cannot perform IO operations, cannot halt the processor. So what can an application due? Asks for permission to access kernel mode. System calls Transition from user to kernel mode only at specific locations specified by the OS. Exceptsions User mode attempts to execute a privileged exception. Generates a processor exception which passes control to kernel at specific locations. <br>
        
        Memory Isolation: Memory accesses outside a process's address space is prohibited.<br>
        Attempt 1: Isolation<br>
        Hardware to the rescue-- base and bound registers. If memory reference was in between the base and bound reference it was 'ok' otherwise an exception is thrown. Limitations: static memory allocation, cannot share memory between processes, location of code & data determined at runtime, cannot relocate/move programs leads to fragmentation. <br>
        Attempt 2: Virtualization<br>
        Virtual Address space-- set of memory address that process can "touch". Physical address space-- set of memory addresses supported by hardware. Map from virtual addresses to physical address through address translation. Benefits: whole space of virtual address space even physical address not resident in memory, same virtual address can map to same physical address, every process's memory always starts at 0, can dynamically change mapping of virtual to physical addresses.<br>

        Interrupts: Ensure kernel can regain control from running process<br>
        Hardware to the rescue. Set to interrupt processor after a specified delay or specified event and transfer control to (specific locations) in Kernel. Resetting timer is a privilege operation.<br>

        Safe Transfers: Correctly transfer control from user-mode to kernel-mode and back.<br>
        <img id="medium_image" src="../assets/cs162/safecontroltransfer.jpg" alt=""><br><br>
        <p id="subtext_bullet">
            System Calls: User program requests OS service. Transfers to kernel at well-defined location. Read input/write to screen, to files, create new processes, send network packets, get time, etc.<br>
            <img id="medium_image" src="../assets/cs162/systemcalls.jpg" alt=""><br><br>

            Exceptions: Any unexpected condition caused by user program behavior. Stop executing process and enter kernel at specific exception handler. E.G. process missteps(division by zero, writing read-only memory) Attempts to execute a privileged instruction in user mode. Debugger breakpoints! Exceptions are handled the same as interrupts.<br>
        
            Interrupts: Asynchronous signal to the processor that some external event has occurred and may require attention. When process interrupt, stop current process and enter kernel at designated interrupt handler. E.G. timer interrupts, IO interrupts, interprocessor interrupts.<br>
        </p>
    </p>
    <p id="subtext">
        Kernel->User:<br><br>
        New Process Creation: Kernel instantiates data structures, sets registers, switches to user mode.<br>
        Resume after an exception/interrupt/syscall: resume execution by restoring PC, registers, and unsetting mode.<br>
        Switching to a different process: save old process state. Load new process state(restore PC, registers). Unset mode.<br>
        User->Kernel:<br><br>
        Key Requirements: malicious user program(or IO device) cannot corrupt the kernel. Interrupts, exceptions or system calls handled similarly => fewer code paths, fewer bugs.<br>
        Limited Entry: cannot jump to arbitrary code in kernel.<br>
        Atomic Switch: switch from process stack to kernel stack.<br>
        Transparent Execution: restore prior state to continue program.<br>
    </p>
    <p id="subtext">
        <img id="medium_image" src="../assets/cs162/interrupthandler.jpg" alt=""><br><br>
        <img id="medium_image" src="../assets/cs162/syscalls.jpg" alt=""><br><br>
        <img id="medium_image" src="../assets/cs162/syscall2.jpg" alt=""><br><br>
        <img id="medium_image" src="../assets/cs162/syscall3.jpg" alt=""><br><br>
    </p>
    <h3 id="FP3">Abstractions: Files, I/O, IPC, Pipes and Sockets</h3>
    <h4 id="subhead">The Programming Interface:</h4>
    <p id="subtext">
        In this section the focus is on process management and input/output.<br>
    </p>
    <h4 id="subhead">Process Management:</h4>
    <p id="subtext">
        A shell: a job control system; both Windows and UNIX have a shell. An early innovation for user-level process management was to allow developers to write their won shell command line interpreters. Many tasks involve a sequence of steps to do something, each of which can be its own program. With a shell, you can write down the sequence of steps, as a sequence of programs to run to do each step. Thus, you can view it as a very early version of a scripting system.<br>
        <p id="subtext_bullet">
            Windows Process Management:<br>
            One approach to process management is to just add a system call to create a process and other system calls for other process process operations. This turns out to be simple in theory and complex in practice. In Windows, there is a routine called, CreateProcess():<br>
            1. Create and initialize the process control block(PCB) in the kernel. <br>
            2. Create and initialize a new address space.<br>
            3. Load the program prog into the address space.<br>
            4. Copy arguments args into memory in the address space.<br>
            4. Initialize the hardware context to start execution at "start".<br>
            5. Inform the scheduler that the new process is ready to run.<br>
            Unfortunately, there are quite a few aspects of the process that the parent might like to control, such as: its privileges, where it sends its input and output, what is should store its files, what to use as a scheduling priority, and so forth. We cannot trust the child process to set its own privileges.<br><br>
            UNIX Process Management:<br>
            UNIX takes a different approach to process management, one that is complex in theory and simple in practice. UNIX splits CreateProcess in two steps, called fork and exec. UNIX fork creates a complete copy of the parent process, with one key exception. The child process sets up privileges, priorities and I/O for the program that is about to be started, e.g., by closing some files, opening others, reducing priority if it is to run in the background, etc. Because the chile runs exactly the same cod as the parent it can be trusted to set up the context for the new program correctly. Once the context is set, the child process calls UNIX exec. UNIX exec brings the new executable image into memory and starts it running. It may seem wasteful to make a complete copy of the parent process, just to overwrite that copy when we bring in the new executable image into memory using exec, it turns out that fork and exec can be implemented efficiently(discussed later) With this design, UNIX fork takes no arguments and returns an integer. UNIX exec takes two arguments(the program to be run and an array of arguments to pass to the program). This is in place of the ten parameters needed for CreateProcess.<br>
            <img id="medium_image" src="../assets/cs162/processmanagement.jpg" alt=""><br>
            UNIX fork:<br>
            1. Create and initialize the process control block(PCB) in the kernel.<br>
            2. Create a new address space.<br>
            3. Initialize the address space.<br>
            4. Initialize the address space with a copy of the entire contents of the address space of the parent.<br>
            5. Inherit the execution context of the parent(e.g., any open files)<br>
            6. Inform the scheduler that the new process is ready to run.<br>
            A strange aspect of UNIX fork is that the system call returns twice: once to the parent and once to the child. To the parent, UNIX returns the process ID of the child; to the child, it returns zero indicating success. Just as if you made a clone of yourself, you would need some way to tell who was the clone and who was the original, UNIX uses the return value from the fork to distinguish the two copies. 
            UNIX exec:<br>
            1. Load the program prog into the current address space.<br>
            2. Copy arguments args into memory in the address space.<br>
            3. Initialize the hardware context to start execution at "start".<br>
            Note: exec does not create a new process. Often the parent process needs to pause until the child process completes, e.g., if the next step depends on the output of the previous step. UNIX has a system call, naturally enough called wait, that pauses the parent until the child finishes, crashes, or is terminated. Since the parent could have created many child processes, wait it parametrized with the process ID of the child.<br>
        </p>
    </p>
    <h4 id="subhead">Input/Output:</h4>
    <p id="subtext">
        One of the primary innovations in UNIX was to regularize all device input and output behind a single common interface. In fact, UNIX took this one giant step further: it uses this same interface for reading and writing files and for interprocess communication. THis approach was so successful that it is almost universally followed in systems today.<br>
        The basic ideas in UNIX I/O interface are:<br><br>
        Uniformity: All device I/O, file operations, and interprocess communication use the same set of system calls: open, close, read and write.<br><br>
        Open before use: Before an application does I/O it must first call open on the device, file, or communication channel. This gives the operating system a chance to check access permissions and to set up any internal bookkeeping. Some deices, such as a printer, only allow one application access at a time -- the open call can return an error if the device is in use. Open returns a handle to be used in later calls to read, write and close to identify the file, device, or channel; this handle is somewhat misleadingly called a file descriptor even when it refers to a device or channel so there is no file involved. For convenience, the UNIX shell starts application with open file descriptor for reading and writing to the terminal.<br><br>
        Byte-oriented: All devices, even those that transfer fixed-size blocks of data, are accessed with byte arrays. SImilarly, file and communication channel access is in terms of bytes, even though we store data structures in files and send data structures across channels.<br><br>
        Kernel-buffered reads: Stream data, such as from the network or keyboard, is stored in a kerne; buffer and returned to the application on request. This allows the UNIX system call read interface to be the same for devices with streaming reads as those with block reads. In both cases, if no data is available to be returned immediately, the red call blocks until it arrives, potentially giving up the processor to some other task with work to do.<br><br>
        Kernel-buffered writes: Likewise, outgoing data is stored in a kerne; buffer for transmission when the device becomes available. In the normal case, the system call write copies the data into the kernel buffer and returns immediately. This decouples the application from the device, allowing each to go at its own speed. If the application generates data faster than the device can receive it, the write system call blocks in the kernel until there is enough room to store the new data in the buffer. <br><br>
        Explicit close: When an application is done with the device or file it calls close. This signals to the operating system that is can decrement the reference-count on the device and garbage collect any unused kernel data structures.<br><br>
        Pipes: A UNIX pipe is a kernel buffer with two file descriptors, one for writing(to put data into the pipe) and one for reading(to pull data out of the pope). Data is read in exactly the same sequence it is written, but since the data is buffered, the execution of the producer and consumer can be decoupled, reducing waiting in the common case. The pip terminates when either endpoint closes the pipe or exits. Note: the internet has a similar facility to UNIX pipes called TCP(Transmission Control Protocol). Where UNIX pipes connect processes on the same machine, TCP provides a bi-directional pip between two processes running on different machines. In TCP, data is written as a sequence of bytes on one machine and rea out as the same sequence on the other machine.<br><br>
        Replace File Descriptor: By manipulating the file descriptors of the child process, the shell can cause the child to read its input from or send its output to, a file or pipe instead of from a keyboard or to the screen. This way the child process does not need to be aware of who is providing or consuming its I/O. The shell does this redirection using a special system call named dup2(from, to) that replaces the to file descriptor with a copy of the from file descriptor.<br><br>
        Wait for Multiple Reads: for client-server computing, a server may have a pip open to multiple client processes. Normally, read will block if there is no data to be read, and it would be inefficient for the server to poll each pipe in turn to check if there is work for it to do. The UNIX system call select(fd[], number) addresses this. Select allows the server to wait for input from any set of file descriptors; it returns the descriptor that has data, but it does not read the data. Windows has an equivalent function, called WaitForMultipleObjects.<br>
    </p>
    <p id="subtext">
        <img id="medium_image" src="../assets/cs162/unixsystemcalls1.jpg" alt="">
    </p>
    <h4 id="subhead">Implementing a Shell:</h4>
    <p id="subtext">
        The UNIX system calls above are enough to build a flexible and powerful command line shell, one that runs entirely at user-level with no special permissions. The process that creates the shell is responsible for providing it an open file descriptor for reading commands for its input called stdin and for writing output called stdout. 
        <img id="medium_image" src="../assets/cs162/implementingashell.jpg" alt=""><br>
        Note: because the commands to read and write to an open file descriptor are the same whether the file descriptor represents a keyboard, screen, file, device, or pipe, UNIX programs do not need to be aware of where their input is coming from, or where their output is going. This is helpful in a number of ways:<br><br>
        <p id="subtext_bullet">
            A program can be a file of commands. Programs are normally a set of machine instructions but on UNIX a program can be a file containing a list of commands for a shell to interpret. To disambiguate shell programs signified in UNIX by putting "#! interpreter" as the first line of the file, where "interpreter" is teh same of the shell executable.<br><br>
            A program can send its output to a file: By changing the stdout file descriptor in the child, the shell can redirect the child's output to a file. In the standard UNIX shell, this is signified with a "greater than" symbol. Thus, "ls > tmp" lists the contents of the current directory into the file "tmp". After the fork and before the exec, the shell can replace the stdout file descriptor for the child using dup2. Because the paretn has been cloned, changing hte stdout for the child has no effect on the parent.<br>
            A program can read its input from a file. Likewise by using dup2 to change the stdin file descriptor, the shell can cause the child to read its input from a file. In the standard UNIX shell, this is signified with a "less than" symbol. Thus, "zork < solution" plays the game "zork" with a list of instructions stored in the file "solution".<br>
            The output of one program can be the input to another program. The shell can use pipe to connect two programs together, so that the output of one is the input of another. This is called a producer-consumer relationship. In the standard UNIX shell, a pipe connecting two programs is signified by a "|" symbol, as in: "cpp file.c | cparse |cgen | as > file.o". In this case the shell creates four separate child processes, each connected by pipes to its predecessor and successor. Each of the phases can run in parallel, with the parent waiting for all of them to finish. <br>
        </p>
    </p>
    <h4 id="subhead">Interprocess Communication:</h4>
    <p id="subtext">
        For many of the same reasons it makes sense to construct complex applications from simpler modules, it often makes sense to create applications that can specialize on specific task, and then combine those applications into more complex structures.<br>
        <p id="subtext_bullet">
            Producer-Consumer. In this model, programs are structured to accept as input the output of other programs. Communication is one-way: the producer only writes, and the consumer only reads. As we explained above, this allows chaining: a consumer can be, in turn, a producer for a different process. Much of the success of UNIX was due to its ability to easily compose many different programs together in this fashion.<br>
            Client-server. An alternative model is to allow two-way communication between processes, as in client-server computing. The server implements some specialized task, such as managing the printer queue or managing the display. Clients send request to the server to do some task, and when operation is complete, there server replies back to the client.<br>
            File System. Another way programs can be connected together is through reading and writing files. A text editor can import an image created by a drawing program, and the editor can in turn write an HTML file that a web server can read to know how to display a web page. A key distinction is that, unlike the first two modes, communication through the file system can be separated in time: the writer of the file does not need to be running at the same time as the file reader. Therefore, data needs to be stored persistently on disk or other stable storage, and the data needs to be named so that you can find the files when needed later on.<br>
        </p>
    </p>
    <p id="subtext">
        <img id="medium_image" src="../assets/cs162/producerconsumer.jpg" alt="">
        <img id="medium_image" src="../assets/cs162/clientserver.jpg" alt=""><br><br>
    </p>
    <h4 id="subhead">Operating System Structure:</h4>
    <p id="subtext">
        There are many dependencies among the modules inside the operating system, and there is often quite frequent interaction between these modules. This has led operating system designers to wrestle with a fundamental tradeoff: by centralizing functionality in the kernel, performance is improved and it makes it easier to arrange tight integration between kernel modules. However, the resulting systems are less flexible, less easy to change and less adaptive to user or application needs.<br><br>
        Monolithic Kernels:<br>
        <img id="medium_image" src="../assets/cs162/monolithickernel.jpg" alt=""><br><br>
        Almost all widely used commercial OS systems use monolithic kernel design, e.g. Windows, MacOS and Linux. In a monolithic kernel design most of the OS functionality runs inside the OS kernel. In truth, the term is a bit of misnomer, because even in so-called monolithic systems, there are often large segments of what users consider the OS that runs outside the kernel, either as utilities like the shell, or in system libraries such as libraries to manage the user interface. Internal to the monolithic kernel, the OS designer is free to develop whatever interfaces between modules that make sense, and so there is quite a bit of variation from OS to OS in those internal structures. However, two common themes emerge across systems: to improve portability, almost all modern operating systems have both a hardware abstraction layer and dynamically loaded device drivers.<br><br>
            Microkernel:<br>
            An alternative to the monolithic kernel approach is to run as much of the operating system as possible in one or more user-level servers. The windows manager on most operating systems works this way: individual applications draw items on their portion of the screen by sending request to the window manager. The window manager adjudicates which application window is in front or in back for each pixel on the screen, then renders the result. If the system has a hardware graphics accelerator present, the window manager can use it to render items more quickly. Some systems have moved other parts other parts of the operating system into user-level servers: the network stack, the file system, device drivers, and so forth. The difference between a monolithic and microkernel design is often transparent to the application programmer. The location of the service can be hidden in a user-level library -- calls go to the library, which casts the requests either as system calls or as reads and writes to the server through a pipe. The location of the server can also be hidden inside the kernel -- the application calls the kernel as if the kernel implements the service but instead the kernel reformats the request into a pipe that the server can read. A microkernel design offers considerable benefit to the operating system developer, as its easier to modularize and debug user-level services than kernel code. Aside from a potential reliability improvement, however, microkernels offer little in the way of visible benefit to end users and can slow down overall performance by inserting extra steps between the application and service it needs. Thus in practice most systems adopt a hybrid model where some operating system services are run at user-level and some are in the kernel depending on the specific tradeoff between code complexity and performance. <br>
    </p>
    <p id="subtext">
        Hardware Abstraction Layer: <br>
        A key goal of operating systems is to be portable across a wide variety of hardware platforms. To accomplish this especially within a monolithic system, requires careful design of the hardware abstraction layer. The hardware abstraction layer(HAL) is a portable interface to machine configuration and processor-specific operations within the kernel. For example, within the same processor family, such as an Intel x86, different computer manufacturers will require different machine-specific code to configure and mange interrupts and hardware timers. Operating systems that are portable across processor families say between an ARM and an x86 or between a 32bit and 64bit x86 will need processor specific code for process and thread context switches. The interrupt, processor exception, and system call trap handling is also processor specific; all systems have those functions, but the specific implementation will vary. With a will defined hardware abstraction layer in place, most of the operating system is machine and processor independent. Thus porting an operating system to a now computer is just a matter of creating new implementations of these low-level HAL routines and re-linking.<br>
        Dynamically Installed Device Drivers:<br>
        A similar consideration leads to operating systems that can easily accommodate a wide variety of physical I/O devices. Although there are only a handful of different instruction set architectures in wide use today, there are a huge number of different types of physical I/O devices, manufactured by a large number of companies. The key innovation widely adopted today is a dynamically loadable device driver. A dynamically loadable device driver is software to manage a specific device, interface, or chipset, added to the operating system kernel after the kernel starts running, to handle the devices that are present on a particular machine. The device manufacturer typically provides the driver code, using a standard interface supported by the kernel. The operating system kernel calls into the driver whenever it needs to read or write data to the device. The operating system boots with a small number of device drivers e.g. for the disk. For the devices physically attached to the computer, the computer manufacturer bundles those drivers into a file it stores along with the bootloader. When the OS starts up, it queries the I/O bus for which devices are attached to the computer and then loads thos drivers form the file on disk. Finally, for any network attached devices, such as a printer, the OS can load those drivers over the Internet. While dynamically loadable device drivers solve one problem, they pose a different one. Errors in a device driver can corrupt the OS kernel and application data structures; just as with a regular program, error may not be caught immediately, so that user may be unaware that their data is being silently modified. Even worse, a malicious attacker can use device drivers to introduce a computer virus into the operating system kernel and thereby silently gain control over the entire computer. Operating system developers have taken five approaches to dealing with this issue:<br>
        <p id="subtext_bullet">
            Code Inspection: operating system vendors typically require all device driver code to be submitted in advance for inspection and testing, before being allowed into the kernel.<br>
            Bug Tracking: after every system crash, the operating system can collect information about the system configuration adn current kernel stack, and sends this information back to a central database for analysis.<br>
            User-level Device Driver. Both Apple and Microsoft strongly encourage new device drivers to run at user-level rather than in kernel. Each device driver runs in a separate user-level process, using system calls to manipulate the physical device. This way, a buggy device driver can only affect its own internal data structures and not the rest of the OS kernel; if the device driver crashes the kernel can restart easily.<br>
            Virtual Machine Device Drivers. To handle legacy device drivers one approach that has gained some traction is to run device driver code inside a guest os running on a virtual machine. The guest so loads the device drivers as if it was running directly on the real hardware, but when the devices attempt to access the physical hardware, the underlying virtual machine monitor regains control to ensure safety. Device drivers can still have bugs, but they can only corrupt the guest os and not other applications running on the underlying virtual machine monitor.<br>
            Driver Sandboxing. A further challenge for both user-level drivers and virtual machine drivers is performance. Some device drivers need frequent interaction with hardware and the rest of the kernel. Some researchers have proposed running device drivers in their own restricted execution environment inside the kerne. This requires light weight sandboxing techniques discussed later.<br><br>
        </p>
    </p>
    <p id="subtext">
        Abstractions Summary:<br>
        System calls can be used by application to create and manage processes, perform I/O, and communicate with other processes. Every operating system has its own unique system call interface. We focused on parts of the UNIX interface because it is both compact and powerful. A key aspect of the UNIX interface are that creating a process(with fork) is separate from starting to run a program in that process(with exec); another key feature is the use of kernel buffers to decouple reading and writing data through the kernel. Operating systems use the system call interface to provide services to applications and to aid in the internal structuring of the operating system itself. Almost all general purpose computer systems today have a user-level shell and/or window manager that can start and manage applications on behalf of the user. Many systems also implement parts of the operating system as user-level services accessed through kernel pipes.<br>
        Future:<br>
        A trend is for applications to become mini-operating systems in their own right, with multiple users, resource sharing and allocation, untrusted third-party code, processor and memory management and so forth. The system call interfaces for Windows and UNIX were not designed with this in mind and an interesting question is how they will change to accommodate this future of powerful meta-applications. Traditionally operating systems make resource allocation decisions -- when to schedule a process or a thread, how much memory to give a particular application, where and when to store its disk blocks, when to send its network packets -- transparently to the application, with a goal of improving user and overall sytem performance. APplication are unaware of how many resources they have, appearing to run by themselves, isolated on their own (virtual) machine. Of course, the reality is often quite different. An alternative model is for operating systems to divide resources among applications and then allow each application to decide for itself how best to use those resources. One can think of this as a type of federalism. If both the operating system and applications are governments doing their own resource allocation, they are likely to get in each other's way if they are not careful.<br>
    </p>


    <h3 id="FP4">Concurrency and Threads</h3>
    <p id="subtext">
        Concurrency: multiple activities happening at the same time. Correctly managing concurrency is a key challenge for operating system developers. The key idea is to write a concurrent program -- one with many simultaneous activities -- as a set of sequential streams of execution, or thread, that interact and share results in very precise ways. Threads let us define a set of tasks that run concurrently while the code for each task is sequential. The thread abstraction lets the programmer create as many threads as needed without worrying about the exact number of physical processors, or exactly which processor is doing what at each instant. Of course, threads are only an abstraction: the physical hardware has a limited number of processors(potentially only one). The operating system's job is to provide the illusion of a nearly infinite number of virtual processors even while the physical hardware is more limited. It sustains the illusion by transparently suspending and resuming threads so that at any given time only a subset of the threads are actively running.<br> 
    </p>
    <h4 id="subhead">Thread Use Cases:</h4>
    <p id="subtext">
        The intuition behind the thread abstraction is simple: in a program, we can represent each concurrent task as a thread. Each thread proceeds the abstraction of sequential execution similar to the traditional programming model. In fact, think of a traditional program as single-threaded with one logical sequence of steps as each instruction follows the previous one. A multi-threaded program is a generalization of the same basic programming model. Each individual thread follows a single sequence of steps as it executes statements, iterates through loops, calls/returns from procedures, etc. However, a program can now have several such threads executing at the same time.<br>
        Four Reasons to Use Threads:<br>
        <p id="subtext">
            Program Structure: expressing logically concurrent tasks. Programs often interact with or simulate real-world applications that have concurrent activities. Threads let you express an applications natural concurrency by writing each concurrent task as a separate thread. <br>
            Responsiveness: shifting work to run in the background. To improve user responsiveness and performance a common design pattern is to create threads to perform work in the background, without the user waiting for the result. This way, the user interface can remain responsive to further commands, regardless of the complexity of the user request. In web browser for example, the cancel button should continue to work even(or especially) if the downloaded page is gigantic or a script on the page takes a long time to execute. Operating system kernels make extensive use of threads to preserve responsiveness. Many operating systems are designed so that the common case is fast. For example, when writing a file the operating system stores the modified data in a kernel buffer, and returns immediately to the application. In the background, the operating system kernel runs a separate thread to flush the modified data out to disk.<br>
            Performance: exploiting multiple processors. Programs can use threads on a multiprocessor to do work in parallel; they can do the same work in less time or more work in the same elapsed time. Today, a server might have more than a dozen processors; a desktop or laptop may include eight processor cores. <br>
            Performance: managing I/O devices. To do useful work, computers must interact with the outside world via I/O devices. By running tasks as separate threads, when one task is waiting for I/O, the processor can make progress on a different task. The benefit of concurrency between the processor and I/O is two-fold: first processors are often much faster than the I/O systems with which they interact, so keeping the processor idle during I/O would waste much of its capacity. Second, I/O provides a way for the computer to interact with external entities, such as users pressing keys on a keyboard or a remote computer sending network packets. The arrival of this type of I/O event is unpredictable, so the processor must be able to work on other tasks while still responding quickly to these external events. <br>
        </p>
    </p>
    <h4 id="subhead">Thread Abstraction:</h4>
    <p id="subtext">
        Thread: a single execution sequence that represents a separately schedulable task.<br>
        <p id="subtext_bullet">
            Single Execution Sequence: each thread executes a sequence of instructions -- assignments, conditionals, loops, procedures, and so on -- just as in the familiar sequential programming model. <br>
            Separately Schedulable Task: the operating system can run, suspend, or resume a thread at any time.<br>
        </p>
    </p>
    <p id="subtext">
        Running, Suspending, and Resuming Threads:<br><br>
        To map an arbitrary set of threads to a fixed set of processors, operating systems include a thread scheduler that can switch between threads that are running and those that are ready but not running. Threads thus provide an execution model in which each thread runs on a dedicated virtual processor with unpredictable and variable speed. From the point of view of a thread's code, each instruction appears to execute immediately after the preceding one. <br>
        <img id="medium_image" src="../assets/cs162/threadsAPI.jpg" alt=""><br><br>
        A good way to understand the simple threads API is that it provides a way to invoke an asynchronous procedure call. A normal procedure call passes a set of arguments to a function, runs the function immediately on the caller’s stack, and when the function is completed, returns control back to the caller with the result. An asynchronous procedure call separates the call from the return: with thread_create, the caller starts the function, but unlike a normal procedure call, the caller continues execution concurrently with the called function. Later, the caller can wait for the function completion (with thread_join).<br>
        Although the interface in Figure 4.5 is simple, it is remarkably powerful. Many multithreaded applications can be designed using only these thread operations and no additional synchronization. With fork-join parallelism, a thread can create child threads to perform work (“fork”, or thread_create), and it can wait for their results (“join”). Data may be safely shared between threads, provided it is (a) written by the parent before the child thread starts or (b) written by the child and read by the parent after the join.<br>f
        As we have seen, each thread represents a sequential stream of execution. The operating system provides the illusion that each thread runs on its own virtual processor by transparently suspending and resuming threads. For the illusion to work, the operating system must precisely save and restore the state of a thread. However, because threads run either in a process or in the kernel, there is also shared state that is not saved or restored when switching the processor between threads. Thus, to understand how the operating system implements the thread abstraction, we must define both the per-thread state and the state that is shared among threads. Then we can describe a thread’s life cycle — how the operating system can create, start, stop, and delete threads to provide the abstraction.<br>
        <img id="medium_image" src="../assets/cs162/threadlifecycle.jpg" alt=""><br>
        The operating system needs a data structure to represent a thread’s state; a thread is like any other object in this regard. This data structure is called the thread control block (TCB). For every thread the operating system creates, it creates one TCB. The TCB holds two types of per-thread information: the state of the computation being performed by the thread, and the metadata about the thread that is used to manage the thread.<br>
        Per-thread Computation State. To create multiple threads and to be able to start and stop each thread as needed, the operating system must allocate space in the TCB for the current state of each thread’s computation: a pointer to the thread’s stack and a copy of its processor registers.<br>
        Per-thread Metadata. The TCB also includes per-thread metadata — information for managing the thread. For example, each thread might have a thread ID, scheduling priority, and status (e.g., whether the thread is waiting for an event or is ready to be placed onto a processor).<br>
        As opposed to per-thread state that is allocated for each thread, some state is shared between threads running in the same process or within the operating system kernel (Figure 4.8). In particular, program code is shared by all threads in a process, although each thread may be executing at a different place within that code. Additionally, statically allocated global variables and dynamically allocated heap variables can store information that is accessible to all threads.<br>
    </p>
    <h4 id="subhead">Thread Lifecycle:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/cs162/threadlifecycle2.jpg" alt=""><br>
        INIT: Thread creation puts a thread into its INIT state and allocates and initializes per-thread data structures. Once that is done, thread creation code puts the thread into the READY state by adding the thread to the ready list. The ready list is the set of runnable threads that are waiting their turn to use a processor. In practice, as discussed in Chapter 7, the ready list is not in fact a “list”; the operating system typically uses a more sophisticated data structure to keep track of runnable threads, such as a priority queue. Nevertheless, following convention, we will continue to refer to it as the ready list.<br>
        READY: A thread in the READY state is available to be run but is not currently running. Its TCB is on the ready list, and the values of its registers are stored in its TCB. At any time, the scheduler can cause a thread to transition from READY to RUNNING by copying its register values from its TCB to a processor’s registers.<br>
        RUNNING: A thread in the RUNNING state is running on a processor. At this time, its register values are stored on the processor rather than in the TCB. A RUNNING thread can transition to the READY state in two ways: The scheduler can preempt a running thread and move it to the READY state by: (1) saving the thread’s registers to its TCB and (2) switching the processor to run the next thread on the ready list. OR A running thread can voluntarily relinquish the processor and go from RUNNING to READY by calling yield (e.g., thread_yield in the thread library). Notice that a thread can transition from READY to RUNNING and back many times. Since the operating system saves and restores the thread’s registers exactly, only the speed of the thread’s execution is affected by these transitions.<br>
        WAITING: A thread in the WAITING state is waiting for some event. Whereas the scheduler can move a thread in the READY state to the RUNNING state, a thread in the WAITING state cannot run until some action by another thread moves it from WAITING to READY. While a thread waits for an event, it cannot make progress; therefore, it is not useful to run it. Rather than continuing to run the thread or storing the TCB on the scheduler’s ready list, the TCB is stored on the waiting list of some synchronization variable associated with the event. When the required event occurs, the operating system moves the TCB from the synchronization variable’s waiting list to the scheduler’s ready list, transitioning the thread from WAITING to READY.<br>
        FINISHED: A thread in the FINISHED state never runs again. The system can free some or all of its state for other uses, though it may keep some remnants of the thread in the FINISHED state for a time by putting the TCB on a finished list. For example, the thread_exit call lets a thread pass its exit value to its parent thread via thread_join. Eventually, when a thread’s state is no longer needed (e.g., after its exit value has been read by the join call), the system can delete and reclaim the thread’s state.<br>
    </p>
    <h4 id="subhead">Kernel Threads:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/cs162/kernelthread2.jpg" alt="">
        <img id="medium_image" src="../assets/cs162/kernelthread.jpg" alt=""><br><br>
        Kernel threads. The simplest case is implementing threads inside the operating system kernel, sharing one or more physical processors. A kernel thread executes kernel code and modifies kernel data structures. Almost all commercial operating systems today support kernel threads.<br>
        Kernel threads and single-threaded processes. An operating system with kernel threads might also run some single-threaded user processes. As shown in Figure these processes can invoke system calls that run concurrently with kernel threads inside the kernel.<br>
        Multi-threaded processes using kernel threads. Most operating systems provide a set of library routines and system calls to allow applications to use multiple threads within a single user-level process. Figure 4.12 illustrates this case. These threads execute user code and access user-level data structures. They also make system calls into the operating system kernel. For that, they need a kernel interrupt stack just like a normal single-threaded process.<br>
        User-level threads. To avoid having to make a system call for every thread operation, some systems support a model where user-level thread operations — create, yield, join, exit, and the synchronization routines described in Chapter 5 — are implemented entirely in a user-level library, without invoking the kernel.<br>
        Creating a Thread:<br><br>
        Allocate per-thread state. The first step in the thread constructor is to allocate space for the thread’s per-thread state: the TCB and stack. As we have mentioned, the TCB is the data structure the thread system uses to manage the thread. The stack is an area of memory for storing data about in-progress procedures; it is allocated in memory like any other data structure.<br>
        Initialize per-thread state. To initialize the TCB, the thread constructor sets the new thread’s registers to what they need to be when the thread starts RUNNING. When the thread is assigned a processor, we want it to start running func(arg). However, instead of having the thread start in func, the constructor starts the thread in a dummy function, stub, which in turn calls func. We need this extra step in case the func procedure returns instead of calling thread_exit. Without the stub, func would return to whatever random location is stored at the top of the stack! Instead, func returns to stub and stub calls thread_exit to finish the thread. <br>
        Put TCB on ready list. The last step in creating a thread is to set its state to READY and put the new TCB on the ready list, enabling the thread to be scheduled.<br>
    </p>
    <p id="subtext">
        When a thread calls thread_exit, there are two steps to deleting the thread: Remove the thread from the ready list so that it will never run again. AND Free the per-thread state allocated for the thread.<br>
        a thread never deletes its own state. Instead, some other thread must do it. On exit, the thread transitions to the FINISHED state, moves its TCB from the ready list to a list of finished threads the scheduler should never run. The thread can then safely switch to the next thread on the ready list. Once the finished thread is no longer running, it is safe for some other thread to free the state of the thread.<br>
    </p>
    <p id="subtext">
        To support multiple threads, we also need a mechanism to switch which threads are RUNNING and which are READY.<br>
        A thread context switch suspends execution of a currently running thread and resumes execution of some other thread. The switch saves the currently running thread’s registers to the thread’s TCB and stack, and then it restores the new thread’s registers from that thread’s TCB and stack into the processor.<br>
        What Triggers a Kernel Thread Context Switch? A thread context switch can be triggered by either a voluntary call into the thread library, or an involuntary interrupt or processor exception.<br>
        <p id="subtext">
            Voluntary. The thread could call a thread library function that triggers a context switch. For example, most thread libraries provide a thread_yield call that lets the currently running thread voluntarily give up the processor to the next thread on the ready list. Similarly, the thread_join and thread_exit calls suspend execution of the current thread and start running a different one.<br>
            Involuntary. An interrupt or processor exception could invoke an interrupt handler. The interrupt hardware saves the state of the running thread and executes the handler’s code. The handler can decide that some other thread should run, and then switch to it. Alternatively, if the current thread should continue running, the handler restores the state of the interrupted thread and resumes execution.<br>
        </p>
    </p>
    <p id="subtext">
        Voluntary Kernel Thread Context Switch. The pseudo-code for thread_yield first turns off interrupts to prevent the thread system from attempting to make two context switches at the same time. The pseudo-code then pulls the next thread to run off the ready list (if any), and switches to it. The thread_switch code may seem tricky, since it is called in the context of the old thread and finishes in the context of the new thread. To make this work, thread_switch saves the state of the registers to the stack and saves the stack pointer to the TCB. It then switches to the stack of the new thread, restores the new thread’s state from the new thread’s stack, and returns to whatever program counter is stored on the new stack. A twist is that the return location may not be to thread_yield! The return is to whatever the new thread was doing beforehand. For example, the new thread might have been WAITING in thread_join and is now READY to run. The thread might have called thread_yield. Or it might be a newly created thread just starting to run. It is essential that any routine that causes the thread to yield or block call thread_switch in the same way. Equally, to create a new thread, thread_create must set up the stack of the new thread to be as if it had suspended execution just before performing its first instruction.Then, if the newly created thread is the next thread to run, a thread can call thread_yield, switch to the newly created thread, switch to its stack pointer, pop the register values off the stack, and “return” to the new thread, even though it had never called switch in the first place<br>
        Involuntary Kernel Thread Context Switch. Chapter 2 explained what happens when an interrupt, exception, or trap interrupts a running user-level process: hardware and software work together to save the state of the interrupted process, run the kernel’s handler, and restore the state of the interrupted process. Save the state. Save the currently running thread’s registers so that the handler can run code without disrupting the interrupted thread. Hardware saves some state when the interrupt or exception occurs, and software saves the rest of the state when the handler runs. Run the kernel’s handler. Run the kernel’s handler code to handle the interrupt or exception. Since we are already in kernel mode, we do not need to change from user to kernel mode in this step. We also do not need to change the stack pointer to the base of the kernel’s interrupt stack. Instead, we can just push saved state or handler variables onto the current stack, starting from the current stack pointer. Restore the state. Restore the next ready thread’s registers so that the thread can resume running where it left off. In short, comparing a switch between kernel threads to what happens on a user-mode transfer: (1) there is no need to switch modes (and therefore no need to switch stacks) and (2) the handler can resume any thread on the ready list rather than always resuming the thread or process that was just suspended. <br>
    </p>
    <h4 id="subhead">Combining Kernel Threads and Single-Threaded User Processes:</h4>
    <p id="subtext">
        Hybrid Thread Join. Thread libraries can avoid transitioning to the kernel in certain cases. For example, rather than always making a system call for thread_join to wait for the target thread to finish, thread_exit can store its exit value in a data structure in the process’s address space. Then, if the call to thread_join happens after the targeted thread has exited, it can immediately return the value without having to make a system call. However, if the call to thread_join precedes the call to thread_exit, then a system call is needed to transition to the WAITING state and let some other thread run. As a further optimization, on a multiprocessor it can sometimes make sense for thread_join to spin for a few microseconds before entering the kernel, in the hope that the other thread will finish in the meantime.<br>
        Per-Processor Kernel Threads. It is possible to adapt the green threads approach to work on a multiprocessor. For many parallel scientific applications, the cost of creating and synchronizing threads is paramount, and so an approach that requires a kernel call for most thread operations would be prohibitive. Instead, the library multiplexes user-level threads on top of kernel threads, in exactly the same way that the kernel multiplexes kernel threads on top of physical processors. When the application starts up, the user-level thread library creates one kernel thread for each processor on the host machine. As long as there is no other activity on the system, the kernel will assign each of these threads a processor. Each kernel thread executes the user-level scheduler in parallel: pull the next thread off the user-level ready list, and run it. Because thread scheduling decisions occur at user level, they can be flexible and application-specific; for example, in a parallel graph algorithm, the programmer might adjust the priority of various threads based on the results of the computation on other parts of the graph. Of course, most of the downsides of green threads are still present in these systems: Any time a user-level thread calls into the kernel, its host kernel thread blocks. This prevents the thread library from running a different user-level thread on that processor in the meantime. Any time the kernel time-slices a kernel thread, the user-level thread it was running is also suspended. The library cannot resume that thread until the kernel thread resumes. Scheduler Activations. To address these issues, some operating systems have added explicit support for user-level threads. One such model, implemented most recently in Windows, is called scheduler activations. In this approach, the user-level thread scheduler is notified (or activated) for every kernel event that might affect the user-level thread system. For example, if one thread blocks in a system call, the activation informs the userlevel scheduler that it should choose another thread to run on that processor. Scheduler activations are like upcalls or signals, except that they do not return to the kernel; instead, they directly perform user-level thread suspend and resume.<br>

    </p>
    <h4 id="subhead">Alternative Abstractions:</h4>
    <p id="subtext">
        Asynchronous I/O and event-driven programming. Asynchronous I/O and events allow a single-threaded program to cope with high-latency I/O devices by overlapping I/O with processing and other I/O.<br>
        Asynchronous I/O is a way to allow a single-threaded process to issue multiple concurrent I/O requests at the same time. The process makes a system call to issue an I/O request but the call returns immediately, without waiting for the result. At a later time, the operating system provides the result to the process by either: (1) calling a signal handler, (2) placing the result in a queue in the process’s memory, or (3) storing the result in kernel memory until the process makes another system call to retrieve it.<br>
        Data parallel programming. With data parallel programming, all processors perform the same instructions in parallel on different parts of a data set.<br>
        One popular model is data parallel programming, also known as SIMD (single instruction multiple data) programming or bulk synchronous parallel programming. In this model, the programmer describes a computation to apply in parallel across an entire data set at the same time, operating on independent data elements. The work on every data item must complete before moving onto the next step; one processor can use the results of a different processor only in some later step. As a result, the behavior of the program is deterministic. Rather than having programmers divide work among threads, the runtime system decides how to map the parallel work across the hardware’s processors.<br>
    </p>


    <h3 id="FP5">Synchronizing Access to Shared Objects</h3>
    <p id="subtext">
        Multi-threaded programs extend the traditional, single-threaded programming model so that each thread provides a single sequential stream of execution composed of familiar instructions. If a program has independent threads that operate on completely separate subsets of memory, we can reason about each thread separately. In this case, reasoning about independent threads differs little from reasoning about a series of independent, single-threaded programs. However, most multi-threaded programs have both per-thread state (e.g., a thread’s stack and registers) and shared state (e.g., shared variables on the heap). Cooperating threads read and write shared state.Unfortunately, when cooperating threads share state, writing correct multi-threaded programs becomes much more difficult. Most programmers are used to thinking “sequentially” when reasoning about programs. For example, we often reason about the series of states traversed by a program as a sequence of instructions is executed. However, this sequential model of reasoning does not work in programs with cooperating threads, for three reasons:<br><br> 
        1 Program execution depends on the possible interleavings of threads’ access to shared state.<br>
        2 Program execution can be nondeterministic.<br>
        3 Compilers and processor hardware can reorder instructions.<br>
    </p>
    <h4 id="subhead">Race Conditions:</h4>
    <p id="subtext">
        A race condition occurs when the behavior of a program depends on the interleaving of operations of different threads. In effect, the threads run a race between their operations, and the results of the program execution depends on who wins the race.<br>
    </p>
    <h4 id="subhead">Atomic Operations:</h4>
    <p id="subtext">
        Atomic operations, indivisible operations that cannot be interleaved with or split by other operations. On most modern architectures, a load or store of a 32-bit word from or to memory is an atomic operation. So, the previous analysis reasoned about interleaving of atomic loads and stores to memory. Conversely, a load or store is not always an atomic operation. Depending on the hardware implementation, if two threads store the value of a 64-bit floating point register to a memory address, the final result might be the first value, the second value, or a mix of the two.<br>
    </p>
    <h4 id="subhead">Structuring Shared Objects:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/cs162/threadsharedstate.jpg" alt="">
        <img id="medium_image" src="../assets/cs162/sharedprograms.jpg" alt=""><br><br>
        Shared objects are objects that can be accessed safely by multiple threads. All shared state in a program — including variables allocated on the heap (e.g., objects allocated with malloc or new) and static, global variables — should be encapsulated in one or more shared objects.<br>
        Programming with shared objects extends traditional object-oriented programming, in which objects hide their implementation details behind a clean interface. In the same way, shared objects hide the details of synchronizing the actions of multiple threads behind a clean interface. The threads using shared objects need only understand the interface; they do not need to know how the shared object internally handles synchronization.Like regular objects, programmers can design shared objects for whatever modules, interfaces, and semantics an application needs. Each shared object’s class defines a set of public methods on which threads operate. To assemble the overall program from these shared objects, each thread executes a “main loop” written in terms of actions on public methods of shared objects.<br>
    </p>
    <p id="subtext">
        Shared object layer: as in standard object oriented programming, shared objects define application-specific logic and hide internal internal implementation details. Externally, they appear to have the same interface as you would define for a single-threaded program.<br>
        Synchronization Variable Layer: rather than implementing shared objects directly with carefully interleaved atomic loads and stores, shared objects include synchronization variables as member variables. Synchronization variables, stored in memory just like any other object, can be included in any data structure. A synchronization variable is a data structure used for coordinating concurrent access to shared state. Both the interface and the implementation of synchronization variables must be carefully designed. In particular, we build shared objects using two types of synchronization variables: locks and condition variables. Synchronization variables coordinate access to state variables, which are just the normal member variables of an object that you are familiar with from single-threaded programming (e.g., integers, strings, arrays, and pointers).Using synchronization variables simplifies implementing shared objects. In fact, not only do shared objects externally resemble traditional single-threaded objects, but, byimplementing them with synchronization variables, their internal implementations are quite similar to those of single-threaded programs.<br>
        Atomic instruction layer. Although the layers above benefit from a simpler programming model, it is not turtles all the way down. Internally, synchronization variables must manage the interleavings of different threads’ actions. Rather than implementing synchronization variables, such as locks and condition variables, using atomic loads and stores as we tried to do for the Too Much Milk problem, modern implementations build synchronization variables using atomic readmodify-write instructions. These processor-specific instructions let one thread have temporarily exclusive and atomic access to a memory location while the instruction executes. Typically, the instruction atomically reads a memory location, does some simple arithmetic operation to the value, and stores the result. The hardware guarantees that any other thread’s instructions accessing the same memory location will occur either entirely before, or entirely after, the atomic read-modify-write instruction.<br>
    </p>
    <h4 id="subhead">Locks: Mutual Exclusion:</h4>
    <p id="subtext">
        A lock is a synchronization variable that provides mutual exclusion — when one thread holds a lock, no other thread can hold it (i.e., other threads are excluded). A program associates each lock with some subset of shared state and requires a thread to hold the lock when accessing that state. Then, only one thread can access the shared state at a time. Mutual exclusion greatly simplifies reasoning about programs because a thread can perform an arbitrary set of operations while holding a lock, and those operations appear to be atomic to other threads. In particular, because a lock enforces mutual exclusion and threads must hold the lock to access shared state, no other thread can observe an intermediate state. Other threads can only observe the state left after the lock release.<br>
        It is much easier to reason about interleavings of atomic groups of operations rather than interleavings of individual operations for two reasons. First, there are (obviously) fewer interleavings to consider. Reasoning about interleavings on a coarser-grained basis reduces the sheer number of cases to consider. Second, and more important, we can make each atomic group of operations correspond to the logical structure of the program, which allows us to reason about invariants not specific interleavings. In particular, shared objects usually have one lock guarding all of an object’s state. Each public method acquires the lock on entry and releases the lock on exit. Thus, reasoning about a shared class’s code is similar to reasoning about a traditional class’s code: we assume a set of invariants when a public method is called and re-establish those invariants before a public method returns. If we define our invariants well, we can then reason about each method independently.<br>
    </p>
    <h4 id="subhead">Locks: API and Properties:</h4>
    <p id="subtext">
        A lock enables mutual exclusion by providing two methods: Lock::acquire() and Lock::release().<br>
        <p id="subtext_bullet">
            A lock can be in one of two states: BUSY or FREE<br>
            A lock is initially in the FREE state<br>
            Lock::acquire waits until the lock is FREE then atomically makes the lock BUSY<br>
            Lock::release makes the lock FREE. If there are pending acquire operations this state change causes one of them to proceed. 
        </p>
    </p>
    <p id="subtext">
        Formal properties. A lock can be defined more precisely as follows. A thread holds a lock if it has returned from a lock’s acquire method more often than it has returned from a lock’s release method. A thread is attempting to acquire a lock if it has called but not yet returned from a call to acquire on the lock.<br>
        A lock should ensure the following three properties:<br>
        <p id="subtext_bullet">
            Mutual Exclusion: at most one thread holds the lock<br>
            Progress: if no thread holds the lock and any thread attempts to acquire the lock then eventually some thread succeeds in acquiring the lock<br>
            Bounded Waiting: if thread T attempts to acquire a lock then there exists a bound on the number of times other threads can successfully acquire the lock before T does.<br>
        </p>
    </p>
    <h4 id="subhead">Case Study: Thread-Safe Bounded Queue:</h4>
    <p id="subtext">
        A bounded queue is a queue with a fixed size limit on the number of items stored in the queue. Operating system kernels use bounded queues for managing interprocess communication, TCP and UDP sockets, and I/O requests. Because the kernel runs in a finite physical memory, the kernel must be designed to work properly with finite resources. For example, instead of a simple, infinite buffer between a producer and a consumer thread, the kernel will instead use a limited size buffer, or bounded queue.<br>
        A thread-safe bounded queue is a type of a bounded queue that is safe to call from multiple concurrent threads. Figure 5.3 gives an implementation; it lets any number of threads safely insert and remove items from the queue. As Figure 5.4 illustrates, a program can allocate multiple such queues (e.g., queue1, queue2, and queue3), each of which includes its own lock and state variables.<br>
        A critical section is a sequence of code that atomically accesses shared state. By ensuring that a thread holds the object’s lock while executing any of its critical sections, we ensure that each critical section appears to execute atomically on its shared state. There is a critical section in each of the methods tryInsert and tryRemove.<br>
    </p>
    <p id="subtext">
        Use locks for mutual exclusion and condition variables for scheduling constraints<br>
    </p>


    <h3 id="FP5">Multi-Object Synchronization</h3>
    <p id="subtext">
        What happens as programs become more complex, with multiple shared objects and multiple locks? Several considerations arise in this context: Multiprocessor performance, correctness, deadlocks.<br>
    </p>
    <h4 id="subhead">Multi-Processor Lock Performance:</h4>
    <p id="subtext">
        In cases where large multiprocessors are used such as a client server application. Once locks and condition variables are added to a server application to allow it to process requests concurrently, throughput may be only slightly faster on a fifty-way multiprocessor than on a uniprocessor. Most often, this can be due to three causes:<br>
        Locking. A lock implies mutual exclusion — only one thread at a time can hold the lock. As a result, access to a shared object can limit parallelism.<br>
        Communication of shared data. The performance of a modern processor can vary by a factor of ten (or more) depending on whether the data needed by the processor is already in its cache or not. Modern processors are designed with large caches, so that almost all of the data needed by the processor will already be stored in the cache. On a uniprocessor, it is rare that the processor needs to wait. However, on a multiprocessor, the situation is different. Shared data protected by a lock will often need to be copied from one cache to another. Shared data is often in the cache of the processor that last held the lock, and it is needed in the cache of the processor that is next to acquire the lock. Moving data can slow critical section performance significantly compared to a uniprocessor.<br>
        False sharing. A further complication is that the hardware keeps track of shared data at a fixed granularity, often in units of a cache entry of 32 or 64 bytes. This reduces hardware management overhead, but it can cause performance problems if multiple data structures with different sharing behavior fit in the same cache entry. This is called false sharing.<br>
    </p>
    <h4 id="subhead">Lock Design Patterns:</h4>
    <p id="subtext">
        We next discuss a set of approaches that can reduce the impact of locking on multiprocessor performance. Often, the best practice is to start simple, with a single lock per shared object. If an object’s interface is well designed, then refactoring its implementation to increase concurrency and performance can be done once the system is built and performance measurements can identify any bottlenecks. An adage to follow is: “It is easier to go from a working system to a working, fast system than to go from a fast system to a fast, working system.”<br>
    </p>
    <p id="subtext">
        Fine Grained Locking:<br>
        <p id="subtext_bullet">
            A simple and widely used approach to decrease contention for a shared lock is to partition the shared object’s state into different subsets, each protected by its own lock. This is called fine-grained locking.<br>
            The web server cache discussed above provides an example. The cache can use a shared hash table to store and locate recently used web pages; because the hash table is shared, it needs a lock to provide mutual exclusion. The lock is acquired and released at the start and end of each of the hash table methods: put(key, value), value = get(key), and value = remove(key). If the single lock limits performance, an alternative is to have one lock per hash bucket. The methods acquire the lock for bucket b before accessing any record that hashes to that bucket. Provided that the number of buckets is large enough, and no single bucket receives a large fraction of requests, then different threads can use and update the hash table in parallel. However, there is no free lunch. Dividing an object’s state into different pieces protected by different locks can significantly increase the object’s complexity. Suppose we want to implement a hash table whose number of hash buckets grows as the number of objects it stores increases. If we have a single lock, this is easy to do. But, what if we use finegrained locking? Then, the design becomes more complex because we have some methods, like put and get, that operate on one bucket and other methods, like resize, that operate across multiple buckets. Solutions:<br><br>
            Introduce a readers/writers lock. Suppose we have a readers/writers lock on the overall structure of the hash table (e.g., the number of buckets and the array of buckets) and a mutual exclusion lock on each bucket. Methods that work on a single bucket at a time, such as put and get, acquire the table’s readers/writers lock in read mode and also acquire the relevant bucket’s mutual exclusion lock. Methods that change the table’s structure, such as resize, must acquire the readers/writers lock in write mode; the readers/writers lock prevents any other threads from using the hash table while it is being resized.<br>
            Acquire every lock. Methods that change the structure of the hash table, such as resize, must first iterate through every bucket, acquiring its lock, before proceeding. Once resize has a lock on every bucket, it is guaranteed that no other thread is concurrently accessing or modifying the hash table.<br>
            Divide the hash key space. Another solution is to divide the hash key space into r regions, to have a mutual exclusion lock for each region, and to allow each region to be resized independently when it becomes heavily loaded. Then, get, put, and resizeRegion each acquire the relevant region’s mutual exclusion lock.<br>
        </p>
    </p>
    <p id="subtext">
        Per-Processor Data Structures<br>
        <p id="subtext_bullet">
            A related technique to fine-grained locking is to partition the shared data structure based on the number of processors on the machine. For example, instead of one shared hash table of cached pages, an alternative design would have N separate hash tables, where N is the number of processors. Each thread uses the hash table based on the processor where it is currently running. Each hash table still needs its own lock in case a thread is context switched in the middle of an operation, but in the common case, only threads running on the same processor contend for the same lock. Often, this is combined with a per-processor ready list, ensuring that each thread preferentially runs on the same processor each time it is context switched, further improving execution speed. <br>
            An advantage of this approach is better hardware cache behavior; as we saw in the previous section, shared data that must be communicated between processors can slow down the execution of critical sections. Of course, the disadvantage is that the hash tables are now partitioned, so that a web page may be cached in one processor’s hash table, and needed in another. Whether this is a performance benefit depends on the relative impact of reducing communication of shared data versus the decreased effectiveness of the cache.<br>
        </p>
    </p>
    <p id="subtext">
        Ownership Design Pattern:<br>
        <p id="subtext_bullet">
            A common synchronization technique in large, multi-threaded programs is an ownership design pattern. In this pattern, a thread removes an object from a container and can then access the object without holding a lock: the program structure guarantees that at most one thread owns an object at a time.<br>
            As an example, a single web page can contain multiple objects, including HTML frames, style sheets, and images. Consider a multi-threaded web browser whose processing is divided into three stages: receiving an object via the network, parsing the object, and rendering the object (see Figure 6.2). The first stage has one thread per network connection; the other stages have several worker threads, each of which processes one object at a time. The work queues between stages coordinate object ownership. Objects in the queues are not being accessed by any thread. When a worker thread in the parse stage removes an object from the stage’s work queue, it owns the object and has exclusive access to it. When the thread is done parsing the object, it puts it into the second queue and stops accessing it. A worker thread from the render stage then removes it from the second queue, gaining exclusive access to it to render it to the screen.<br>
        </p>
    </p>
    <p id="subtext">
        Staged Architecture:<br>
        <p id="subtext_bullet">
            The staged architecture pattern, illustrated in Figure 6.3, divides a system into multiple subsystems, called stages. Each stage includes state private to the stage and a set of one or more worker threads that operate on that state. Different stages communicate by sending messages to each other via shared producer-consumer queues. Each worker thread repeatedly pulls the next message from a stage’s incoming queue and then processes it, possibly producing one or more messages for other stages’ queues.<br>
            <img id="medium_image" src="../assets/cs162/stagedarchitecture.jpg" alt=""><br><br>
            Figure 6.3 shows a staged architecture for a simple web server that has a first connect stage that uses one thread to set up network connections and that passes each connection to a second read and parse stage. The read and parse stage has several threads, each of which repeatedly gets a connection from the incoming queue, reads a request from the connection, parses the request to determine what web page is being requested, and checks to see if the page is already cached.<br>
            Assuming the page is not already cached, if the request is for a static web page (e.g., an HTML file), the read and parse stage passes the request and connection to the read static page stage, where one of the stage’s threads reads the specified page from disk. Otherwise, the read and parse stage passes the request and connection to the generate dynamic page stage, where one of the stage’s threads runs a program that dynamically generates a page in response to the request. Once the page has been fetched or generated, the page and connection are passed to the send page stage, where one of the threads transmits the page over the connection. The key property of a staged architecture is that the state of each stage is private to that stage. This improves modularity, making it easier to reason about each stage individually and about interactions across stages.<br>
            As an example of the modularity benefits, consider a system where different stages are produced by different teams or even different companies. Each stage can be designed and tested almost independently, and the system is likely to work as expected when the stages are brought together. For example, it is common practice for a web site to use a web server from one company and a database from another company and for the two to communicate via messages. Another benefit is improved cache locality. A thread operating on a subset of the system’s state may have better cache behavior than a thread that accesses state from all stages. On the other hand, for some workloads, passing a request from stage to stage could hurt cache behavior compared to doing all of the processing for a request on one processor. Also note that for good performance, the processing in each stage must be large enough to amortize the cost of sending and receiving messages<br>
            One challenge with staged architectures is dealing with overload. System throughput is limited by the throughput of the slowest stage. If the system is overloaded, the slowest stage will fall behind, and its work queue will grow. Depending on the system’s implementation, two bad things could happen. First, the queue could grow indefinitely, consuming more and more memory until the system memory heap is exhausted. Second, if the queue is limited to a finite size, once that size is reached, earlier stages must either discard work for the overloaded stage or block until the queue has room. Notice that if they block, then the backpressure will limit the throughput of earlier stages to that of the bottleneck stage, and their queues in turn may begin to grow. One solution is to dynamically vary the number of threads per stage. If a stage’s incoming queue is growing, the program can shift processing resources to it by reducing the number of threads for a lightly-loaded stage in favor of more threads for the stage that is falling behind.<br>
        </p>
    </p>
    <h4 id="subhead">Lock Contention:</h4>
    <p id="subtext">
        Sometimes, even after applying the techniques described in the previous section, locking may remain a bottleneck to good performance on a multiprocessor. For example, with finegrained locking of a hash table, if a bucket contains a particularly popular item, say the cached page for Justin Bieber, then the lock on that bucket can be a source of contention.<br>
        MCS Locks. MCS is an implementation of a spinlock optimized for the case when there are a significant number of waiting threads.<br>
        RCU Locks. RCU is an implementation of a reader/writer lock, optimized for the case when there are many more readers than writers. RCU reduces the overhead for readers at a cost of increased overhead for writers. More importantly, RCU has somewhat different semantics than a normal reader/writer lock, placing a burden on the user of the lock to understand its dangers.<br>
    </p>
    <p id="subtext">
        MCS Locks:<br>
        <p id="subtext_bullet">
            A more scalable solution is to assign each waiting thread a separate memory location where it can spin. To release a lock, the bit is set for one thread, telling it that it is the next to acquire the lock. The most widely used implementation of this idea is known as the MCS lock, after the initials of its authors, Mellor-Crummey and Scott. The MCS lock takes advantage of an atomic read-modify-write instruction called compare-and-swap that is supported on most modern multiprocessor architectures. Compare-and-swap tests the value of a memory location and swaps in a new value if the old value has not changed.<br><br>
            Compare-and-swap can be used to build a queue of waiting threads, without a separate spinlock. A waiting thread atomically adds itself to the tail of the queue, and then spins on a flag in its queue entry. When a thread releases the lock, it sets the flag in the next queue entry, signaling to the thread that its turn is next. Figure 6.5 provides an implementation, and Figure 6.6 illustrates the algorithm in action.Because each thread in the queue spins on its own queue entry, the lock can be passed efficiently from one thread to another along the queue. Of course, the overhead of setting up the queue means that an MCS lock is less efficient than a normal spinlock unless there are a large number of waiting threads.<br>
            <img id="medium_image" src="../assets/cs162/MCSlock.jpg" alt=""><br><br>
            SEE BOOK FOR IMPLEMENTATION<br><br>

        </p>
    </p>
    <p id="subtext">
        Read-Copy-Update (RCU):
        <p id="subtext_bullet">
            Read-copy-update (RCU) provides high-performance synchronization for data structures that are frequently read and occasionally updated. In particular, RCU optimizes the read path to have extremely low synchronization costs even with a large number of concurrent readers. However, writes can be delayed for a long time — tens of milliseconds in some implementations.<br>
            Standard readers/writers locks are a poor fit for certain types of read-dominated workloads. Recall that these locks allow an arbitrary number of concurrent active readers, but when there is an active writer, no other writer or reader can be active. The problem occurs when there are many concurrent reads with short critical sections. Before reading, each reader must acquire a readers/writers lock in read mode and release it afterwards. On both entrance and exit, the reader must update some state in the readers/writers synchronization object. Even when there are only readers, the readers/writers synchronization object can become a bottleneck. This limits the rate at which readers can enter the critical section, because they can only acquire the lock one at a time. For critical sections of less than a few thousand cycles, and for programs with dozens of threads simultaneously reading a shared object, the standard readers/writers lock can limit performance. While the readers/writers synchronization object could be implemented with an MCS lock and thereby reduce some of the effects of lock contention, it does not change the inherent serial access of the readers/writers control structure.<br>
            Approach: How can concurrent reads access a data structure — one that can also be written — without having to update the state of a synchronization variable on each read? To meet this challenge, an RCU lock retains the basic structure of a reader/writers lock: readers (and writers) surround each critical section with calls to acquire and release the RCU lock in read-mode (or write-mode). An RCU lock makes three important changes to the standard interface:<br>
            Restricted update. With RCU, the writer thread must publish its changes to the shared data structure with a single, atomic memory write. Typically, this is done by updating a single pointer, as we illustrate below by using RCU to update a shared list. Although restricted updates might seem to severely limit the types of data structure operations that are possible under RCU, this is not the case. A common pattern is forthe writer thread to make a copy of a complex data structure (or a portion of it), update the copy, and then publish a pointer to the copy into a shared location where it can be accessed by new readers.<br>
            Multiple concurrent versions. RCU allows any number of read-only critical sections to be in progress at the same time as the update. These read-only critical sections may see the old or new version of the data structure.<br>
            Integration with the thread scheduler. Because there may be readers still in progress when an update is made, the shared object must maintain multiple versions of its state, to guarantee that an old version is not freed until all readers have finished accessing it. The time from when an update is published until the last reader is done with the previous version is called the grace period. The RCU lock uses information provided by the thread scheduler to determine when a grace period ends.<br>
            <img id="medium_image" src="../assets/cs162/RCUlockes.jpg" alt=""><br><br>
            <img id="medium_image" src="../assets/cs162/RCUlockapi.jpg" alt=""><br><br>
            SEE BOOK FOR IMPLEMENTATION<br><br>
        </p>
    </p>
    <h4 id="subhead"> Multi-Object Atomicity:</h4>
    <p id="subtext">
        Once a program has multiple shared objects, it becomes both necessary and challenging to reason about interactions across objects. For example, consider a system storing a bank’s accounts. A reasonable design choice might be for each customer’s account to be a shared object with a lock (either a mutual exclusion lock or a readers/writers lock, as described in Chapter 5).<br>
        a general problem that arises whenever a program contains multiple shared objects. Even if the object guarantees that each method operates atomically, sequences of operations by different threads can be interleaved. The same issues of managing multiple locks also apply to fine-grained locking within an object.<br>
    </p>
    <p id="subtext">
        Acquire-All/Release-All:<br>
        One approach, called acquire-all/release-all is to acquire every lock that may be needed at any point while processing the entire set of operations in the request. Then, once the thread has all of the locks it might need, the thread can execute the request, and finally, release the locks.<br>
        Consider a hash table with one lock per hash bucket. To move an item from one bucket to another, the hash table supports a changeKey(item, k1, k2) operation. Withacquire-all/release-all, this function could be implemented to first acquire both the locks for k1 and k2, then remove the item under k1 and insert it under k2, and finally release both locks. Acquire-all/release-all allows significant concurrency. When individual requests touch nonoverlapping subsets of state protected by different locks, they can proceed in parallel. A key property of this approach is serializability across requests: the result of any program execution is equivalent to an execution in which requests are processed one at a time in some sequential order. Serializability allows one to reason about multi-step tasks as if each task executed alone.<br>
        One challenge to using this approach is knowing exactly what locks will be needed by a request before beginning to process it. A potential solution is to conservatively acquire more locks than needed (e.g., acquire any locks that may be needed by a particular request), but this may be difficult to determine. Without first executing the request, how can we know which locks will be needed?<br>
    </p>
    <p id="subtext">
        Two Phase Locking:<br>
        <p id="subtext_bullet">
            Two phase locking refines the acquire-all/release-all pattern to address this concern. Instead of acquiring all locks before processing the request, locks can be acquired as needed for each operation. However, locks are not released until all locks needed by the request have been acquired. Most implementations simply release all locks at the end of the request.Two-phase locking avoids needing to know what locks to grab a priori. Therefore, programs can avoid acquiring locks they do not need, and they may not need to hold locks as long.<br>
            Unlike acquire-all/release-all, however, two-phase locking can in some cases lead to deadlock, the topic of the next section. Suppose one thread starts executing changeKey(item, k1, k2) and another thread simultaneously tries to move a different item in the other direction from k2 to k1. If the first thread acquires k1’s lock and the second thread acquires k2’s lock, neither will be able to make progress.<br>
        </p>
    </p>
    <h4 id="subhead">Deadlock:</h4>
    <p id="subtext">
        A challenge to constructing complex multi-threaded programs is the possibility of deadlock. A deadlock is a cycle of waiting among a set of threads, where each thread waits for some other thread in the cycle to take some action.<br>
        Deadlock can occur in many different situations, but one of the simplest is recursive locking, shown in the code fragment below:<br><br>
        <img id="medium_image" src="../assets/cs162/Deadlock.jpg" alt=""><br><br>
        The problem of deadlock is much broader than just locks and condition variables. Deadlock can occur anytime a thread waits for an event that cannot happen because of a cycle of waiting for a resource held by the first thread. As in the examples above, resources can be locks, but they can also be any other scarce quantity: memory, processing time, disk blocks, or space in a buffer.<br>
    </p>
    <p id="subtext">
        Deadlock vs. Starvation:<br>
        Deadlock and starvation are both liveness concerns. In starvation, a thread fails to make progress for an indefinite period of time. Deadlock is a form of starvation but with the stronger condition: a group of threads forms a cycle where none of the threads make progress because each thread is waiting for some other thread in the cycle to take action. Thus, deadlock implies starvation (literally, for the dining philosophers), but starvation does not imply deadlock.<br>
        Just because a system can suffer deadlock or starvation does not mean that it always will. A system is subject to starvation if a thread could starve in some circumstances. A system is subject to deadlock if a group of threads could deadlock in some circumstances. Here, the circumstances that affect whether deadlock or starvation occurs could include a broad range of factors, such as: the choices made by the scheduler, the number of threads running, the workload or sequence of requests processed by the system, which threads win races to acquire locks, and which threads are enabled in what order when signals or broadcasts occur.<br>
    </p>
    <p id="subtext">
        Necessary Conditions for Deadlock:<br>
        There are four necessary conditions for deadlock to occur. Knowing these conditions is useful for designing solutions: if you can prevent any one of these conditions, then you can eliminate the possibility of deadlock.<br>
        <p id="subtext_bullet">
            Bounded resources. There are a finite number of threads that can simultaneously use a resource<br>
            No preemption. Once a thread acquires a resource, its ownership cannot be revoked until the thread acts to release it.<br>
            Wait while holding. A thread holds one resource while waiting for another. This condition is sometimes called multiple independent requests because it occurs when a thread first acquires one resource and then tries to acquire another.<br>
            Circular waiting. There is a set of waiting threads such that each thread is waiting for a resource held by another.<br>
        </p>
    </p>
    <p id="subtext">
        Preventing Deadlock:<br>
        Exploit or limit the behavior of the program. Often, we can change the behavior of a program to prevent one of the four necessary conditions for deadlock, and thereby eliminate the possibility of deadlock. In the above example, we can eliminate deadlock by changing the program to never wait for B while holding C.<br>
        Predict the future. If we can know what threads may or will do, then we can avoid deadlock by having threads wait (e.g., thread 2 can wait at step 2 above) before they would head into a possible deadlock.<br>
        Detect and recover. Another alternative is to allow threads to recover or “undo” actions that take a system into a deadlock; in the above example, when thread 2 finds itself in deadlock, it can recover by reverting to an earlier state.<br>
        Bounded resources: Provide sufficient resources. One way to ensure deadlock freedom is to arrange for sufficient resources to satisfy all threads’ demands. A simple example would be to add a single chopstick to the middle of the table in Dining Philosophers; that is enough to eliminate the possibility of deadlock. As another example, thread implementations often reserve space in the TCB for the thread to be inserted into a waiting list or the ready list. While it would be theoretically possible to dynamically allocate space for the list entry only when it is needed, that could open up the chance that the system would run out of memory at exactly the wrong time, leading to deadlock.<br>
        No preemption: Preempt resources. Another technique is to allow the runtime system to forcibly reclaim resources held by a thread. For example, an operating system can preempt a page of memory from a running process by copying it to disk in order to prevent applications from deadlocking as they acquire memory pages.<br>
        Wait while holding: Release lock when calling out of module. For nested modules, each of which has its own lock, waiting on a condition variable in an inner module can lead to a nested waiting deadlock. One solution is to restructure a module’s code so that no locks are held when calling other modules.<br>
        Circular waiting: Lock ordering. An approach used in many systems is to identify an ordering among locks and only acquire locks in that order.<br>
    </p>
    <p id="subtext">
        The Banker's Algorithm:<br>
        A general technique to eliminate wait-while-holding is to wait until all needed resources are available and then to acquire them atomically at the beginning of an operation, rather than incrementally as the operation proceeds. We saw this earlier with acquire-all/release-all; it cannot deadlock as long as the implementation acquires all of the locks atomically rather than one at a time. As another example, a dining philosopher might wait until the two neighboring chopsticks are available and then simultaneously pick them both up.<br>
        Of course, a thread may not know exactly which resources it will need to complete its work, but it can still acquire all resources that it might need. Consider an operating system for mobile phones where memory is constrained and cannot be preempted by copying it to disk. Rather than having applications request additional memory as needed, we might instead have each application state its maximum memory needs and allocate that much memory when it starts. Disadvantages of this approach include: the effect on program modularity, the challenge of having applications accurately estimate their worst-case needs, and the cost of allocating significantly more resources than may be necessary in the common case.<br>
        In the Banker’s Algorithm, a thread states its maximum resource requirements when it begins a task, but it then acquires and releases those resources incrementally as the task runs. The runtime system delays granting some requests to ensure that the system never deadlocks. The insight behind the algorithm is that a system that may deadlock will not necessarily do so: for some interleavings of requests it will deadlock, but for others it will not. By delaying when some resource requests are processed, a system can avoid interleavings that could lead to deadlock.<br>
        The Banker’s Algorithm keeps a system in a safe state. The algorithm is based on a loose analogy with a small-town banker who has a maximum amount, total, that can be loaned at one time and a set of businesses that each have a credit line, max[i], for business i. A business borrows and pays back amounts of money as various projects start and end, so that business i always has an outstanding loan amount between 0 and max[i]. If all of a business’s requests within the credit line are granted, the business eventually reaches a state where all current projects are finished, and the loan balance returns to zero.<br>
        A conservative banker might issue credit lines only until the sum is at most the total funds that the banker has available. This approach is analogous to acquire-all or provide sufficient resources. It guarantees that the system remains in a safe state. All businesses with credit lines eventually complete their projects. However, a more aggressive banker can issue more credit as long as the bank can cover its commitment to each business — i.e., to provide a loan of max[i] if business i requests it. The algorithm assumes the bank is permitted to delay requests to increase a loan amount. For example, the bank might lose the paperwork for a few hours, days, or weeks. By delaying loan requests, the bank remains in a safe state — a state for which there exists at least one series of loan fulfillments by which every business i can eventually receive its maximal loan max[i], complete its projects, and pay back all of its loan. The bank can then use that repaid money to grant pending loans to other businesses.<br>
        The high-level idea is simple: when a request arrives, wait to grant the request until it is safe to do so. As Figure 6.19 shows, we can realize this high-level approach by tracking: (i) the current allocation of each resource to each thread, (ii) the maximum allocation possible for each thread, and (iii) the current set of available, unallocated resources. Figure 6.21 shows how to test whether a state is safe. Recall that a state is safe if some sequence of thread executions allows each thread to obtain its maximum resource need, finish its work, and release its resources. We first see if the currently free resources suffice to allow any thread to finish. If so, then the resources held by that thread will eventually be  released back to the system. Next, we see if the currently free resources plus any resources held by the thread identified in the first step suffice to allow any other thread to finish; if so, the second thread’s resources will also eventually be released back to the system. We continue this process until we have identified all threads guaranteed to finish, provided we serve requests in a particular order. If that set includes all of the threads, the state is safe.<br>
        The Banker’s Algorithm is noticeably more involved than other approaches we discuss. Although it is rarely used in its entirety, understanding the distinction between safe, unsafe, and deadlocked states and how deadlock events depend on request ordering are key to preventing deadlock. Additionally, understanding the Banker’s Algorithm can help to design simple solutions for specific problems.<br>
    </p>
    <p id="subtext">
        Detecting and Recovering From Deadlocks:<br>
        Recovering:<br>
        <p id="subtext_bullet">
            Rather than preventing deadlocks, some systems allow deadlocks to occur and recover from them when they arise. Why allow deadlocks to occur at all? Sometimes, it is difficult or expensive to enforce sufficient structure on the system’s data and workloads to guarantee that deadlock will never occur. If deadlocks are rare, why pay the overhead in the common case to prevent them? For this approach to work, we need: (i) a way to recover from deadlock when it occurs, ideally with minimal harm to the goals of the user, and (ii) a way to detect deadlock so that we know when to invoke the recovery mechanism. We discuss recovery first because it provides context for understanding the tradeoffs in implementing detection.<br>
            Proceed without the resource. Web services are often designed to be resilient to resource unavailability. A rule of thumb for the web is that a significant fraction of a web site’s customers will give up and go elsewhere if the site’s latency becomes too long, for whatever reason. Whether the problem is a hardware failure, software failure, or deadlock, does not really matter. The web site needs to be designed to quickly respond back to the user, regardless of the type of problem. Amazon’s web site is a good example of this design paradigm. It is designed as an interlocking set of modules, where any individual module can be offline because of a failure. Thus, all other parts of the web site must be designed to be able to cope when some needed resource is unavailable. For example, under normal operation, Amazon’s software will check the inventory to ensure that an item is in stock before completing an order. However, if a deadlock or failure causes the inventory server to delay responding beyond some threshold, the front-end web server will give up, complete the order, and then queue a background check to make sure the item was in fact in the inventory. If the item was in fact not available (e.g., because some other customer purchased it in the meantime), an apology is sent to the customer. As long as that does not happen often, it can be better than making the customer wait, especially in the case of deadlock, where the wait could be indefinite. Because deadlocks are rare and hard to test for, this requires coding discipline to handle error conditions systematically throughout the program.<br>
            Transactions: rollback and retry. A more general technique is used by transactions; transactions provide a safe mechanism for revoking resources assigned to a thread. We discuss transactions in detail in Chapter 14; they are widely used in both databases and file systems. For deadlock recovery, transactions provide two important services: 1. Thread rollback. Transactions ensure that revoking locks from a thread does not leave the system’s objects in an inconsistent state. Instead, we rollback, or undo, the deadlocked thread’s actions to a clean state. To fix the deadlock, we can choose one or more victim threads, stop them, undo their actions, and let other threads proceed. 2. Thread restarting. Once the deadlock is broken and other threads have completed some or all of their work, the victim thread is restarted. When these threads complete, the system behaves as if the victim threads never caused a deadlock but, instead, just had their executions delayed.<br>
            A key feature of transactions is that no other thread is allowed to see the results of a transaction until the transaction completes. That way, if the changes a transaction makes need to be rolled back due to a deadlock, only that one thread is affected. This can be accomplished with two-phase locking, provided locks are not released until after the transaction is complete. If the transaction is successful, it commits, the transaction’s locks are released, and the transaction’s changes to shared state become visible to other threads. If, however, a transaction fails to reach its endTransaction statement (e.g., because of a deadlock or because some other exception occurred), the transaction aborts. The system can reset all of the state modified by the transaction to what it was when the transaction began. One way to support this is to maintain a copy of the initial values of all state modified by each transaction; this copy can be discarded when the transaction commits. If a transactional system becomes deadlocked, the system can abort one or more of the deadlocked transactions. Aborting these transactions rolls back the system’s state to what it would have been if these transactions had never started and releases the aborted transactions’ locks and other resources. If aborting the chosen transactions releases sufficient resources, the deadlock is broken, and the remaining transactions can proceed. If not, the system can abort additional transactions.<br>
            A related question that arises in transactional systems is which thread to abort and which threads to allow to proceed. An important consideration is liveness. Progress can be ensured, and starvation avoided, by prioritizing the oldest transactions. Then, when the system needs to abort some transaction, it can abort the youngest. This ensures that some transaction, e.g., the oldest, will eventually complete. The aborted transaction eventually becomes the oldest, and so it also will complete. An example of this approach is wound wait. With wound wait, a younger transaction may wait for a resource held by an older transaction. Eventually, the older transaction will complete and release the resource, so deadlock cannot result. However, if an older transaction needs to wait on a resource held by a younger transaction, the resource is preempted and the younger transaction is aborted and restarted.<br>    
        </p>
    </p>
    <p id="subtext">
        Detecting:<br>
        <p id="subtext_bullet">
            Once we have a general way to recover from a deadlock, we need a way to tell if a deadlock has occurred, so we know when to trigger the recovery. An important consideration is that the detection mechanism can be conservative: it can trigger the repair if we might be in a deadlock state. This approach risks a false positive where a nondeadlocked thread is incorrectly classified as deadlocked. Depending on the overhead of the repair operation, it can sometimes be more efficient to use a simpler mechanism for detection even if that leads to the occasional false positive.<br>
            If there are several resources and only one thread can hold each resource at a time (e.g., one printer, one keyboard, and one audio speaker or several mutual exclusion locks), then we can detect a deadlock by analyzing a simple graph. In the graph, shown on the left in Figure 6.22, each thread and each resource is represented by a node. There is a directed edge (i) from a resource to a thread if the resource is owned by the thread and (ii) from a thread to a resource if the thread is waiting for the resource. There is a deadlock if and only if there is a cycle in such a graph.<br>
        </p>
    </p>
    <h4 id="subhead">Non-Blocking Synchronization:</h4>
    <p id="subtext">
        Chapter 5 described a core abstraction for synchronization — shared objects, with one lock per object. This abstraction works well for building multi-threaded programs the vast majority of the time. As concurrent programs become more complicated, however, issues of lock contention, the semantics of operations that span multiple objects, and deadlock can arise. Worse, the solutions to these issues often require us to compromise modularity; for example, whether a particular program can deadlock requires understanding in detail how the implementations of various shared objects interact. Some researchers have posed a radical question: would it be better to write complex concurrent programs without locks? By eliminating locking, we would remove lock contention and deadlock as design considerations, fostering a more modular program structure. However, these techniques can be much more complex to use. To date, concurrent implementations without locks have only been used for a few carefully designed runtime library modules written by expert programmers. We sketch the ideas because there is a chance that they will become more important as the number of processors per computer continues to increase.<br>
        Today, the cases where these approaches are warranted are rare. These advanced techniques should only be considered by experienced programmers who have mastered the basic lock-based approaches. Many of you will probably never need to use these techniques. If you are tempted to do so, take extra care. Measure the performance of your system to ensure that these techniques yield significant gains, and seek out extra peer review from trusted colleagues to help ensure that the code works as intended. Programmers often assume that acquiring a lock is an expensive operation, and therefore try to reduce locking throughout their programs. The most likely result from premature optimization is a program that is buggy, hard to maintain, no faster than a clean implementation, and, ironically, harder to tune than a cleanly architected program. On most platforms, acquiring or releasing a lock is a highly tuned primitive — acquiring an uncontended lock is often nearly free. If there is contention, you probably needed the lock! In Section 6.3, we saw an example of synchronization without locks. RCU lets reads proceed without acquiring a lock or updating shared synchronization state, but it still requires updates to acquire locks. If the thread that holds the lock is interrupted, has a bug that causes it to stop making progress, or becomes deadlocked, other threads can be delayed for a long — perhaps unlimited — period of time.<br>
        It is possible to build data structures that are completely non-blocking for both read and write operations. A non-blocking method is one where one thread is never required to wait for another thread to complete its operation. Acquiring a lock is a blocking operation: if the thread holding the lock stops, is delayed, or deadlocks, all other threads must wait for it to finish the critical section. More formally, a wait-free data structure is one that guarantees progress for every thread: every method finishes in a finite number of steps, regardless of the state of other threads executing in the data structure or their rate of execution. A lock-free data structure is one that guarantees progress for some thread: some method will finish in a finite number of steps.<br>
        A common building block for wait-free and lock-free data structures is the atomic compareand-swap instruction available on most modern processors. We saw a taste of this in the implementation of the MCS lock in Section 6.3. There, we used compare-and-swap to atomically append to a linked list of waiting threads without first acquiring a lock. Wait-free and lock-free data structures apply this idea more generally to completely eliminate the use of locks. For example, a lock-free hash table could be built as an array of pointers to each bucket:<br><br>
        Lookup. A lookup de-references the pointer and checks the bucket.<br>
        Update. To update a bucket, the thread allocates a new copy of the bucket, and then uses compare-and-swap to atomically replace the pointer if and only if it has not been changed in the meantime. If two threads simultaneously attempt to update the bucket (for example, to add a new entry), one succeeds and the other must retry<br><br>
        The logic can be much more complex for more intricate data structures, and as a result, designing efficient wait-free and lock-free data structures remains the domain of experts. Nonetheless, non-blocking algorithms exist for a wide range of data structures, including FIFO queues, double-ended queues, LIFO stacks, sets, and balanced trees. Several of these can be found in the Java Virtual Machine runtime library. In addition, considerable effort has also gone into studying ways to automate the construction of wait-free and lock-free data structures. For example, transactions with optimistic concurrency control provide a very flexible approach to implementing lock-free applications. Recall that optimistic concurrency control lets transactions proceed without locking the data they access. Transactions abort if, at commit-time, any of their accessed data has changed in the meantime. Most modern databases use a form of optimistic concurrency control to provide atomic and fault-tolerant updates of on-disk data structures.<br>
    </p>


</body>
</html>