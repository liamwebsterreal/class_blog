<!DOCTYPE html>
<html>
    <head>
        <title>Hi</title>
        <link href="../css/styles.css" rel="stylesheet" type="text/css">
      </head>
<body>

<h2>Computer Science 61c: Great Ideas of Computer Architecture</h2>
<a href="../index.html">Home</a>
<br>
<br>
<div id="toc_container">
    <p class="toc_title">Content:</p>
    <ul class="toc_list">
    <li><a href="#FP1">Introduction</a></li>
    <li><a href="#FP2">Number Representation</a></li>
    <li><a href="#FP3">C Programming Language</a></li>
    <li><a href="#FP4">Memory</a></li>
    <li><a href="#FP5">Floating Point</a></li>
    <li><a href="#FP6">RISC-V Language</a></li>
    <li><a href="#FP7">CALL(compiler,assembler,linker,loader)</a></li>
    <li><a href="#FP8">Synchronous Digital Systems</a></li>
    <li><a href="#FP9">Datapath</a></li>
    <li><a href="#FP10">Caches</a></li>
    <li><a href="#FP11">Parallelism</a></li>
    <li><a href="#FP12">Operating  System</a></li>

    </ul>
    </div>
    <h3 id="FP1">Introduction</h3>
    <p id="subtext">
        One of my favorite courses I have ever taken. I really began to understand the inner workings of a computer. This class helps to demystify the hardware-software interface.<br>
        The five great ideas in Computer Architecture:<br>
        <img id="medium_image" src="../assets/oldshardwaresoftwaremap.PNG" alt="Old Hardware-Software Interface Map">
        <img id="medium_image" src="../assets/newhardwaresoftwaremap.PNG" alt="New Hardware-Software Interface Map">
        <h4 id="subhead">Great Ideas of Computer Science:</h4>
        <p id="subtext_bullet">
            Abstraction(layers of representation/Interpretation<br>
            Moore's Law(designing through trends)<br>
            Principle of Locality(memory hierarchy)<br>
            Parallelism & Amdahl's law<br>
            Dependability vs Redundancy<br>
        </p>
    </p>
    <h3 id="FP2">Number Representation</h3>
    <h4 id="subhead">Binary:</h4>
    <p id="subtext">
        Binary: a method of representing numbers using a string of 0s and 1s. The fundamental building block of a computer is a transistor which can only represent two values. Out of simplicity there two values are 0 and 1. Denote a binary number by prepending "0b" or appending a subscripted 2.<br>
        Terminology:<br>
        <p id="subtext_bullet">
            Bit: 1 binary digit, Nibble: 4 binary digits, Byte: 8 binary digits<br>
            Base: the number of different digits that a system has to represent numbers.<br>
            Most Significant bit(MSB): the bit in the highest position(typically furthest to the left).<br>
            Least Significant bit(LSB): the bit in the lowest position(typically furthest to the right).<br>
            Not leading zeros don't change the value.<br>
        </p>
    </p>
    <p id="subtext">
        Binary Addition/Subtraction: same as elementary stack on top of each other addition. Overflow occurs when you cannot represent the result of the operation in the given number of bits.<br>
        With n binary digits(bits) we can represent 2^n values/things. If our range starts at 0 then we can represent [0,2^n - 1] values.<br>
    </p>
    <p id="subtext">
        Conversion(Binary to Decimal | Decimal to Binary):<br>
        <img id="medium_image" src="../assets/binarytodecimal.PNG" alt="Binary to Decimal">
        <img id="medium_image" src="../assets/deciamltobinary.PNG" alt="Decimal to Binary"><br>
    </p>
    <h4 id="subhead">Hexadecimal:</h4>
    <p id="subtext">
        Hexadecimal: method of representing binary that's easier for humans to read, base 16. One hex digit can represent 16 numbers. One hex digit = 1 nibble.<br>
        With n hex digits(4 bits) we can represent 16^n values. If our range starts at 0 then we cna represent [0,16^n - 1] values.<br>
        Denote a hexadecimal value by prepending "0x" or appending a subscripted 16.<br>
        Conversion:<br>
        <img id="medium_image" src="../assets/binarytohex.PNG" alt="Binary to Hex">
        <img id="medium_image" src="../assets/binarytodecimal.PNG" alt="Hex to Binary">
        <img id="medium_image" src="../assets/hextodecimal.PNG" alt="Hex to Decimal">
        <img id="medium_image" src="../assets/decimaltohex.PNG" alt="Decimal to Hex">
    </p>
    <h4 id="subhead">Sign and Magnitude:</h4>
    <p id="subtext">
        We need a way to represent negative values. This scheme does just that:<br>
        The MSB represents the sign of the value: 0 for positive and 1 for negative. The remaining bits represent the magnitude of the number.
        Example: -5<br>
        <p id="subtext_bullet">
            Sign: 1 | Magnitude: 0101 thus the answer = 10101<br>
        </p>
    </p>
    <p id="subtext">
        Range of Sign and Magnitude: Recall the range or unsigned representation is [0,2^n - 1]. We can represent this range minus one bit for the sign in both positive and negative values thus [-(2^(n-1) - 1), 2^(n-1) - 1].<br>
        There are problems with Sign and Magnitude. There are two zeros since we can have negative and positive zero this doesn't make any sense. This makes it difficult to implement in hardware.<br>
    </p>
    <h4 id="subhead">Two's Complement:</h4>
    <p id="subtext">
        To eliminate the two zeros we face in the Sign and Magnitude representation we implement Two's Complement.<br>
        To represent the negative value take the positive value flip the bits and add one.<br>
        Range of Two's Complement: We shifted the negative values over by one, eliminating the negative zero, and meaning that we can represent one more negative value. Thus we have [-(2^(n-1), 2^(n-1) - 1)] values.<br>
        Two's Complement Addition/Subtraction: works just at normal binary addition in this case for subtraction just add the two complement(negative value). <br>
        Two's Complement Overflow: when the result of an operation cannot be represented in the given number of bits. When adding two positive numbers overflow occurs when the result if negative. When adding two negative numbers, overflow occurs when the result is positive. Overflow will never occur when adding two numbers of opposite signs.
        Examples:
        <img id="medium_image" src="../assets/twocomplementex.JPG" alt="Two's Complement Examples">   
        <img id="medium_image" src="../assets/twoscomplementoverflow.JPG" alt="Two's Complement Overflow Examples"><br>   
    </p>
    <h4 id="subhead">Bias Encoding:</h4>
    <p id="subtext">
        A method for storing a range or values where the lowest value is encoded as all zeros.<br>
        Bias Encoding Convention: non-biased -> bias then subtract the bias | bias -> non-biased add the bias<br>
        Bias Formula for 2's Complement number with n bits: N = -(2^(n-1) - 1)<br>
        <img id="medium_image" src="../assets/nonbiastobiasencoding.PNG" alt="Non-Biased to Biased Encoding">  
        <img id="medium_image" src="../assets/biastononbiasencoding.PNG" alt="Biased to Non-Biased Encoding">   
    </p>
    <h4 id="subhead">Prefixes:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/presets.png" alt="Prefixes">
        <img id="medium_image" src="../assets/convertIEC.PNG" alt="Converting base 2 to IEC">
    </p>
    <h3 id="FP3">C Programming Language</h3>
    <p id="subtext">
        "C is not a "ver high-level" language, nor a "big" one, and is not specialized to any particular are of application. Buts its absence of restrictions and its generality make it more convenient and effective for many tasks than supposedly more powerful languages" -Kernie and Ritchie. Enabled the first operating system not written in assembly language: UNIX - a portable OS.<br>
        In C programs that allow us to exploit underlying features of architecture(memory management, etc) adn do it in a portable way(C compilers universally available for all existing processor architectures).<br>
        <img id="medium_image" src="../assets/cvsjava1.PNG" alt="C vs Java">
        <img id="medium_image" src="../assets/cvsjava2.PNG" alt="C vs Java"><br>
    </p>
    <h4 id="subhead">Declaration:</h4>
    <p id="subtext">
        Must declare the type of data a variable will hold, the type of data a function will return, the type of data arguments are, and declare functions(usually in a separate header file).<br>
        Variable Declarations: All variable declarations must appear before they are used. All must be at the beginning of a block. A variable may be initialized in its declaration.<br>
        C only guarantees minimum and relative size of "int" "short" etc. If you need to know the exact size specify with uint8_t(8 bit unsigned integer) or int64_t(64 bit integer) etc.<br>
        Constant is assigned a typed value once in the declaration; value can't change during the entire execution of program. <br>
        Structs are structured groups of variables with type. Stucts allocate enough memory and padding for all said parameters.<br>
        Unions are structure groups of variables with type, but the memory allocated is for the largest type in the union. Essentially one var is active in a union.<br>
    </p>
    <h4 id="subhead">Control Flow:</h4>
    <p id="subtext">
        Control Flow: very similar to Java. if-else, while, for, switch, goto.<br>
        <img id="medium_image" src="../assets/Ccontrolflow1.PNG" alt="C Control Flow"><br>

    </p>
    <h4 id="subhead">Memory:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/cmemorymodel.JPG" alt="C Control Flow"><br>
    </p>
    <h4 id="subhead">Pointers:</h4>
    <p id="subtext">
        An address refers to a particular memory location aka it points to a memory location.<br>
        Pointer: a variable that contains the address of a variable.<br>
        C as well as Java pass basic parameter "by value" we can use pointers to pass "by reference".<br>
        Important to Pointer Arithmetic is the compile time operation sizeof() takes an arg such as char; sizeof(structtype)<br>
        You can cast basic C types.<br>
        <img id="medium_image" src="../assets/cpointer2.JPG" alt="C Pointer Syntax">
        <img id="medium_image" src="../assets/pointersyntax.JPG" alt="C Pointer Syntax"><br>
    </p>
    <h4 id="subhead">Arrays:</h4>
    <p id="subtext">
        In C Array variable is simply a "pointer" to the first (0th) element. Os array variables are almost identical to pointers: char *string and char string[] are nearly identical declarations. Thus a[i] == *(a+1). Note an array is passed to a function as a pointer so the array size is lost so we must pass the size with it.<br>
        C strings are just arrays of chars: char string[] = "abc". Last character is always followed by a 0 byte(null terminator "\0"), the string length operator stops at this terminator and does not include it in the length it returns.<br>
    </p>
    <h4 id="subhead">Arguments in main():</h4>
    <p id="subtext">
        To get arguments to the main function use: int main(int argc, char *argv[])<br>
        agrc: contains the number of strings on the command line<br>
        argv: pointer to an array containing the arguments as strings<br>
    </p>
    <h4 id="subhead">Endianness:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/endianness.JPG" alt="Endianness"><br>
    </p>
    <h3 id="FP4">Memory Management</h3>
        <p id="subtext">
        <img id="medium_image" src="../assets/memorymanagement1.JPG" alt="Memory Management"><br>
        </p>
    <h4 id="subhead">Where are Variables Allocated?:</h4>
        <p id="subtext">
            If declared outside a function allocated in "static" storage.<br>
            If declared inside the function, allocated on the "stack" and freed when function returns(Note main() is treated like a function).<br>
            For both of these types of memory the management is automatic: no need to deallocate when no longer using, but a variable DNE once a function ends.<br>
        </p>
    <h4 id="subhead">The Stack:</h4>
        <p id="subtext">
            Every time a function is called, a new "stack frame" is allocated on the stack.<br>
            Stack frame includes: return addresses, arguments, space for local variables.<br>
            Stack frames use contiguous blocks of memory; stack pointer indicates start of stack frame. When function ends stack pointer moves up; frees memory for future stack frames.<br> 
        </p>
    <h4 id="subhead">The Heap:</h4>
        <p id="subtext">
            C functions for heap management:<br>
            <p id="subtext_bullet">
                malloc(): allocate a block of uninitialized memory<br>
                calloc(): allocate a block of zeroed memory<br>
                free(): free previously allocated block of memory<br>
                realloc(): change size of previously allocated block(Note block may move and will not update pointers pointing to the same block of memory)<br>
            </p>
        </p>
    <h4 id="subhead">Observations:</h4>
        <p id="subtext">
            Code and static storage are easy: they never grow or shrink<br>
            Stack space is relatively easy: stack frames are created and destroyed in last-in, first-out order(LIFO)<br>
            Managing the heap is tricky: memory can be allocated/deallocated at any time<br>
            <p id="subtext_bullet">
                If you forget to deallocate memory: "Memory Leak"(program will eventually run out of memory)<br>
                If you call free twice on the same memory: "Double Free"(possible crash or exploitable vulnerability)<br>
                If you use data after calling free: "Use after free"(crash or exploitable vulnerability)<br>
                Examples:
                Failure to free allocated memory. Remember to free memory upon function return or don't loose the pointer to memory in a callee function then you won't be able to free the memory.<br>
                Writing off the end of arrays.<br>
                Returning pointers up into the stack. Say a callee returns an array what was declared in itself. Then jumping back up the stack that array will eventually be overwritten.<br>
                Trying ti access memory that has been freed likely won't be the same memory and get incorrect results.<br>
                Free the wrong stuff, freeing things that weren't malloc'd. Or double freeing something.<br>
            </p>
        </p>
    <h4 id="subhead">Alignment:</h4>
    <p id="subtext">
        These are the default alignment(centered around a "32b architecture": integers and pointers are 32b values):<br>
        <p id="subtext_bullet">
            char: 1 byte, no alignment needed when stored in memory.<br>
            short: 2 bytes, 1/2 word aligned<br>
            int & pointers: 4 bytes, word aligned<br>

        </p>
    </p>
    <h3 id="FP5">Floating Point</h3>
    <h4 id="subhead">Bitwise Operations:</h4>
    <p id="subtext">
        We have the boolean operations:<br>
        <p id="subtext_bullet">
            || := boolean or<br>
            && := boolean and<br>
        </p>
    </p>
    <p id="subtext">
        We have the bitwise operations:<br>
        <p id="subtext_bullet">
            Treat the data as raw bits and apply them on a bit by bit bases.<br>
            | := bitwise or,  Ex: 0b0011 | 0b0101 = 0b0111<br>
            & := bitwise and, Ex: 0b0011 & 0b0101 = 0b0001<br>
            ^ := bitwise xor, Ex: 0b0011 ^ 0b0101 = 0b0110<br>
        </p>
    </p>
    <p id="subtext">
        We have the bit shift operations:<br>
        <p id="subtext_bullet">
            a << b := shift the value in a to the left by b bits, shifting in 0.<br>
            Equivalent to multiplying by 2^b<br>
            0b00101 << 2 = 0b10100(Note bits off the left are dropped)<br>
            a >> b := shift the value in a to the right by b bits.<br>
            If a is signed we sign extend(copy MSB)<br>
                Ob10100 >> 2 = 0b11101<br>
            If a is unsigned we zero extend<br>
                0b10100 >> 2 = 0b00101<br>
            Note this is not quite the same as dividing by 2^b due to rounding.<br>
        </p>
    </p>
    <h4 id="subhead">IEEE 754 Floating-Point Standard:</h4>
    <p id="subtext">
        Standard arithmetic for all computers(important because computer representation of real numbers is approximate).<br>
        Keep as much precision as possible.<br>
        Help programmer with errors in real arithmetic(infinity, NaN, exponent overflow, etc)<br>
        Keep encoding such that it is somewhat compatible with two's compliment.<br>
        <img id="medium_image" src="../assets/floatpoint.JPG" alt="Floating Point"><br>
        <br>
    </p>
    <p id="subtext">
        Mantissa: In normalized form there must be one non-zero number to the left of the point, in binary the only non-zero number is 1 so every binary number written in normalized form will have 1 to the left of the point(except 0). We can save a bit by not storing this 1.<br>
        Exponent: Is written in biased notation so that the smallest number is written as all zeros. The range is [-126,127]. The exponent is biased by adding 127 to get the number into the range [1,254]; 0 and 255 have special meanings.(Note we bias the exponent so comparisons are easier because you can just perform an unsigned comparison). For IEEE-754 32 bit floating point numbers there are 8 exponent bits: Bias = -(2^(8-1) - 1) = -127<br>
        Range of floating point: Positive: [2^-126, (2 - 2^(-23) * 2^127)] Negative: [-(2 - 2^-23) * 2^127, -2^(-126)]<br>
        <img id="medium_image" src="../assets/floatpointchart.JPG" alt="Floating Point Chart">
        <img id="medium_image" src="../assets/floatpointrange.JPG" alt="Floating Point Range"><br>
        Floating Point Step Size: 
        <p id="subtext_bullet">
            If x is the biased exponent and y is the significand; how do we write our current number in terms of x and y?<br>
            (1 + y) * 2^(x-127)<br>
            How do we write the next number in terms of x and y?<br>
            (1 + y + 2^(-23)) * 2^(x-127)<br>
            So Step Size = next_num - current_num: (1 + y + 2^-23) * 2^(x-127) - (1 + y) * 2(x-127) = 2^(x-150)<br>
            The step size increases by a factor of 2 every time the exponent increases by 1.<br>
        </p>
    </p>
    <p id="subtext">
        Denormalized Numbers: The gap between 0->(smallest positive number) = 2^-126 but the gap between 2^-126->(the next smallest positive number) = 2^-149. Denormalized numbers allow us to get closer to zero. An exponent field of all zeros encodes a denormalized number.<br>
        <img id="medium_image" src="../assets/denormalized.JPG" alt="Denormalized Floating Point"><br>
        Range of Denormalized Floating Point: Positive[2^-149, 2^-126 - 2^-149] Negative[-(2^-126 - 2^-149), -(2^-149)]<br>
        Denormalized Floating Point Step Size: The step size is the same for all denorm values because the exponent stays constant: 2^-149.<br>
    </p>
    <p id="subtext">
        Rounding issues will occur:<br>
        <p id="subtext_bullet">
            Round to Nearest: round to nearest number; if the number falls midway it is rounded to the nearest value with an even(zero) least significant bit, which means its rounded up/down 50/50 percent.<br>
            Round toward 0: truncate<br>
            Round toward positive infinity: round up always.<br>
            Round toward negative infinity: round down always.<br>
        </p>
    </p>
    <p id="subtext">
        Floating Point addition is not associative
    </p>    
    <h3 id="FP6">RISC-V Language</h3>
    <p id="subtext">
        Assembly Language: Basic job of a CPU: execute instructions one after another in sequence, each instruction does a small amount of work.<br>
        Different CPU's implement different sets of instructions. The set of instructions that a particular CPU implements is called its Instruction Set Architecture(ISA).<br>
        RISC(Reduced Instruction Set Computer): A single instruction can only perform one operation. Keep the instruction set small and simple, makes it easier to build fast hardware. Philosophy developed by Cocke IBM, Patterson, Hennessy, 1980s<br>
    </p>
    <img id="medium_image" src="../assets/greencard1.png" alt="RISC-V Green Card">
    <img id="medium_image" src="../assets/greencard2.png" alt="RISC-V Green Card"><br><br>
    <p id="subtext">
    <h4 id="subhead">Registers:</h4>
        <p id="subtext">
            Unlike high-level languages like C or Java assembly languages do not use variables. Instead they use registers, small storage units that are located in the processor. Operations are performed on registers. Registers are extremely fast due to their location and size.<br>
            Notes: 32 registers in RISC-V, word = 32 bits, Register FIle: the general purpose registers inside of the processor.<br>
            Registers are numbered from 0 to 31; referred to as x0-x31. x0 always holds the value 0. Registers have no type.<br>
            <img id="medium_image" src="../assets/computermemoryarchitecture.JPG" alt="Memory/Register Architecture"><br>
        </p>    
    </p>
    <h4 id="subhead">Instruction Formats:</h4>
    <p id="subtext">
        Each RISC-V is 32 bits wide, its broken down into different field in an order corresponding with its respective instruction type format.<br>
        opcode: partially specifies which instruction it is.<br>
        funct7+funct3: combined with opcode, these two fields describe what operation to perform.<br>
        See Green Card for specificities.<br>
        <img id="medium_image" src="../assets/riscvinstructionformat.JPG" alt="Instruction Formats"><br>

    </p>
    <h4 id="subhead">Specificities:</h4>
    <p id="subtext">
        See green card for function specificities.<br>
        Shifting:<br>
        <p id="subtext_bullet">
            Shift Left Logical:<br>
            Shift left by a register value<br>
            sll x10, x11, x12 #x10 = x11 << x12<br>
            Shift left by a constant value:<br>
            slli x10, x11, 2 #x10 = x11 << 2<br>
            Shifting left the left bits fall off and zeros are inserted on the right. Left shifting by n is equivalent to multiplying by 2^n.<br>
            <br>
            Right Shift Logical:<br>
            shift right by register value or immediate: the bits on the right fall off and zeros are inserted on the left.<br>
            srl|i x10, x11, x12/2 #x10 = x11 << x12|2<br>
            Right Shift Arithmetic:<br>
            shift right by register value or immediate: the bits on the right fall off and the bits are sign extended.<br>
            sra|i x10, x11|2, x12 #x10 = x11 >> x10|2<br>
            Right shifting positive numbers and even numbers is equivalent to dividing by 2^n with the factional part of the result being truncated. Right shifting by negative odd numbers is equivalent to dividing by 2^n and rounding the result towards negative infinity.<br>
        </p>
    </p>
    <p id="subtext">
        Branching:<br>
        <p id="subtext_bullet">
            Branch instructions change the control flow of the program. There are two types of branch instructions: conditional and unconditional.<br>
            Labels are used to give control flow instructions a place to go.<br>
            Note blt and bge perform signed comparisons of the numbers. To perform unsigned comparison use bltu and bgeu.<br>
            Program Counter: is a register that holds the memory address of the instruction being executed.<br>
            When we jump to a function we need a return address use jal rd, label(rd=register where the return address will be stored). When we jump because of a loop or branch we don't need a return address we can use jal x0 label == j label. Note label is a 20 bit offset so we cannot jump everywhere in memory, so we have another instruction: jalr rd, rs, imm(rd=register where return address is stored, rs=register containing the base address, imm=immediate value to be added to the base register, PC=rs+imm. Conditional branch instructions have a limited range of +- 2^10 32 bit instructions. J format branch instructions have a range of +- 2^18 32 bit instructions. JALR branch instructions have a range of 2^12 instructions. <br>
            When we call another function, we assume the registers are overwritten. If the caller register are need for future use we have to save them into the stack. Calling convention delegates that the caller save temporary registers and the callee saves saved registers.<br>
            PC Relative Addressing: jump to a location based on the current location of the PC: PC + offset.<br>
            PC Absolute Addressing: jump to a location using that locations full address. LUI and AUIPC can be used to jump via PC relative or absolute addressing.<br>
        </p>
    </p>
    <p id="subtext">
        <img id="medium_image" src="../assets/callingconvention.JPG" alt="Calling Convention"><br>
    </p>
    <h3 id="FP7">CALL(compiler,assembler,linker,loader)</h3>
    <h4 id="subhead">Language Execution Continuum:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/languageexecution.JPG" alt="Language Execution Continuum"><br>
    </p>
    <h4 id="subhead">Interpretation vs Translation:</h4>
    <p id="subtext">
        Interpreter: Directly executes a program in the source code. Its generally easier to write an interpreter. Interpreter closer to high-level so it can give better error messages. Interpreter is slower, code smaller. Interpreter provides instruction set independence: run on any machine.<br>

        Translator: Converts a program from the source language to an equivalent program in another language. Translated/Compiled code almost always more efficient and therefore higher performance. Compiled code does the hard work once: during compilation.<br>
    </p>
    <h4 id="subhead">CALL chain: Compiling a C program:</h4>
    <p id="subtext">
        <img id="medium_image" src="../assets/compilingCprogram.JPG" alt="Compiling a C Program"><br>
    </p>
    <p id="subtext">
        Compiler:<br>
        Input: high level language code(e.g. foo.c)<br>
        Output: assembly language code(e.g. foo.s for RISC-V)<br>
        Steps in Compiler:<br>
        <p id="subtext_bullet">
            Lexer: turns the input into "tokens" recognizes problems with the tokens<br>
            Parser: turns the tokens into an "Abstract Syntax Tree", recognizes problems in the program structure<br>
            Semantic Analysis and Optimization: checks for semantic errors, may reorganize the code to make it better<br>
            Code Generation: output the assembly code<br>
            Next we move to the Assembler
        </p>
    </p>
    <p id="subtext">
        Assembler(a dumb compiler for assembly language):<br>
        Input: Assembly Language Code (e.g., foo.s)<br>
        Output: Object Code, information tables<br>
        <p id="subtext_bullet">
            Reads and Uses Directives(directives: give directions to assembler but do not produce machine instructions: .text,.data,.globl sym,.string str,.word w1...wn)<br>
            Replace Pseudo-instructions<br>
            Produce Machine Language rather than just Assembly Language<br>
        </p>
    </p>
    <p id="subtext">
        Linker:<br>
        Input: Object code files with information tables(e.g. foo.o, libc.o)<br>
        Output: Executable code(e.g. a.out)<br>
        <p id="subtext_bullet">
            Enable separate compilation of files(changes to one file do not require recompilation of the whole program)<br>
            Combines several objects (.o) files into a single executable<br>
            <p id="subtext_bullet2">
                Step 1: take text segment from each .o file and put them together<br>
                Step 2: take data segment from each .o file, put them together and concatenate this onto end of text segments.<br>
                Step 3: resolve references(go through relocation table) aka fill in all absolute addresses<br>
            </p>
        </p>
    </p>
    <p id="subtext">
        <p id="subtext_bullet">
            Resolving References:<br>
            Linker assumes first word of first text segment is at address 0x04000000<br>
            Linker Knows: length of each text and data segment, ordering of text and data segments<br>
            Linker Calculates: absolute address of each label to be jumped to and each piece of data being referenced<br>
            To Resolve References: search reference(data or label) in all "user" symbol tables, if not found search library files, once absolute address is determined fill in the machine code appropriately<br>
            Thus we have our outputted executable file containing text, data and a header.<br>
        </p>
    </p>
    <p id="subtext">
        Loader:<br>
        Input: Executable Code(e.g. a.out)<br>
        Output: program is run<br>
        <p id="subtext_bullet">
            Executable files are stored on disk. When one is run, loader's job is to load it into memory and start it running.<br>
            In reality, loader is the operating system(OS): loading is one of the OS's tasks, and these days the loader actually does a lot of then linking.<br>
            Step 1: Reads executable file's header to determine the size of text and data segments.<br>
            Step 2: Creates new address space for program large enough to hold text and data segments along with a stack segment.<br>
            Step 3: Copies instructions and data from executable file into the new address space.<br>
            Step 4: Copies arguments passed to the program onto the stack.<br>
            Step 5: Initializes machine registers(most registers cleared but stack pointer assigned address of 1st free stack location).<br>
            Step 6: Jumps to start-up routine that copies program's arguments from stack to registers & sets the PC.(if main routine returns, start-up routine terminates program with the exit system call.)<br>
        </p>
    </p>
    <h4 id="subhead">Producing Machine Language:</h4>
    <p id="subtext">
        Simple Case: arithmetic, logical, shifts, etc instructions. All necessary info is within the instruction already, so just convert into the binary representations.<br>
        What about branches? PC Relative, so once pseudo-instructions are replaced by real ones we know by how many instructions to branch. So these can be computed. "Forward Reference" problem: branch instructions can refer to labels that are "forward" in the program. This is solved by taking 2 passes over the program, and remembering the address of the labels.<br>
        What about jumps(j and jal)? Jumps within a file are PC relative and thus can be computed. Jumps to other files we can't.<br>
        What about references to static data? la(load address) gets broken up into lui or auipc and addi. These will require the full 32 bit address of the data: auipc when we include into a relocatable block, lui when we have an absolute address. These can't be determined yet so we create two tables.<br>
        Symbol Table:
        <p id="subtext_bullet">
            List of "items" in this file that may be used by other files. Includes- labels:function calling, data:anything in the .data section; variables which may be accessed across files.<br>
        </p>
    </p>
    <p id="subtext">
        Relocation Table:
        <p id="subtext_bullet">
            List of "items" this file needs the address of later. Includes- any external label jumped to(including library files), any piece of data in the static section(such as the la instruction)<br>
        </p>
    </p>
    <p id="subtext">
        Object File Format:
        <p id="subtext_bullet">
            object file header: size and position of the other pieces of the object file<br>
            text segment: the machine code<br>
            data segment: binary representation of the static data in the source file<br>
            relocation information: identifies lines of code that need to be fixed up later<br>
            symbol table: list of this file's labels and static data that can be referenced<br>
            debugging information<br>
        </p>
    </p>
    <h4 id="subhead">Addresses:</h4>
    <p id="subtext">
        PC-Relative Addressing(beq, bne, jal): never relocate<br>
        External Function Reference(usually jal): always relocate<br>
        Static Data Reference(often auipc and addi): always relocate<br>
    </p>
    <h4 id="subhead">CALL chain: Compiling a C program SUMMARY:</h4>
    <p id="subtext">
        Compiler converts a single HLL file into a single assembly language. Assembler removes pseudo--instructions, converts what it can to machine language, and creates a checklist for the linker(relocation table). A .s file becomes a .o file. Note the Assembler does two passes to resolve addresses, handling internal forward references. Linker combines several .o files and resolves absolute addresses. Enables separate compilation, libraries that need not be compiled, and resolves remaining addresses. Loader loads executable into memory and begins execution.<br>

    </p>
    <h3 id="FP8">Synchronous Digital Systems</h3>
    <h4 id="subhead">Digital Systems:</h4>
    <p id="subtext">
        Digital: all values are discrete; a value can either be on(1) or off(0).<br>
        Analog: a continuous range of values.<br>
    </p>
    <h4 id="subhead">Logic Gates:</h4>
    <p id="subtext">
        The building blocks of digital circuits(AND, OR, XOR, NOT, NAND, NOR, XNOR)<br>
        <img id="medium_image" src="../assets/logicgates.png" alt="Logic Gates"><br>
    </p>
    <h4 id="subhead">Boolean Algebra:</h4>
    <p id="subtext">
        A branch of algebra in which the operands can only be 0 or 1, with the basic operations composed of AND, OR, and NOT.<br>
        <img id="medium_image" src="../assets/booleanalgebra2.JPG" alt="Boolean Algebra"><br>
        <img id="medium_image" src="../assets/booleanalgebra.png" alt="Boolean Algebra"><br>
        Example: an adder<br>
        <img id="medium_image" src="../assets/adderAbstraction.JPG" alt="Adder Algebra"><br>
    </p>
    <h4 id="subhead">Arithmetic Logic Unit(ALU):</h4>
    <p id="subtext">
        Carries out arithmetic and logical operations on integer binary numbers.<br>
        Multiplexor: selects an input to propagate to the output, out = A~S + BS.<br>
        Combinational Logic: As soon as the inputs are available, the output starts being computed. Output depends only on the current input.<br>
        Sequential Logic: Synchronized with a clock signal and output depends on a combination of inputs and previous states.
    </p>
    <h4 id="subhead">Synchronous Digital Systems:</h4>
    <p id="subtext">
        Synchronous: all operations are coordinated by something called a clock.<br>
        Clock Signal: oscillates between a high and low state; period: time between one rising edge to the next rising edge; frequency: 1/period. Unit for frequency is Hertz(Hz), a common clock frequency is 4GHz(the clock goes through 4 billion cycles every second and period = 1/4GHz or 0.25 ns)<br>
        Flip-Flops: state element(a circuit component that can hold a value), can be either asynchronous(independent of the clock) or synchronous(dependent on the clock).<br>
        D Flip-Flop: rising edge triggered(stores D to Q the instant the clock goes from 0 to 1) or falling edge triggered(stores D to Q the instant the clock goes from 1 to 0)<br>
        Clock-to-Q Delay: The amount of time that it takes for the input to propagate to the output after the clock trigger.<br>
        Set-up Time: The amount of time that the input needs to be stable before the clock trigger.<br>
        Hold Time: The amount of time that the input needs to be stable after the clock trigger.<br>
        Max Hold Time: The amount of time that it takes for the input to B to change after the trigger(clk-to-q delay of register A + combinational logic delay).<br>
        Combinational Logic Delay: The amount of time that it takes for a value to propagate through the combinational logic.<br>
        Minimum Clock Cycle(critical path): The time it takes for the input of one state element to reach the input of the next state element(clk-to-q delay + longest combinational delay + setup time).<br>
        So the question "How to store a 32 bit number?" Put 32 flip flips together and thus a register(state element) is formed. A register has an extra input Write Enable which when 0 the contents of the register stay the same but when 1 the contents of the register are updated on the clock trigger.<br>
        <img id="medium_image" src="../assets/registerdiagram.JPG" alt="Register Diagram"><br>
    </p>
    <h4 id="subhead">Transistors:</h4>
    <p id="subtext">
        Metal Oxide Semiconductor Field Effect Transistor: three terminals(source = input, gate = controls whether the witch is open or closed, Drain = output )<br>
        nFET vs pFET Transistors:<br>
        nFET: g=1 switch is closed | g=0 switch is open<br>
        pFET: g=0 switch is closed | g=1 switch is open<br>
        <img id="medium_image" src="../assets/nFETvspFET.JPG" alt="nFET vs pFET Transistors"><br>
        nFETs are not good at passing 1s so we usually hook up the source of an nFET to a 0. pFETs are not good at passing 0s so we usually hook up the source of a pFET to 1.<br>
    </p>
    <h4 id="subhead">CMOS:</h4>
    <p id="subtext">
        Uses complementary and symmetrical pairs of p-type and n-type MOSFETs to build logical functions. Consists of a pull-up and pull-down network.<br>
        We can construct our boolean gates with CMOS transistors.<br>
        CMOS Inverter:<br>
        <img id="medium_image" src="../assets/CMOSinverter.JPG" alt="CMOS Inverter"><br>
        <img id="medium_image" src="../assets/ANDtransistors.PNG" alt="AND Transistor">
        <img id="medium_image" src="../assets/NANDtransistors.PNG" alt="NAND Transistor"><br>
        <img id="medium_image" src="../assets/ORtransistors.PNG" alt="OR Transistor">
        <img id="medium_image" src="../assets/NORtransistors.PNG" alt="NOR Transistor"><br>
    </p>
    <h4 id="subhead">Realistic Transistor Model:</h4>
    <p id="subtext">
        Transistors are not perfect switches; the leak when off and have finite resistance when on.<br>
        All circuits nodes have capacitance; to change their voltage level must displace charge.<br>
        This cause a water bucket analogy where the capacitors a filled or drained in time not instantly. Consequences: every logic gate has a delay from input change to output change, for cascaded gates delay accumulates.<br>
        <img id="medium_image" src="../assets/nFETrealistic.JPG" alt="nFET Realistic Model">
        <img id="medium_image" src="../assets/pFETrealistice.PNG" alt="pFET Realistic Model"><br>
        CMOS circuits use electrical energy.<br>
        <p id="subtext_bullet">
            Energy is the ability to do work(joules)<br>
            Power is the rate of expending energy(watts)<br>
            Energy Efficiency: energy per operation<br>
            E = 1/2 * C * V^2<br>
            P = 1/2 α C * V^2 * F(α=activity factor, C=total chip capacitance, F=clock frequency).<br>
            Power proportional to F, reducing frequency will reduce power. But that doesn't improve energy efficiency(just spread computation over longer time).<br>
            Energy Efficiency: E ∝ V^2 but τ ∝ V; therefore by lowering supply voltage energy efficiency is lowered, make up for less performance by using parallelism.<br>
        </p>
    </p>
<h3 id="FP9">Datapath</h3>
<p id="subtext">
    To build a complete processor two things are needed: the datapath that implements the computation and the control logic that looks at the instruction and tells the datapath what to do. Think of this setup like a Marionette; the puppet being the datapath and the strings being the control logic.<br>
</p>
<h4 id="subhead">"State" Required by RV321 ISA:</h4>
<p id="subtext">
    Each instruction reads and updates this state during execution:<br>
</p>
<p id="subtext_bullet">
    Registers(0x...x31):<br>
    Register file Reg holds 32 registers x 32 bits/register: Reg[0]...Reg[31]<br>
    First register read specified by rs1 field in instruction. Second register read specified by rs2 field in instruction. Write register specified by rd field in instruction.<br>
    x0 is always 0.<br>
    Program Counter(PC):<br>
    Holds address of current instruction.<br>
    Memory(MEM):<br>
    Holds both instructions and data, in one 32--byte-addressed memory space. To help with abstraction memory for instructions and data was separated in the course. Instructions are read from IMEM(instruction memory-read_only). Load/store instructions access data memory.<br>
</p>
<h4 id="subhead">Basic Phases of Instruction Execution:</h4>
<p id="subtext">
    There are five basic phases of instruction execution:<br>
    Instruction Fetch: Send address to the instruction memory (IMEM), and read IMEM at that address.<br>
    Instruction Decode: Generate control signals from instruction bits, generate the immediate, and read registers from the reg file.<br>
    Execute: Perform ALU computations, and do branch comparisons.<br>
    Memory: Read from or write to the data memory(DMEM).<br>
    Writeback: write back either PC + 4, the result of the ALU operation, or data from the memory to the Reg file.<br>
    <img id="medium_image" src="../assets/singlecycledatapath.JPG" alt="RISC-V Single Cycle Datapath"><br>
</p>
<h4 id="subhead">Specificities:</h4>
<p id="subtext">
    I-Format Immediate: high 12 bits of instruction(inst[31:20]) copied to low 12 bits of immediate(imm[11:0]). Immediate is sign extended by copying value of inst[31] to fill the upper 20 bits of the immediate value.<br>
    I & S Immediate Generator: Just need a 5-bit mux to select between two positions where low five bits of immediate can reside in instruction. Other bits in immediate are wired to fixed positions in instruction.<br>
    B Format: mostly the same as S-Format with two register sources (rs1/rs2) and a 12-bit immediate. But now immediate represents values --4096 to +4094 in 2 byte increments. The 12 immediate bits encode an even 13 bit signed byte offset(lowest bit of offset is always zero so no need to store it).<br>
    Branch Comparator: BrEq = 1, if A=B | BrLt = 1, if A < B | BrUn = 1 selects unsigned comparison for BrLt, 0=singed. | BGE branchL A>=B if ~(A < B)<br>
    Jal Instruction: sets PC = PC + offset; target somewhere within +- 2^19 locations, 2 bytes apart. Or +- 2^18 32 bit instructions.<br>
</p>
<h4 id="subhead">Universal Datapath Summary:</h4>
<p id="subtext">
    Capable of executing all RISC-V instructions in one cycle each. Datapath is the "union" of all the units used by all the instructions. Muxes provide the options. Not all units(hardware) used by all instructions. 5 phases of execution and not all instructions are active in all phases. Controller specifies how to execute instructions.<br>
</p>
<h4 id="subhead">Control:</h4>
<p id="subtext">
    <img id="medium_image" src="../assets/controlblock.JPG" alt="Control Block"><br><br>
    Tells universal datapath how to execute each instruction.<br>
    ROM(read-only Memory):<br>
    Regular structure made from transistors. Cna be easily reprogrammed during the design process to fix errors or add instructions. Popular when designing control logic manually. An address decoder is used to access the ROM.<br>
    Combinatorial Logic: Today, chip designers often use logic synthesis tools to convert truth tables to networks of gates. Logic equations for each control signal. Can exploit output "don't cares" and input "for all values" to simplify circuit.<br>
</p>
<h4 id="subhead">Performance:</h4>
<p id="subtext">
    At the current moment we have our datapath incrementing along the 5 stages processing 1 instruction. Essentially 4/5 phases of the datapath are unused while one is working. We can optimized this with a factory line approach.<br>
    Performance Measures:<br>
    <p id="subtext_bullet">
        Latency: instruction execution time<br>
        Throughput: total number of instructions executed per unit time<br>
        Energy Efficiency: energy per instruction, e.g. how many total instructions executed per batter charge<br>
    </p>
</p>
<p id="subtext">
    "Iron Law" of Processor Performance:<br>
    <img id="large_image" src="../assets/ironlawPerfromance.JPG" alt="Iron Law of Performance"><br><br>
    Instructions per Program: Determined by task specification, algorithm, programming language, compiler, and Instruction Set Architecture.<br>
    (averg)Clock Cycles per Instruction: Determined by Instruction Set Architecture(CISC vs RISC), processor implementation, pipelined processors(CPI > 1), superscalar processors(CPI < 1).<br>
    Time per Cycle: Determined by processor microarchitecture, technology, supply voltage(lower voltage reduces transistor speed but improves energy efficiency).<br>
    Note: Energy per task can also be written as C*Vdd^2 where "C" is the capacitance depending on technology, microarchitecture, circuit details and Vdd is the supply voltage.<br>
    Energy "Iron Law":<br>
    Energy efficiency is a key metric in all computing devices. For power constrained systems need better energy efficiency to get more performance at same power. For energy constrained systems need better energy efficiency to prolong batter life.<br>
</p>
<h4 id="subhead">Pipelining:</h4>
<p id="subtext">
    Performance(tasks/second) = power(Joules/sec) * energy efficiency(tasks/Joule)<br>
    Pipelining: is the process of implementing task completion in parallel, instead of limiting task completion to sequentially occur. Pipelining increases throughput but can never improve latency(sometimes decreases).<br>
    <img id="medium_image" src="../assets/riscvpipeling.PNG" alt="RISC-V Pipelining"><br><br>
    Pipeline registers separate stages, and hold date for each instruction in flight. Control signals derived from instruction, as in single cycle implementation. Information is stored in pipeline registers for use by later stages.<br>
    Multiple tasks operating simultaneously using different resources. Potential speedup = Number pipe stages. Time to "fill" or "drain" pipeline reduces speedup.<br>
    <img id="medium_image" src="../assets/pipelining.png" alt="Pipelining">
    <img id="medium_image" src="../assets/pipelingperformance.png" alt="Pipelining Performance"><br><br>
</p>
    <p id="subtext">
        Pipelining Hazards:<br>
        Structural, Data(R-type/load instructions), and Control are all hazard types. 
    </p>
    <p id="subtext_bullet">
        Structural Hazard: Two or more instructions in the pipelining compete for access to a single physical resource. Solution 1- Instructions take turns to use resource, some instructions have to stall. Solution 2- Add more hardware to machine.<br>
        Reg File Structural Hazards: Each instruction can read up to two operands in decode stage. Can write one value in writeback stage. Therefore two different instructions might be accessing the register file on the same cycle. Avoid structural hazards by having separate ports. Read from one instruction and writes from another happens simultaneously. <br>
        Memory Structural Hazard: Instruction and data memory used simultaneously. Caches: relatively small and fast "buffer" memories. Solution use two different cache memories.<br>
        Summary: Conflict for use of resource. IN RISC-V pipeline with a single memory: load/store requires data access, without separate memories instruction fetch would have to stall for that cycle. Pipelined datapaths require separate instruction/data caches. Multi-ported register file. RISC ISAs (including RISC-V) designed to avoid structural hazards.<br>
    </p>
    <p id="subtext_bullet">
        Data Hazard: Instruction dependent on data that causes issue. 
        R-type Instruction: Instruction depend on result from previous instruction. Solution 1- bubble and stall the dependent instruction, stalls reduce performance, compiler could try to arrange code to avoid hazards and stalls but this requires code compiler understanding. Solution 2- forwarding(bypassing): grab operand from pipeline stage, rather than register file, requires additional connections in the datapath. To detect a need for forwarding compare destination register of older instructions in pipeline with sources of new instruction in decode stage. Thus we can use a previous result of an add in the next clock cycle.<br>
        Load Instruction: Trying to compute upon a value that is being loaded from memory, 1 cycle stall unavoidable. We stall, and repeat and instruction and forward. Slot after a load is called the load delay slot, if that instruction uses the result of the load then the hardware will stall for one cycle, equivalent to inserting an explicit nop. Put unrelated instruction into load delay slot. We reorder instruction so that the lw values are used directly following aka microarchitecture optimization.<br>
    </p>
    <p id="subtext_bullet">
        Control Hazard: two instructions executed following a branch instruction regardless of branch outcome because its down stream. Solution 1- if branch not taken then instructions fetched sequentially after branch are correct, if branch or jump taken, then need to flush incorrect instructions from pipeline by converting to NOPs. Every taken branch in simple pipeline costs 2 dead cycles. To improve performance, use “branch prediction” to guess which way branch will go earlier in pipeline. Only flush pipeline if branch prediction was incorrect. 
        Branch Prediction: Keep a branch prediction buffer/cache: Small memory addressed by the lowest bits of PC. During instruction decode, if branch: Look up whether branch was taken last time? If yes, compute PC + offset and fetch that (or store actual branch target address from last time). If no, stick with PC + 4. If branch hasn't been seen before: assume forward branches are not taken, backward branches are taken. Update state on predictor with results of branch when it is finally calculated. Jumps are never mispredicted... but JALR can be. 
    </p>
<h4 id="subhead">Superscalar Processors:</h4>
<p id="subtext">
    Increasing single Processor Core Performance: increase clock rate, deeper pipeline, multi-issue "superscalar" processor.<br>
    Multiple issue "superscalar":<br>
    Replicate pipeline stages => multiple pipelines<br>
    start multiple instructions per clock cycle<br>
    CPI > 1, so use instructions Per Cycle(IPC)<br>
    "out of order execution": Reorder instructions dynamically in hardware to reduce impact of hazards<br>
    <img id="medium_image" src="../assets/superscalarprocessor.png" alt="Superscalar Processor"><br><br>
    Don't just have one super-scalar processor core, have multiple!<br>
    Past approach: Take your best single-core and replicate it...Problem: Those superscalar beasts consume a huge amount of energy. <br>
    New approach: Big/Little...Some of your processor cores are the single-core beasts, Some are a more energy-optimized design, When absolute performance isn't needed, run tasks on the energy-efficient cores.<br>
</p>
<h4 id="subhead">Pipelining and RISC-V:</h4>
<p id="subtext">
    RISC-V ISA designed for pipelining:<br>
    All instructions are 32-bits in the RV-32 ISA(Easy to fetch and decode in one cycle), Few and regular instruction formats(Decode and read registers in one step), Load/store addressing(Calculate address in 3rd stage, access memory in 4th stage, great for 5 stage pipeline), Alignment of memory operands(Memory access takes only one cycle).<br>
    Apple M1 Processor:<br>
    <img id="medium_image" src="../assets/appleM1.png" alt="Apple M1 Processor"><br><br>
</p>
<h3 id="FP10">Caches:</h3>
<h4 id="subhead">Memory Hierarchy:</h4>
<p id="subtext">
    Mismatch between processor and memory speeds leads us to add a new level... introducing a "memory cache". Usually on the same chip as the cpu, faster but more expensive then DRAM. Cache is a copy of a subset of main memory. Most processors have separate caches for instructions and data.<br>
    Memory hierarchy:<br>
    <img id="medium_image" src="../assets/memoryhierachy.JPG" alt="memory Hierarchy"><br><br>
    Cache contains copies of data that are being used. Caches work on the principles of temporal and spatial locality:<br> 
    <p id="subtext_bullet">
        Temporal Locality- if use it now chances are that we'll want to use it again soon. Keep most recently accessed data items closer to the processor.<br>
        Spatial Locality- if we use a piece of memory chances are we'll use the neighboring pieces. Move blocks consisting of contiguous words closer to the processor.<br>
    </p>
</p>
<h4 id="subhead">Memory Access Pattern:</h4>
<p id="subtext">
    <img id="medium_image" src="../assets/memoryaccesspatterns.JPG" alt="Memory Access Patterns"><br><br>
    Memory Access with/without Caches:<br>
    <img id="medium_image" src="../assets/memoryaccesswithoutcache.PNG" alt="Memory Access without Cache">
    <img id="medium_image" src="../assets/memoryacceswithcache.PNG" alt="Memory Access with Cache"><br><br>
    Cache Hit: the data you were looking for is in the cache, retrieve the data from the cache and bring it to the processor.<br>
    Cache Miss: the data you were looking for is not in the cache, go to the memory and put the data in the cache, then bring to the processor.<br>
    Hit Rate: number of hits/number of accesses<br>
    Miss Rate: 1- hit rate<br>
    Hit Time: the time that is takes for you to access an time on a cache hit<br>
    Miss Penalty: on a miss the time it takes to access the block after discovering that ts not in cache.<br>
</p>
<h4 id="subhead">How is our data stored in the cache?:</h4>
<p id="subtext">
    Fully Associative Cache:<br>
    Cache line/block: a single entry in the cache, each line has its own entry<br>
    Line size/block size: the number of bytes in each cache line<br>
    Tag: identifies the data stored at a given cache line<br>
    Valid Bit: tells you fi the data stored at a given cache line is valid<br>
    Capacity: the total number of data bytes that can be stored in a cache<br>
    Handling Stores: Store instructions write to memory, changing values, need to make sure cache and memory have consistent information.  
    Write Through: write to the cache and memory at the same time, the write to memory takes longer. Write Back: write data in cache and set the dirty bit to 1, when this line gets evicted from the cache write it to memory.<br>
    Write-allocate: on a write miss, you bring the line into the cache and then update the line. No Write-allocate: on a write miss, don't bring the line into the cache, you only update memory. For both you always bring the line in on a read miss. Common Combinations: write through and no write-allocate, or wite back and write allocate.<br> 
    Hardware Implementation: Need a comparator for each row in the cache to check the tag.<br>
    <img id="medium_image" src="../assets/cachestructure.JPG" alt="Cache Structure">
    <img id="medium_image" src="../assets/cachestructure2.JPG" alt="Cache Structure"><br><br>
    <img id="medium_image" src="../assets/cachestructure3.PNG" alt="Cache Structure">
    <img id="medium_image" src="../assets/cachestructure4.JPG" alt="Cache Structure"><br><br><br>
    Direct Mapped Caches:<br>
    Go to the index, check the tag and compare then load or store.<br> 
    <img id="medium_image" src="../assets/directmapcache1.JPG" alt="Cache Structure"><br>
    <img id="medium_image" src="../assets/directmapcache2.PNG" alt="Cache Structure">
    <img id="medium_image" src="../assets/directmapcache3.PNG" alt="Cache Structure"><br><br>
    Set Associative Caches:<br>
    Compromise between fully associative and direct map cache.<br>
    The data can only be stored at one index but there are multiple slots to store it in.The LRU bit indicates which set was least recently used. <br>
    <img id="medium_image" src="../assets/setassociativecache.jpeg" alt="Cache Structure"><br>

</p>
<p id="subtext">
    Eviction Policies:
    <p id="subtext_bullet">
        Least Recently Used: evict the line that was least recently used. A set includes all of the ways of that index, the LRU tells us which way is the least recently used in the set.<br>
        Approximate LRU(clock algorithm):<br>
        Each cache line has 1 referenced bit. This bit is set to 1 each time the line is accessed. When a line needs to be evicted we that at the first entry in the cache and check its reference bit. If 1, set to 0 and move on. Else choose as victim. The next time we need to evict a line from the cache we will start searching at the point where wee lest off.<br>
    </p>
    <p id="subtext_bullet">
        Most Recently Used: evict the line that was most recently used.<br>
    </p>
    <p id="subtext_bullet">
        Random: chooses a random line to evict, thus don't have to keep track of additional metadata.<br>
    </p>
</p>
<p id="subtext">
    Types of Misses:<br>
    <p id="subtext">
        Compulsory Miss: caused by first access to block that has never been in the cache.<br>
        Capacity Miss: caused when the cache cannot contain all the blocks needed during the execution of a program. Occur when blocks were in the cache, replaced, and later retrieved.<br>
        Conflict Miss: Occur in set-associative or direct mapped caches when multiple blocks compete for the same set.<br>
    </p>
</p>
<p id="subtext">
    Cache Summary:<br>
    <img id="medium_image" src="../assets/cachecomparisons.jpeg" alt="Cache Structure"><br>
</p>
<h4 id="subhead">Cache Performance:</h4>
<p id="subtext">
    Important to understand cache configurations/structure to design software that utilizes cache in an efficient way.<br>
    Cache Blocking: A technique where data accesses are rearranged to make better use of the data that is brought into the cache. Helps prevent repeatedly evicting and fetching the same data from the main memory.<br>
    <p id="subtext_bullet">
        Example: Matrix multiplication, transposing the second matrix to make cache accesses more efficient. For even better performance, use cache blocking take chunks of each matrix multiply and move on to the next chunk.<br><br>
        <img id="medium_image" src="../assets/cacheblocking1.jpeg" alt="Cache Blocking"><br>
    </p>
</p>
<p id="subtext">
    Measuring Cache Performance:<br>
    <p id="subtext_bullet">
        Hit Rate: number of hits / number of accesses<br>
        Miss Rate: 1 - hit rate<br>
        Hit Time: The time that it takes for you to access an item on the cache hit<br>
        Miss Penalty: On a miss, the time it takes to access the lock after discovering that its not in the cache<br>
        AMAT(average memory accesses time): hit time + miss rate * miss penalty<br>
        <p id="subtext_bullet2">
            How does increasing Associativity Affect AMAT?<br>
            Hit time increases as associativity increases, because of the multiplexors, such that a 2 way associative set is more efficient then a 4 way set associative.<br>
            Miss rate decreases as associativity increases, because due to less conflict misses.<br>
            Miss Penalty Mostly unchanged, replacement policy runs in parallel with fetching missing line from memory.<br>
            How does increasing number of entries affect AMAT?<br>
            Hit time increases since reading tags and data from larger memory structures<br>
            Miss rate decreases due to increased capacity and fewer conflict misses<br>
            Miss penalty is unchanged<br>
            How does increasing block size affect AMAT?<br>
            Hit time mostly unchanged but might be slightly reduced as number of tags is reduced<br>
            Miss rate goes down at first due to spatial locality then increases due to increased conflict misses due to fewer blocks in the cache<br>
            Miss penalty rises<br>            
        </p>
    </p>
</p>
<p id="subtext">
    Another way to reduce miss penalty is to include another cache between processor and main memory. The L2 cache is bigger than L1, accessed only if the requested data is not found in L1. L2 takes longer to access because it is larger and further away from the processor. All data in L1 can be found in L2. If the line in L1 is dirty when evicted you update the copy in L2.<br>
    L1 Cache: embedded in processor chip, fast but limited storage.<br>
    L2 Cache: embedded in processor chip or on its own chip, reduces L1 miss penalty.<br>
    L3 Cache: on a separate chip, reduces L1 and L2 miss penalty.<br>
    L4 Cache: uncommon.<br>
    Hit Rate is thus differentiated into Local and global scales; local being hits at said level and global being total hits. <br>
    New AMAT = L1 hit time + L1 miss rate*(L2 hit time + L2 miss rate * L2 miss penalty) Note: similar convention follows for cascading caches.<br>
</p>
<h3 id="FP11">Parallelism</h3>
<h4 id="subhead">Why Parallelism?:</h4>
<p id="subtext">
    CPU Clock rates are no longer increasing: technical & economic challenges; advanced cooling technology too expensive and energy cost are prohibitive. Thus, parallel processing is only path to higher speed.<br>
    Two Basic Approaches:
    <p id="subtext_bullet">
        Multiprogramming: run multiple independent programs in parallel, easy. <br>
        Parallel Computing: run one program faster, hard.<br>
    </p>
</p>
<h4 id="subhead">Parallel Computing:</h4>
<p id="subtext">
    Single Instruction/Single Data Stream(SISD): Sequential computer that exploits no parallelism in either the instruction or data streams. Examples of SISD architecture are traditional uniprocessor machines; our RISC-V processor.<br>
    <img id="small_image" src="../assets/SISD.jpeg" alt="SISD"><br>
    Single Instruction/Multiple Data Stream(SIMD): SIMD computer processes multiple data streams using a single instruction stream, example Intel SIMD instruction extensions.<br>
    <img id="small_image" src="../assets/SIMD.jpeg" alt="SIMD"><br>
    Multiple-Instruction/Multiple-Data Streams(MIMD): Multiple autonomous processors simultaneously executing different instructions on different data, example multicore and Warehouse-scale computers.<br>
    <img id="small_image" src="../assets/MIMD.jpeg" alt="MIMD"><br>
    Multiple-Instruction/Single-Data Stream(MISD): Multiple-Instruction, Single-Data stream computer that processes multiple instruction streams with a single data stream, example historical significance.<br>
    <img id="small_image" src="../assets/MISD.jpeg" alt="MISD"><br><br>
    Currently SIMD and MIMD are the most common parallelism architecture, usually both in the same system.<br>
</p>
<h4 id="subhead">Loop Unrolling:</h4>
<p id="subtext">
    On high performance processors, optimizing compilers performs "loop unrolling" operation to expose more parallelism and improve performance.<br>
</p>
<h4 id="subhead">Blocking:</h4>
<p id="subtext">
    Rearrange code to use values loaded in cache many times, only "few" accesses to slow main memory(DRAM) per floating point operation.<br>
</p>
<h4 id="subhead">Amdahl's Law:</h4>
<p id="subtext">
    <img id="medium_image" src="../assets/AmdahlsLaw.jpeg" alt="Amdahl's Law"><br><br>
</p>
<h4 id="subhead">Strong and Weak Scaling:</h4>
<p id="subtext">
    Strong Scaling: when speedup can be achieved on a parallel processor without increasing the size of the problem.<br>
    Weak Scaling: when speedup is achieved on a parallel processor by increasing the size of the problem proportionally to the increase in the number of processors.<br>
    Load Balancing: every processor doing same amount of work, e.g. one unit with twice the load of others cuts speedup almost in half. <br>
</p>
<h4 id="subhead">Multiprocessor Execution:</h4>
<p id="subtext">
    <img id="medium_image" src="../assets/multiprocessorExecution.jpeg" alt="Multiprocessor Execution"><br><br>
</p>
<h4 id="subhead">Comparing Types of Parallelism:</h4>
<p id="subtext">
    SIMD(data parallel): a SIMD favorable problem can map easily to a MIMD type fabric. SIMD type generally offer a much higher throughput per dollar. Much simpler control logic, common approach is "vector" like.<br>
    MIMD(data-dependent branches): a MIMD favorable problem will not map easily to SIMD type fabric.<br>
    Only path to performance is parallelism. Key challenge is to craft parallel programs that have high performance on multiprocessors as the number of processors increase -i.e. that scale.<br>
</p>
<h4 id="subhead">Threads:</h4>
<p id="subtext">
    <img id="medium_image" src="../assets/threads.jpeg" alt="Threads"><br><br>
    Processor resources are expensive and should not be left idle. Hardware switches threads to bring in other useful work while waiting for cache miss. Put in redundant hardware so don't have to save context on ever thread switch i.e. PC, registers.<br>
</p>
<h4 id="subhead">Multithreading vs Multicore:</h4>
<p id="subtext">
    <img id="medium_image" src="../assets/multithreadingVmulticore.jpeg" alt="Multithreading vs Multicore"><br><br>
</p>
<h4 id="subhead">Latest Design-Big_Lil:</h4>
<p id="subtext">
    "Big" processors are needed for both single threaded and multi-threaded performance, but such processors are very inefficient. Big_Lil designs "smaller" processors for idle and small tasks; "big" processors are used once the "small" can't keep up.<br>
</p>
<h4 id="subhead">OpenMP:</h4>
<p id="subtext">
    OpenMP is a language extension used for multi-threaded, shared memory parallelism. Portable, standardized and easy to compile(cc -fopenmp name.c). Create threads with markers, so that independent code blocks can be run at the same time.<br>
    Pros: takes advantage of shared memory, programmer need not worry about data placement. Compiler directives are simple and easy to use. Legacy serial code does not need to be written.<br>
    Cons: Code can only be run in shared memory environments. Compiler must support openMP. Amdahl's law is gonna get you after not too many cores...<br>
    Data Race: if different threads attempt to access the same location, and at least one is a write,, and they occur one after another. Avoid data races by synchronizing writing and reading to get deterministic behavior.<br>
    <p id="subtext_bullet">
        Lock synchronization: use a "lock" to grat access to a region(critical section) so that only one thread can operate at a time. Processors read lock and either wait or set lock and go into critical section; 0-unlocked 1-locked. Check the lock, if unlocked set lock and access critical section then unset lock; else idle till unlocked or go do another process until unlocked.<br>
        To build a lock hardware support is needed; Atomic read/write memory operation. Single inst- atomic swap of register <-> memory. Pair inst: one for read, one for write.<br>
    </p>
</p>
<p id="subtext">
    Deadlock: a system stat in which no progress is possible because everything is locked waiting for something else. Many solutions but typically program specific. Locks innately inhibit parallelism, so try and limit locks or eliminate.<br>
</p>
<p id="subtext">
    <img id="medium_image" src="../assets/RISCVsynchronization.jpeg" alt="RISC-V Synchronization"><br><br>
    Example: use OpenMp to break up a for loop, that has not loop dependency. If the loop is doing a sum operation splitting it up will require the reduction operation, each loop has its own sum varibale of which are combined after the parallel loops execute.<br> 
</p>
<p id="subtext">
    <img id="medium_image" src="../assets/OpenMP.jpeg`" alt="OpenMP"><br><br>
</p>
<h4 id="subhead">SMP:</h4>
<p id="subtext">
    SMP(Shared Memory) Symmetric Multiprocessor:<br>
    <img id="medium_image" src="../assets/SharedMemoryMultiprocessor.jpeg" alt="SMP"><br><br>
    Problems Arrise in data races, if one cache pulls data from main mem at X, then a second cache pulls the same data from main mem at X, then the first cache writes to memory X the second cache has stale data but how does it know that?<br>
    Coherency: keep cache values coherent. When any processor has caches miss or writes notify other processor via interconnection entwork. Write transactions from one processor, other caches "snoop" the common interconnect chec iing for tages they hold. We want write-back caches, we want to minimize writes overall. The neighboring processor's cache is faster to get to than main memory.<br>
   Common Cache Coherency Policy: MOESI
    Shared: up-to-date data, other cahces may have a copy(valid bit set, shared bit set)<br>
    Modified: up-to-date data, changed(dirty) no other cache has a copy, OK to write, memory out-of-date(valid and dirty bit set)<br>
    Exclusive: up-to-date data no other cache has a copy, OK to write, memory up-to-date(valid bit set). If any other cache reads this line the state becomes shared, if I write this line sates becomes modified but I don't need to broadcast this when I do the write.<br>
    Owner: up-to-date data, other caches may have a copy, memory is not up-to-date(valid, dirty, and shared is set). This cache now supplies data on read instead of going to memory, when you write once again have to have the other caches invalidate.<br>
    Invalid: Invalid
    <img id="medium_image" src="../assets/MOESI.jpeg" alt="MOESI"><br><br>
    Problem: Everyt time one processor does a write the other processor will take a cache miss on the next read or write...new class of cache miss type: coherency.<br>
</p>
<h4 id="subhead">GO Language:</h4>
<p id="subtext">
    Language created at Google starting in 2007. Language continues to evolve but a commitment to backwarrds campatibilty.<br>
    <img id="medium_image" src="../assets/golanguage.jpeg" alt="GO"><br><br>
</p>
<h3 id="FP12">Operating System</h3>
<h4 id="subhead">Introduction:</h4>
<p id="subtext">
    The OS is the software that runs on a computer. It is the software that provides the hardware with the ability to perform tasks such as reading and writing to memory, executing programs, and managing the hardware. Common OS: Unix(Berkeley Software Distribution, macOS), Linux(Debian, Ubuntu, Red Hat), Microsoft Windows<br>
    Illusionist: The OS is divided up into threads to run mutliple processors at once. It provides clean, easy-to-use abstractions of physical resources. Masks limitations and abstracts higher level objects(files, sockets, etc.)
    Referee: It manages protection, isolation and sharing of resources. The OS isolates processes from each other, OS isolates itself from other process, even though they are actually running on the same hardware.<br>
    <img id="medium_image" src="../assets/operatingsystem.jpeg" alt="Operating System"><br><br>
</p>
<h4 id="subhead">Context Switch:</h4>
<p id="subtext">
    Context Switch: the swithcing from executing one program to another. Allows multiple processes to run on the same processor. The OS determines when to context switch.<br>
    What Happens: The OS takes control of the CPU from the current process -> the OS saves the state of the current process -> the OS loads the state of the next process -> the OS hands over the CPU to the next process.<br>
</p>
<h4 id="subhead">Dual Mode Operation:</h4>
<p id="subtext">
    Hardware provides at least two modes: Kernel mode("supervisor" mode) and User mode<br>
    Certain operations are prohibited when running in suer mode: interacting directly with hardwared, writing to kernel memory.<br>
    OS mostly runs in user mode.<br>
    Switching between user mode and kernel mode: system calls, interupts, and exceptions.<br>
</p>
<p id="subtext">
    System Call:
    <p id="subtext_bullet">
        Allows the program to request a service from the operating system. Examples: creating and deleting files, reading and writing files, accessing external devices like a scanner, etc. Similiar to function calls except it's executed by the kernel.<br>
        <img id="medium_image" src="../assets/systemcall.jpeg" alt="System Call"><br><br>
    </p>
</p>
<p id="subext">
    Interupts:
    <p id="subtext_bullet">
            Caused buy an event external to the current program. Example keyboard press or mouse click. Asynchronous to the current program; does not need to be handled immediatly but should be handled soon.<br>
    </p>
</p>
<p id="subext">
    Exceptions:
    <p id="subtext_bullet">
        Caused by an event during the execution of the current program. Example illegal intsruction, divide by zero. Synchronous to the current program; must be handled immediatly.<br>
        <img id="medium_image" src="../assets/exceptions.jpeg" alt="Exceptions"><br><br>
    </p>
</p>
<p id="subtext">
    Trap Handler: code the services the interupt or exception; from the program's point of view it mus look like nothing happened.<br>
    Traps: all instruciton befor ethe faulting instruction must complete -> all instructions after the faulting instruction must be flushed -> the faulting instruction must be flushed -> executiong of the trap handler begins.<br>
    What it does: saves the state of the current program -> determine what caused the exxception or interupt -> handle the exception or interupt -> continue execution of the program: restoring the state of the program and returning the control to the program or terminate the execution of the program: free resources and schedule a new program.<br>
    <img id="medium_image" src="../assets/traphandler.jpeg" alt="Trap Handler"><br><br>
    ia is the faulting instruction we jump to the handler code who decides wether continuation or termination of the program is appropriate. Typically interupts and certain memory exceptions call for the contineued execution of the program, in contrast, illegal instructions and certain illegal memory accesses call for the termination of the program.<br>
</p>
<h4 id="subhead">Kernel:</h4>
<p id="subtext">
    <img id="medium_image" src="../assets/kernel.jpeg" alt="Kernel"><br><br>
</p>
<h4 id="subhead">Shell:</h4>
<p id="subtext">
    A program the exposes the operating system's services.<br>
</p>
<h4 id="subhead">What happens at Boot?</h4>
<p id="subtext">
    The BIOS(Basic Input Output System) runs the Power on Self Test(Post) and the bootloader -> the bootloader loads in part the operating system -> the operating system initializes services, drivers, etc -> launch a process that waits for an input in a loop<br>
    Bootstrapping: a chain of stages, in which at each stage, a smaller, simpler program loads and then execurtes the larger more complicated program of the next stage.<br>
</p>
<h4 id="subhead">how do you begin executing a program?</h4>
<p id="subtext">
    Loader: responsible for loading the program into memory.<br>
    The loader loads program into memory -> the loaders sets argc and argv -> the OS jumps to main and transfers control to the process<br>
</p>
</body>
</html>